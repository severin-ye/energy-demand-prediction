"""
UCIæ•°æ®æ¨ç†æµ‹è¯•
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆæ˜“è¯»çš„ç»“æœæŠ¥å‘Š
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime
from pathlib import Path

from src.pipeline.inference_pipeline import InferencePipeline
from src.visualization.inference_visualizer import InferenceVisualizer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_test_data(test_file, n_samples=100):
    """
    åŠ è½½æµ‹è¯•é›†æ•°æ®
    
    Args:
        test_file: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
        n_samples: ä½¿ç”¨å¤šå°‘æ ·æœ¬è¿›è¡Œæµ‹è¯•
    """
    logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    df = pd.read_csv(test_file)
    
    # åªå–å‰n_samplesä¸ªæ ·æœ¬ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦
    if len(df) > n_samples:
        df = df.iloc[:n_samples].copy()
        logger.info(f"ä½¿ç”¨å‰{n_samples}ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†")
    
    # å‡†å¤‡ç‰¹å¾
    feature_cols = ['Global_reactive_power', 'Voltage', 'Global_intensity']
    target_col = 'Global_active_power'
    
    # é‡å‘½åç›®æ ‡åˆ—ä¸ºEDPï¼ˆå…¼å®¹è®­ç»ƒæ—¶çš„å‘½åï¼‰
    df_test = df[feature_cols + [target_col]].copy()
    df_test = df_test.rename(columns={target_col: 'EDP'})
    
    logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {df_test.shape}")
    logger.info(f"EDPèŒƒå›´: [{df_test['EDP'].min():.2f}, {df_test['EDP'].max():.2f}]")
    
    return df_test


def format_prediction_results(results):
    """
    å°†é¢„æµ‹ç»“æœæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æŠ¥å‘Š
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
    """
    report = []
    report.append("=" * 80)
    report.append(" " * 25 + "ğŸ“Š æ¨ç†ç»“æœæŠ¥å‘Š")
    report.append("=" * 80)
    
    # 1. åŸºæœ¬ä¿¡æ¯
    report.append("\nã€1. åŸºæœ¬ä¿¡æ¯ã€‘")
    report.append(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(results['predictions'])} ä¸ª")
    report.append(f"  é¢„æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 2. é¢„æµ‹ç»Ÿè®¡
    predictions = results['predictions']
    true_values = results.get('true_values', [])
    
    report.append("\nã€2. é¢„æµ‹ç»Ÿè®¡ã€‘")
    report.append(f"  é¢„æµ‹EDPèŒƒå›´: [{predictions.min():.3f}, {predictions.max():.3f}] kW")
    report.append(f"  é¢„æµ‹EDPå‡å€¼: {predictions.mean():.3f} kW")
    report.append(f"  é¢„æµ‹EDPæ ‡å‡†å·®: {predictions.std():.3f} kW")
    
    if len(true_values) > 0:
        # ç¡®ä¿ true_values ä¸ predictions é•¿åº¦åŒ¹é…
        true_values = np.array(true_values[:len(predictions)])
        report.append(f"\n  çœŸå®EDPèŒƒå›´: [{true_values.min():.3f}, {true_values.max():.3f}] kW")
        report.append(f"  çœŸå®EDPå‡å€¼: {true_values.mean():.3f} kW")
        
        # è®¡ç®—è¯¯å·®æŒ‡æ ‡
        errors = predictions - true_values
        mae = np.abs(errors).mean()
        rmse = np.sqrt((errors ** 2).mean())
        mape = np.mean(np.abs(errors / (true_values + 1e-8))) * 100
        
        report.append(f"\n  ã€æ€§èƒ½æŒ‡æ ‡ã€‘")
        report.append(f"    MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f} kW")
        report.append(f"    RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f} kW")
        report.append(f"    MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%")
    
    # 3. çŠ¶æ€åˆ†å¸ƒ
    if 'edp_states' in results:
        states = results['edp_states']
        state_counts = pd.Series(states).value_counts()
        
        report.append("\nã€3. EDPçŠ¶æ€åˆ†å¸ƒã€‘")
        total = len(states)
        for state, count in state_counts.items():
            percentage = count / total * 100
            bar = "â–ˆ" * int(percentage / 2)
            report.append(f"  {state:8s}: {count:4d} ({percentage:5.1f}%) {bar}")
    
    # 4. DLPç‰¹å¾åˆ†å¸ƒ
    report.append("\nã€4. æ·±åº¦å­¦ä¹ å‚æ•°(DLP)ç‰¹å¾ã€‘")
    
    if 'cam_clusters' in results:
        cam_clusters = results['cam_clusters']
        cam_counts = pd.Series(cam_clusters).value_counts().sort_index()
        
        report.append("  CAMèšç±»åˆ†å¸ƒ:")
        for cluster, count in cam_counts.items():
            percentage = count / len(cam_clusters) * 100
            report.append(f"    Cluster {cluster}: {count:4d} ({percentage:5.1f}%)")
    
    if 'attention_types' in results:
        attention_types = results['attention_types']
        attention_counts = pd.Series(attention_types).value_counts()
        
        report.append("\n  Attentionç±»å‹åˆ†å¸ƒ:")
        for att_type, count in attention_counts.items():
            percentage = count / len(attention_types) * 100
            report.append(f"    {att_type:10s}: {count:4d} ({percentage:5.1f}%)")
    
    # 5. æ ·æœ¬æ¡ˆä¾‹å±•ç¤º
    report.append("\nã€5. å…¸å‹æ ·æœ¬æ¡ˆä¾‹ã€‘")
    
    # é€‰æ‹©3ä¸ªä»£è¡¨æ€§æ ·æœ¬
    n_samples = len(predictions)
    indices = [0, n_samples // 2, n_samples - 1]
    
    for i, idx in enumerate(indices):
        if idx >= n_samples:
            continue
            
        report.append(f"\n  æ ·æœ¬ {i+1} (ç´¢å¼• {idx}):")
        report.append(f"    é¢„æµ‹EDP: {predictions[idx]:.3f} kW")
        
        if len(true_values) > 0 and idx < len(true_values):
            error = predictions[idx] - true_values[idx]
            report.append(f"    çœŸå®EDP: {true_values[idx]:.3f} kW")
            report.append(f"    è¯¯å·®: {error:+.3f} kW ({error/true_values[idx]*100:+.1f}%)")
        
        if 'edp_states' in results and idx < len(results['edp_states']):
            report.append(f"    çŠ¶æ€: {results['edp_states'][idx]}")
        
        if 'cam_clusters' in results and idx < len(results['cam_clusters']):
            report.append(f"    CAMèšç±»: {results['cam_clusters'][idx]}")
        
        if 'attention_types' in results and idx < len(results['attention_types']):
            report.append(f"    Attention: {results['attention_types'][idx]}")
    
    # 6. é¢„æµ‹åŒºé—´ç»Ÿè®¡
    report.append("\nã€6. é¢„æµ‹åŒºé—´ç»Ÿè®¡ã€‘")
    
    bins = [0, 0.5, 1.0, 1.5, 2.0, 3.0, 10.0]
    labels = ['0-0.5', '0.5-1.0', '1.0-1.5', '1.5-2.0', '2.0-3.0', '>3.0']
    
    pred_binned = pd.cut(predictions, bins=bins, labels=labels, include_lowest=True)
    bin_counts = pred_binned.value_counts().sort_index()
    
    report.append("  é¢„æµ‹EDPåˆ†å¸ƒ (kW):")
    for label, count in bin_counts.items():
        percentage = count / len(predictions) * 100
        bar = "â–“" * int(percentage / 2)
        report.append(f"    {label:10s}: {count:4d} ({percentage:5.1f}%) {bar}")
    
    report.append("\n" + "=" * 80)
    
    return "\n".join(report)


def save_detailed_results(results, output_file):
    """
    ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSVæ–‡ä»¶
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # æ„å»ºè¯¦ç»†ç»“æœDataFrame
    data = {
        'index': range(len(results['predictions'])),
        'predicted_edp': results['predictions'],
    }
    
    if 'true_values' in results and len(results['true_values']) > 0:
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        true_vals = np.array(results['true_values'][:len(results['predictions'])])
        data['true_edp'] = true_vals
        data['error'] = results['predictions'] - true_vals
        data['abs_error'] = np.abs(data['error'])
        data['relative_error'] = data['error'] / (true_vals + 1e-8)
    
    if 'edp_states' in results:
        data['state'] = results['edp_states']
    
    if 'cam_clusters' in results:
        data['cam_cluster'] = results['cam_clusters']
    
    if 'attention_types' in results:
        data['attention_type'] = results['attention_types']
    
    df_results = pd.DataFrame(data)
    
    # ä¿å­˜åˆ°CSV
    df_results.to_csv(output_file, index=False)
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return df_results


def generate_html_reports(results: dict, test_data: pd.DataFrame, output_dir: Path):
    """
    ä¸ºæ¯ä¸ªæ ·æœ¬ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
        test_data: æµ‹è¯•æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
    """
    logger.info("\nç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š...")
    
    visualizer = InferenceVisualizer()
    html_dir = output_dir / 'html_reports'
    html_dir.mkdir(parents=True, exist_ok=True)
    
    predictions = results['predictions']
    true_values = results.get('true_values', [])
    edp_states = results.get('edp_states', [])
    cam_clusters = results.get('cam_clusters', [])
    attention_types = results.get('attention_types', [])
    
    # è®¡ç®—ä¸­ä½æ•°ç”¨äºåç¦»åˆ¤æ–­
    median_value = np.median(predictions)
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬ç”ŸæˆHTML
    num_samples = min(len(predictions), 10)  # åªç”Ÿæˆå‰10ä¸ªæ ·æœ¬çš„HTML
    logger.info(f"å°†ä¸ºå‰{num_samples}ä¸ªæ ·æœ¬ç”ŸæˆHTMLæŠ¥å‘Š")
    
    for idx in range(num_samples):
        # å‡†å¤‡æ ·æœ¬æ•°æ®
        sample_data = {
            'sample_id': idx,
            'window_size': 'N/A',  # å®é™…åº”è¯¥ä»æ•°æ®è·å–
            'target_name': 'Global Active Power (EDP)',
            
            # è¾“å…¥ç‰¹å¾ï¼ˆå–å½“å‰æ—¶åˆ»çš„å€¼ï¼‰
            'input_features': {
                'Global Reactive Power': test_data.iloc[idx]['Global_reactive_power'],
                'Voltage': test_data.iloc[idx]['Voltage'],
                'Global Intensity': test_data.iloc[idx]['Global_intensity'],
            },
            
            # CAMå’ŒAttention
            'cam_cluster': int(cam_clusters[idx]) if idx < len(cam_clusters) else 0,
            'attention_type': attention_types[idx] if idx < len(attention_types) else 'Unknown',
            
            # é¢„æµ‹ç»“æœ
            'prediction': float(predictions[idx]),
            'true_value': float(true_values[idx]) if idx < len(true_values) else 0,
            'error': float(predictions[idx] - true_values[idx]) if idx < len(true_values) else 0,
            'error_percent': float((predictions[idx] - true_values[idx]) / (true_values[idx] + 1e-8) * 100) if idx < len(true_values) else 0,
            
            # çŠ¶æ€
            'state': edp_states[idx] if idx < len(edp_states) else 'Unknown',
            'median_value': float(median_value),
            
            # ç¦»æ•£åŒ–ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼‰
            'discrete_features': {
                'Global Reactive Power': _discretize_value(test_data.iloc[idx]['Global_reactive_power'], 'reactive'),
                'Voltage': _discretize_value(test_data.iloc[idx]['Voltage'], 'voltage'),
                'Global Intensity': _discretize_value(test_data.iloc[idx]['Global_intensity'], 'intensity'),
            }
        }
        
        # ç”ŸæˆHTML
        html_file = html_dir / f'sample_{idx:03d}.html'
        visualizer.generate_html(sample_data, html_file)
        
        if idx == 0:
            logger.info(f"âœ… ç¤ºä¾‹æŠ¥å‘Š: {html_file}")
    
    logger.info(f"âœ… å·²ç”Ÿæˆ {num_samples} ä¸ªHTMLæŠ¥å‘Šåˆ°: {html_dir}")
    
    # ç”Ÿæˆç´¢å¼•é¡µé¢
    _generate_index_page(html_dir, num_samples)
    logger.info(f"âœ… ç´¢å¼•é¡µé¢: {html_dir}/index.html")


def _discretize_value(value: float, feature_type: str) -> str:
    """ç¦»æ•£åŒ–æ•°å€¼"""
    if feature_type == 'reactive':
        if value < 0.05:
            return 'å¾ˆä½'
        elif value < 0.15:
            return 'ä¸­ç­‰'
        else:
            return 'åé«˜'
    elif feature_type == 'voltage':
        if value < 230:
            return 'åä½'
        elif value < 245:
            return 'æ­£å¸¸'
        else:
            return 'åé«˜'
    elif feature_type == 'intensity':
        if value < 5:
            return 'ä½'
        elif value < 15:
            return 'ä¸­ç­‰'
        else:
            return 'é«˜'
    return 'æœªçŸ¥'


def _generate_index_page(html_dir: Path, num_samples: int):
    """ç”Ÿæˆç´¢å¼•é¡µé¢"""
    index_html = f'''
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¨ç†ç»“æœç´¢å¼•</title>
    <style>
        body {{
            font-family: 'Microsoft YaHei', sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 40px;
            margin: 0;
        }}
        .container {{
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            border-radius: 20px;
            padding: 40px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }}
        h1 {{
            text-align: center;
            color: #333;
            margin-bottom: 10px;
        }}
        .subtitle {{
            text-align: center;
            color: #666;
            margin-bottom: 40px;
        }}
        .grid {{
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 20px;
            margin-top: 30px;
        }}
        .card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            text-align: center;
            text-decoration: none;
            transition: all 0.3s;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }}
        .card:hover {{
            transform: translateY(-5px);
            box-shadow: 0 10px 30px rgba(0,0,0,0.3);
        }}
        .card-title {{
            font-size: 1.5em;
            font-weight: bold;
            margin-bottom: 10px;
        }}
        .card-subtitle {{
            font-size: 0.9em;
            opacity: 0.9;
        }}
        .footer {{
            text-align: center;
            margin-top: 40px;
            color: #666;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ”® ç”µåŠ›è´Ÿè·æ™ºèƒ½é¢„æµ‹å¯è§†åŒ–</h1>
        <p class="subtitle">UCIå®¶åº­ç”µåŠ›æ¶ˆè€—æ•°æ®é›† - æ¨ç†ç»“æœæŠ¥å‘Š</p>
        
        <div class="grid">
'''
    
    for i in range(num_samples):
        index_html += f'''
            <a href="sample_{i:03d}.html" class="card">
                <div class="card-title">æ ·æœ¬ #{i}</div>
                <div class="card-subtitle">æŸ¥çœ‹è¯¦ç»†æ¨ç†æµç¨‹</div>
            </a>
'''
    
    index_html += '''
        </div>
        
        <div class="footer">
            <p><strong>Parallel CNN-LSTM-Attention + Causal Inference</strong></p>
            <p>ç”Ÿæˆæ—¶é—´: ''' + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '''</p>
        </div>
    </div>
</body>
</html>
'''
    
    with open(html_dir / 'index.html', 'w', encoding='utf-8') as f:
        f.write(index_html)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='UCIæ•°æ®é›†æ¨ç†æµ‹è¯•')
    parser.add_argument(
        '--model-dir',
        default='outputs/training_uci/models',
        help='æ¨¡å‹ç›®å½•'
    )
    parser.add_argument(
        '--test-data',
        default='data/uci/splits/test.csv',
        help='æµ‹è¯•æ•°æ®æ–‡ä»¶'
    )
    parser.add_argument(
        '--n-samples',
        type=int,
        default=100,
        help='ä½¿ç”¨å¤šå°‘æµ‹è¯•æ ·æœ¬ï¼ˆé»˜è®¤100ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        default='outputs/inference_uci',
        help='è¾“å‡ºç›®å½•'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        logger.info("=" * 80)
        logger.info(" " * 30 + "æ¨ç†æµ‹è¯•å¼€å§‹")
        logger.info("=" * 80)
        
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        test_data = load_test_data(args.test_data, args.n_samples)
        true_values = test_data['EDP'].values
        
        # 2. åˆå§‹åŒ–æ¨ç†æµæ°´çº¿
        logger.info(f"\nåŠ è½½æ¨¡å‹: {args.model_dir}")
        pipeline = InferencePipeline(models_dir=args.model_dir)
        
        # 3. æ‰§è¡Œæ¨ç†
        logger.info(f"\nå¼€å§‹æ¨ç†...")
        results = pipeline.predict(test_data)
        
        # æ·»åŠ çœŸå®å€¼åˆ°ç»“æœä¸­
        results['true_values'] = true_values
        
        # 4. ç”Ÿæˆæ˜“è¯»æŠ¥å‘Š
        logger.info(f"\nç”Ÿæˆæ¨ç†æŠ¥å‘Š...")
        report = format_prediction_results(results)
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + report)
        
        # 5. ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = output_dir / 'inference_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # 6. ä¿å­˜è¯¦ç»†ç»“æœ
        detail_file = output_dir / 'inference_details.csv'
        df_results = save_detailed_results(results, detail_file)
        
        # 7. ä¿å­˜JSONæ ¼å¼ç»“æœ
        json_file = output_dir / 'inference_results.json'
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        true_vals_matched = true_values[:len(results['predictions'])]
        json_results = {
            'predictions': results['predictions'].tolist(),
            'true_values': true_vals_matched.tolist() if isinstance(true_vals_matched, np.ndarray) else list(true_vals_matched),
            'edp_states': list(results.get('edp_states', [])),
            'cam_clusters': [int(x) for x in results.get('cam_clusters', [])],
            'attention_types': list(results.get('attention_types', [])),
            'statistics': {
                'mae': float(np.abs(results['predictions'] - true_vals_matched).mean()),
                'rmse': float(np.sqrt(((results['predictions'] - true_vals_matched) ** 2).mean())),
                'n_samples': len(results['predictions'])
            }
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        logger.info(f"JSONç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
        # 8. ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¨ ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š")
        logger.info("=" * 80)
        generate_html_reports(results, test_data, output_dir)
        
        logger.info("\n" + "=" * 80)
        logger.info(" " * 30 + "æ¨ç†æµ‹è¯•å®Œæˆ")
        logger.info("=" * 80)
        
        # 9. è¿”å›æ€§èƒ½æ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“Š æ€§èƒ½æ‘˜è¦")
        print("=" * 80)
        print(f"MAE:  {json_results['statistics']['mae']:.4f} kW")
        print(f"RMSE: {json_results['statistics']['rmse']:.4f} kW")
        print(f"æ ·æœ¬æ•°: {json_results['statistics']['n_samples']}")
        print("=" * 80)
        print(f"\nğŸ’¡ æŸ¥çœ‹HTMLå¯è§†åŒ–: {output_dir}/html_reports/index.html")
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
