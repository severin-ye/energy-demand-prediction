"""
UCIæ•°æ®æ¨ç†æµ‹è¯•
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆæ˜“è¯»çš„ç»“æœæŠ¥å‘Š
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime
from pathlib import Path

from src.pipeline.inference_pipeline import InferencePipeline
from src.visualization.inference_visualizer import InferenceVisualizer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_test_data(test_file, n_samples=100, min_samples=50):
    """
    åŠ è½½æµ‹è¯•é›†æ•°æ®
    
    Args:
        test_file: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
        n_samples: ä½¿ç”¨å¤šå°‘æ ·æœ¬è¿›è¡Œæµ‹è¯•
        min_samples: æœ€å°æ ·æœ¬æ•°è¦æ±‚ï¼ˆé»˜è®¤50ï¼Œå¯¹åº”sequence_length=20ï¼‰
    """
    logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    df = pd.read_csv(test_file)
    
    # æ£€æŸ¥æ ·æœ¬æ•°æ˜¯å¦è¶³å¤Ÿ
    actual_samples = min(len(df), n_samples)
    if actual_samples < min_samples:
        logger.error(f"\nâŒ æ ·æœ¬æ•°ä¸è¶³ï¼")
        logger.error(f"   å½“å‰æ ·æœ¬æ•°: {actual_samples}")
        logger.error(f"   æœ€å°è¦æ±‚: {min_samples}")
        logger.error(f"\nåŸå› : æ¨¡å‹ä½¿ç”¨åºåˆ—é•¿åº¦=20ï¼Œéœ€è¦è‡³å°‘50ä¸ªæ ·æœ¬æ‰èƒ½ç”Ÿæˆè¶³å¤Ÿçš„åºåˆ—")
        logger.error(f"è®¡ç®—å…¬å¼: åºåˆ—æ•° = æ ·æœ¬æ•° - 20")
        logger.error(f"   â€¢ æ ·æœ¬æ•°=20 â†’ åºåˆ—æ•°=0  âŒ")
        logger.error(f"   â€¢ æ ·æœ¬æ•°=30 â†’ åºåˆ—æ•°=10 âš ï¸")
        logger.error(f"   â€¢ æ ·æœ¬æ•°=50 â†’ åºåˆ—æ•°=30 âœ…")
        logger.error(f"\nè§£å†³æ–¹æ¡ˆ:")
        logger.error(f"   python scripts/run_inference_uci.py \\")
        logger.error(f"     --model-dir {Path(test_file).parent.parent}/training/26-01-16/models \\")
        logger.error(f"     --test-data {test_file} \\")
        logger.error(f"     --n-samples {min_samples}  # æˆ–æ›´å¤š")
        raise ValueError(f"æ ·æœ¬æ•°({actual_samples})å°‘äºæœ€å°è¦æ±‚({min_samples})ï¼Œæ— æ³•ç”Ÿæˆåºåˆ—")
    
    # åªå–å‰n_samplesä¸ªæ ·æœ¬ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦
    if len(df) > n_samples:
        df = df.iloc[:n_samples].copy()
        logger.info(f"ä½¿ç”¨å‰{n_samples}ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†")
    
    # å‡†å¤‡ç‰¹å¾
    feature_cols = ['Global_reactive_power', 'Voltage', 'Global_intensity']
    target_col = 'Global_active_power'
    
    # é‡å‘½åç›®æ ‡åˆ—ä¸ºEDPï¼ˆå…¼å®¹è®­ç»ƒæ—¶çš„å‘½åï¼‰
    df_test = df[feature_cols + [target_col]].copy()
    df_test = df_test.rename(columns={target_col: 'EDP'})
    
    logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {df_test.shape}")
    logger.info(f"EDPèŒƒå›´: [{df_test['EDP'].min():.2f}, {df_test['EDP'].max():.2f}]")
    
    return df_test


def format_prediction_results(results):
    """
    å°†é¢„æµ‹ç»“æœæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æŠ¥å‘Š
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
    """
    report = []
    report.append("=" * 80)
    report.append(" " * 25 + "ğŸ“Š æ¨ç†ç»“æœæŠ¥å‘Š")
    report.append("=" * 80)
    
    # 1. åŸºæœ¬ä¿¡æ¯
    report.append("\nã€1. åŸºæœ¬ä¿¡æ¯ã€‘")
    report.append(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(results['predictions'])} ä¸ª")
    report.append(f"  é¢„æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 2. é¢„æµ‹ç»Ÿè®¡
    predictions = results['predictions']
    true_values = results.get('true_values', [])
    
    report.append("\nã€2. é¢„æµ‹ç»Ÿè®¡ã€‘")
    report.append(f"  é¢„æµ‹EDPèŒƒå›´: [{predictions.min():.3f}, {predictions.max():.3f}] kW")
    report.append(f"  é¢„æµ‹EDPå‡å€¼: {predictions.mean():.3f} kW")
    report.append(f"  é¢„æµ‹EDPæ ‡å‡†å·®: {predictions.std():.3f} kW")
    
    if len(true_values) > 0:
        # ç¡®ä¿ true_values ä¸ predictions é•¿åº¦åŒ¹é…
        true_values = np.array(true_values[:len(predictions)])
        report.append(f"\n  çœŸå®EDPèŒƒå›´: [{true_values.min():.3f}, {true_values.max():.3f}] kW")
        report.append(f"  çœŸå®EDPå‡å€¼: {true_values.mean():.3f} kW")
        
        # è®¡ç®—è¯¯å·®æŒ‡æ ‡
        errors = predictions - true_values
        mae = np.abs(errors).mean()
        rmse = np.sqrt((errors ** 2).mean())
        mape = np.mean(np.abs(errors / (true_values + 1e-8))) * 100
        
        report.append(f"\n  ã€æ€§èƒ½æŒ‡æ ‡ã€‘")
        report.append(f"    MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f} kW")
        report.append(f"    RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f} kW")
        report.append(f"    MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%")
    
    # 3. çŠ¶æ€åˆ†å¸ƒ
    if 'edp_states' in results:
        states = results['edp_states']
        state_counts = pd.Series(states).value_counts()
        
        report.append("\nã€3. EDPçŠ¶æ€åˆ†å¸ƒã€‘")
        total = len(states)
        for state, count in state_counts.items():
            percentage = count / total * 100
            bar = "â–ˆ" * int(percentage / 2)
            report.append(f"  {state:8s}: {count:4d} ({percentage:5.1f}%) {bar}")
    
    # 4. DLPç‰¹å¾åˆ†å¸ƒ
    report.append("\nã€4. æ·±åº¦å­¦ä¹ å‚æ•°(DLP)ç‰¹å¾ã€‘")
    
    if 'cam_clusters' in results:
        cam_clusters = results['cam_clusters']
        cam_counts = pd.Series(cam_clusters).value_counts().sort_index()
        
        report.append("  CAMèšç±»åˆ†å¸ƒ:")
        for cluster, count in cam_counts.items():
            percentage = count / len(cam_clusters) * 100
            report.append(f"    Cluster {cluster}: {count:4d} ({percentage:5.1f}%)")
    
    if 'attention_types' in results:
        attention_types = results['attention_types']
        attention_counts = pd.Series(attention_types).value_counts()
        
        report.append("\n  Attentionç±»å‹åˆ†å¸ƒ:")
        for att_type, count in attention_counts.items():
            percentage = count / len(attention_types) * 100
            report.append(f"    {att_type:10s}: {count:4d} ({percentage:5.1f}%)")
    
    # 5. æ ·æœ¬æ¡ˆä¾‹å±•ç¤º
    report.append("\nã€5. å…¸å‹æ ·æœ¬æ¡ˆä¾‹ã€‘")
    
    # é€‰æ‹©3ä¸ªä»£è¡¨æ€§æ ·æœ¬
    n_samples = len(predictions)
    indices = [0, n_samples // 2, n_samples - 1]
    
    for i, idx in enumerate(indices):
        if idx >= n_samples:
            continue
            
        report.append(f"\n  æ ·æœ¬ {i+1} (ç´¢å¼• {idx}):")
        report.append(f"    é¢„æµ‹EDP: {predictions[idx]:.3f} kW")
        
        if len(true_values) > 0 and idx < len(true_values):
            error = predictions[idx] - true_values[idx]
            report.append(f"    çœŸå®EDP: {true_values[idx]:.3f} kW")
            report.append(f"    è¯¯å·®: {error:+.3f} kW ({error/true_values[idx]*100:+.1f}%)")
        
        if 'edp_states' in results and idx < len(results['edp_states']):
            report.append(f"    çŠ¶æ€: {results['edp_states'][idx]}")
        
        if 'cam_clusters' in results and idx < len(results['cam_clusters']):
            report.append(f"    CAMèšç±»: {results['cam_clusters'][idx]}")
        
        if 'attention_types' in results and idx < len(results['attention_types']):
            report.append(f"    Attention: {results['attention_types'][idx]}")
    
    # 6. é¢„æµ‹åŒºé—´ç»Ÿè®¡
    report.append("\nã€6. é¢„æµ‹åŒºé—´ç»Ÿè®¡ã€‘")
    
    bins = [0, 0.5, 1.0, 1.5, 2.0, 3.0, 10.0]
    labels = ['0-0.5', '0.5-1.0', '1.0-1.5', '1.5-2.0', '2.0-3.0', '>3.0']
    
    pred_binned = pd.cut(predictions, bins=bins, labels=labels, include_lowest=True)
    bin_counts = pred_binned.value_counts().sort_index()
    
    report.append("  é¢„æµ‹EDPåˆ†å¸ƒ (kW):")
    for label, count in bin_counts.items():
        percentage = count / len(predictions) * 100
        bar = "â–“" * int(percentage / 2)
        report.append(f"    {label:10s}: {count:4d} ({percentage:5.1f}%) {bar}")
    
    report.append("\n" + "=" * 80)
    
    return "\n".join(report)


def save_detailed_results(results, output_file):
    """
    ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSVæ–‡ä»¶
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # æ„å»ºè¯¦ç»†ç»“æœDataFrame
    data = {
        'index': range(len(results['predictions'])),
        'predicted_edp': results['predictions'],
    }
    
    if 'true_values' in results and len(results['true_values']) > 0:
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        true_vals = np.array(results['true_values'][:len(results['predictions'])])
        data['true_edp'] = true_vals
        data['error'] = results['predictions'] - true_vals
        data['abs_error'] = np.abs(data['error'])
        data['relative_error'] = data['error'] / (true_vals + 1e-8)
    
    if 'edp_states' in results:
        data['state'] = results['edp_states']
    
    if 'cam_clusters' in results:
        data['cam_cluster'] = results['cam_clusters']
    
    if 'attention_types' in results:
        data['attention_type'] = results['attention_types']
    
    df_results = pd.DataFrame(data)
    
    # ä¿å­˜åˆ°CSV
    df_results.to_csv(output_file, index=False)
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return df_results


def generate_html_reports(results: dict, test_data: pd.DataFrame, output_dir: Path):
    """
    ä¸ºæ¯ä¸ªæ ·æœ¬ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
        test_data: æµ‹è¯•æ•°æ®
        output_dir: è¾“å‡ºç›®å½•
    """
    logger.info("\nç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š...")
    
    visualizer = InferenceVisualizer()
    html_dir = output_dir / 'html_reports'
    html_dir.mkdir(parents=True, exist_ok=True)
    
    predictions = results['predictions']
    true_values = results.get('true_values', [])
    edp_states = results.get('edp_states', [])
    cam_clusters = results.get('cam_clusters', [])
    attention_types = results.get('attention_types', [])
    
    # è®¡ç®—ä¸­ä½æ•°ç”¨äºåç¦»åˆ¤æ–­
    median_value = np.median(predictions)
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬ç”ŸæˆHTML
    num_samples = min(len(predictions), 10)  # åªç”Ÿæˆå‰10ä¸ªæ ·æœ¬çš„HTML
    logger.info(f"å°†ä¸ºå‰{num_samples}ä¸ªæ ·æœ¬ç”ŸæˆHTMLæŠ¥å‘Š")
    
    for idx in range(num_samples):
        # å‡†å¤‡æ ·æœ¬æ•°æ®
        sample_data = {
            'sample_id': idx,
            'window_size': 'N/A',  # å®é™…åº”è¯¥ä»æ•°æ®è·å–
            'target_name': 'Global Active Power (EDP)',
            
            # è¾“å…¥ç‰¹å¾ï¼ˆå–å½“å‰æ—¶åˆ»çš„å€¼ï¼‰
            'input_features': {
                'Global Reactive Power': test_data.iloc[idx]['Global_reactive_power'],
                'Voltage': test_data.iloc[idx]['Voltage'],
                'Global Intensity': test_data.iloc[idx]['Global_intensity'],
            },
            
            # CAMå’ŒAttention
            'cam_cluster': int(cam_clusters[idx]) if idx < len(cam_clusters) else 0,
            'attention_type': attention_types[idx] if idx < len(attention_types) else 'Unknown',
            
            # é¢„æµ‹ç»“æœ
            'prediction': float(predictions[idx]),
            'actual_value': float(true_values[idx]) if idx < len(true_values) else 0,  # ä¿®æ­£å­—æ®µå
            'error': float(predictions[idx] - true_values[idx]) if idx < len(true_values) else 0,
            'error_percent': float((predictions[idx] - true_values[idx]) / (true_values[idx] + 1e-8) * 100) if idx < len(true_values) else 0,
            
            # çŠ¶æ€
            'state': edp_states[idx] if idx < len(edp_states) else 'Unknown',
            'median_value': float(median_value),
            
            # ç¦»æ•£åŒ–ç‰¹å¾ï¼ˆç¤ºä¾‹ï¼‰
            'discrete_features': {
                'Global Reactive Power': _discretize_value(test_data.iloc[idx]['Global_reactive_power'], 'reactive'),
                'Voltage': _discretize_value(test_data.iloc[idx]['Voltage'], 'voltage'),
                'Global Intensity': _discretize_value(test_data.iloc[idx]['Global_intensity'], 'intensity'),
            },
            
            # å› æœåˆ†æè¯´æ˜
            'causal_explanation': _generate_causal_explanation(
                state=edp_states[idx] if idx < len(edp_states) else 'Unknown',
                prediction=float(predictions[idx]),
                actual=float(true_values[idx]) if idx < len(true_values) else 0,
                features=test_data.iloc[idx]
            ),
            
            # ä¼˜åŒ–å»ºè®®
            'recommendations': _generate_recommendations(
                state=edp_states[idx] if idx < len(edp_states) else 'Unknown',
                error_percent=float((predictions[idx] - true_values[idx]) / (true_values[idx] + 1e-8) * 100) if idx < len(true_values) else 0,
                features=test_data.iloc[idx]
            )
        }
        
        # ç”ŸæˆHTML
        html_file = html_dir / f'sample_{idx:03d}.html'
        visualizer.generate_html(sample_data, idx, html_file)
        
        if idx == 0:
            logger.info(f"âœ… ç¤ºä¾‹æŠ¥å‘Š: {html_file}")
    
    logger.info(f"âœ… å·²ç”Ÿæˆ {num_samples} ä¸ªHTMLæŠ¥å‘Šåˆ°: {html_dir}")
    
    # ç”Ÿæˆç´¢å¼•é¡µé¢
    _generate_index_page(html_dir, num_samples)
    logger.info(f"âœ… ç´¢å¼•é¡µé¢: {html_dir}/index.html")


def _discretize_value(value: float, feature_type: str) -> str:
    """ç¦»æ•£åŒ–æ•°å€¼"""
    if feature_type == 'reactive':
        if value < 0.05:
            return 'å¾ˆä½'
        elif value < 0.15:
            return 'ä¸­ç­‰'
        else:
            return 'åé«˜'
    elif feature_type == 'voltage':
        if value < 230:
            return 'åä½'
        elif value < 245:
            return 'æ­£å¸¸'
        else:
            return 'åé«˜'
    elif feature_type == 'intensity':
        if value < 5:
            return 'ä½'
        elif value < 15:
            return 'ä¸­ç­‰'
        else:
            return 'é«˜'
    return 'æœªçŸ¥'


def _generate_causal_explanation(state: str, prediction: float, actual: float, features) -> str:
    """ç”Ÿæˆå› æœåˆ†æè¯´æ˜"""
    voltage = features['Voltage']
    reactive = features['Global_reactive_power']
    intensity = features['Global_intensity']
    
    explanations = []
    
    # çŠ¶æ€åˆ¤æ–­é€»è¾‘
    if state == 'Peak':
        explanations.append(f"<strong>è´Ÿè·å³°å€¼çŠ¶æ€</strong>: é¢„æµ‹åŠŸç‡ä¸º {prediction:.3f} kWï¼Œé«˜äºæ­£å¸¸æ°´å¹³")
        if voltage > 240:
            explanations.append(f"â€¢ ç”µå‹åé«˜ ({voltage:.1f}V)ï¼Œå¯èƒ½å­˜åœ¨ç”µç½‘æ³¢åŠ¨")
        if intensity > 10:
            explanations.append(f"â€¢ ç”µæµå¼ºåº¦è¾ƒå¤§ ({intensity:.1f}A)ï¼Œè®¾å¤‡è´Ÿè½½è¾ƒé‡")
    elif state == 'Lower':
        explanations.append(f"<strong>ä½è´Ÿè·çŠ¶æ€</strong>: é¢„æµ‹åŠŸç‡ä¸º {prediction:.3f} kWï¼Œå¤„äºè¾ƒä½æ°´å¹³")
        if voltage < 235:
            explanations.append(f"â€¢ ç”µå‹åä½ ({voltage:.1f}V)ï¼Œç”¨ç”µè´Ÿè·è¾ƒå°")
        if intensity < 3:
            explanations.append(f"â€¢ ç”µæµå¼ºåº¦è¾ƒå° ({intensity:.1f}A)ï¼Œè®¾å¤‡ä½¿ç”¨è¾ƒå°‘")
    else:
        explanations.append(f"<strong>æ­£å¸¸è´Ÿè·çŠ¶æ€</strong>: é¢„æµ‹åŠŸç‡ä¸º {prediction:.3f} kW")
    
    # æ— åŠŸåŠŸç‡åˆ†æ
    if reactive > 0.2:
        explanations.append(f"â€¢ æ— åŠŸåŠŸç‡è¾ƒé«˜ ({reactive:.3f} kW)ï¼Œå­˜åœ¨æ„Ÿæ€§è´Ÿè½½")
    elif reactive < 0.05:
        explanations.append(f"â€¢ æ— åŠŸåŠŸç‡å¾ˆä½ ({reactive:.3f} kW)ï¼Œè´Ÿè½½ä¸»è¦ä¸ºé˜»æ€§")
    
    # é¢„æµ‹å‡†ç¡®æ€§
    if actual > 0:
        error_pct = abs(prediction - actual) / actual * 100
        if error_pct < 10:
            explanations.append(f"â€¢ é¢„æµ‹è¯¯å·® {error_pct:.1f}%ï¼Œå‡†ç¡®åº¦è¾ƒé«˜")
        elif error_pct < 30:
            explanations.append(f"â€¢ é¢„æµ‹è¯¯å·® {error_pct:.1f}%ï¼Œå‡†ç¡®åº¦ä¸­ç­‰")
        else:
            explanations.append(f"â€¢ é¢„æµ‹è¯¯å·® {error_pct:.1f}%ï¼Œå­˜åœ¨ä¸€å®šåå·®")
    
    return '<br>'.join(explanations) if explanations else 'å½“å‰æ•°æ®æ­£å¸¸ï¼Œæ— å¼‚å¸¸å› ç´ '


def _generate_recommendations(state: str, error_percent: float, features) -> list:
    """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
    recommendations = []
    
    voltage = features['Voltage']
    reactive = features['Global_reactive_power']
    intensity = features['Global_intensity']
    
    # åŸºäºçŠ¶æ€çš„å»ºè®®
    if state == 'Peak':
        recommendations.append({
            'action': 'å‰Šå³°å¡«è°·',
            'explanation': 'å½“å‰å¤„äºè´Ÿè·å³°å€¼ï¼Œå»ºè®®è°ƒæ•´ç”¨ç”µæ—¶æ®µï¼Œé¿å¼€é«˜å³°æœŸ',
            'expected_impact': 'é™ä½10-20%ç”¨ç”µæˆæœ¬'
        })
        if intensity > 15:
            recommendations.append({
                'action': 'æ£€æŸ¥å¤§åŠŸç‡è®¾å¤‡',
                'explanation': f'ç”µæµå¼ºåº¦è¾¾åˆ° {intensity:.1f}Aï¼Œå»ºè®®æ£€æŸ¥æ˜¯å¦æœ‰å¤§åŠŸç‡è®¾å¤‡åŒæ—¶è¿è¡Œ',
                'expected_impact': 'é¿å…è¿‡è½½é£é™©'
            })
    
    # é¢„æµ‹è¯¯å·®è¾ƒå¤§æ—¶çš„å»ºè®®
    if abs(error_percent) > 50:
        recommendations.append({
            'action': 'æ¨¡å‹ä¼˜åŒ–',
            'explanation': f'é¢„æµ‹è¯¯å·®è¾ƒå¤§ ({abs(error_percent):.1f}%)ï¼Œå»ºè®®ï¼š1) å¢åŠ ç±»ä¼¼åœºæ™¯è®­ç»ƒæ ·æœ¬ 2) æ£€æŸ¥æ•°æ®è´¨é‡',
            'expected_impact': 'æå‡é¢„æµ‹å‡†ç¡®åº¦20-30%'
        })
    elif abs(error_percent) > 30:
        recommendations.append({
            'action': 'æ•°æ®æ ¡éªŒ',
            'explanation': f'é¢„æµ‹è¯¯å·® {abs(error_percent):.1f}%ï¼Œå»ºè®®æ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦å­˜åœ¨å¼‚å¸¸å€¼',
            'expected_impact': 'æå‡é¢„æµ‹ç¨³å®šæ€§'
        })
    
    # ç”µå‹ç›¸å…³å»ºè®®
    if voltage < 220:
        recommendations.append({
            'action': 'ç”µå‹ç›‘æµ‹ - æ¬ å‹',
            'explanation': f'ç”µå‹è¿‡ä½ ({voltage:.1f}V < 220V)ï¼Œå¯èƒ½å½±å“è®¾å¤‡æ­£å¸¸è¿è¡Œ',
            'expected_impact': 'ä¿éšœç”¨ç”µå®‰å…¨'
        })
    elif voltage > 250:
        recommendations.append({
            'action': 'ç”µå‹ç›‘æµ‹ - è¿‡å‹',
            'explanation': f'ç”µå‹è¿‡é«˜ ({voltage:.1f}V > 250V)ï¼Œå»ºè®®è”ç³»ä¾›ç”µéƒ¨é—¨',
            'expected_impact': 'ä¿éšœè®¾å¤‡å®‰å…¨'
        })
    
    # æ— åŠŸåŠŸç‡å»ºè®®
    if reactive > 0.3:
        recommendations.append({
            'action': 'åŠŸç‡å› æ•°è¡¥å¿',
            'explanation': f'æ— åŠŸåŠŸç‡è¾ƒé«˜ ({reactive:.3f} kW)ï¼Œå»ºè®®å®‰è£…è¡¥å¿ç”µå®¹å™¨',
            'expected_impact': 'é™ä½5-10%ç”µè´¹'
        })
    
    # å¦‚æœæ²¡æœ‰ç‰¹æ®Šå»ºè®®
    if not recommendations:
        if abs(error_percent) < 20:
            recommendations.append({
                'action': 'ä¿æŒç°çŠ¶',
                'explanation': 'å½“å‰ç”¨ç”µæ¨¡å¼åˆç†ï¼Œé¢„æµ‹å‡†ç¡®åº¦è‰¯å¥½',
                'expected_impact': 'æŒç»­ç¨³å®šè¿è¡Œ'
            })
        else:
            recommendations.append({
                'action': 'æŒç»­ç›‘æµ‹',
                'explanation': 'å»ºè®®æŒç»­è§‚å¯Ÿç”¨ç”µæ¨¡å¼ï¼Œæ”¶é›†æ›´å¤šæ•°æ®',
                'expected_impact': 'ä¼˜åŒ–é¢„æµ‹æ¨¡å‹'
            })
    
    return recommendations


def _generate_index_page(html_dir: Path, num_samples: int):
    """ç”Ÿæˆç®€æ´ç´¢å¼•é¡µé¢"""
    index_html = f'''
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¨ç†ç»“æœç´¢å¼•</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Microsoft YaHei', sans-serif;
            background: #f5f5f5;
            padding: 30px;
            line-height: 1.6;
        }}
        .container {{
            max-width: 1000px;
            margin: 0 auto;
            background: white;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            overflow: hidden;
        }}
        .header {{
            background: #2c3e50;
            color: white;
            padding: 30px;
            border-bottom: 3px solid #3498db;
        }}
        h1 {{
            font-size: 1.8em;
            font-weight: 600;
            margin-bottom: 8px;
        }}
        .subtitle {{
            font-size: 0.95em;
            opacity: 0.85;
        }}
        .content {{
            padding: 30px;
        }}
        .stats {{
            background: #ecf0f1;
            padding: 15px 20px;
            border-radius: 4px;
            margin-bottom: 25px;
            font-size: 0.95em;
            color: #2c3e50;
        }}
        .grid {{
            display: grid;
            grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));
            gap: 15px;
        }}
        .card {{
            background: white;
            border: 2px solid #ecf0f1;
            border-radius: 6px;
            padding: 20px;
            text-align: center;
            text-decoration: none;
            transition: all 0.2s;
            color: #2c3e50;
        }}
        .card:hover {{
            border-color: #3498db;
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(52,152,219,0.15);
        }}
        .card-title {{
            font-size: 1.3em;
            font-weight: 600;
            margin-bottom: 6px;
            color: #3498db;
        }}
        .card-subtitle {{
            font-size: 0.85em;
            color: #7f8c8d;
        }}
        .footer {{
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #ecf0f1;
            text-align: center;
            font-size: 0.9em;
            color: #7f8c8d;
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ç”µåŠ›è´Ÿè·é¢„æµ‹ - æ¨ç†ç»“æœ</h1>
            <p class="subtitle">UCIå®¶åº­ç”µåŠ›æ¶ˆè€—æ•°æ®é›†</p>
        </div>
        
        <div class="content">
            <div class="stats">
                <strong>æ¨¡å‹:</strong> Parallel CNN-LSTM-Attention + å› æœæ¨ç† &nbsp;|&nbsp; 
                <strong>æ ·æœ¬æ€»æ•°:</strong> {num_samples} ä¸ª
            </div>
            
            <div class="grid">
'''
    
    for i in range(num_samples):
        index_html += f'''
            <a href="sample_{i:03d}.html" class="card">
                <div class="card-title">#{i}</div>
                <div class="card-subtitle">æ ·æœ¬åˆ†æ</div>
            </a>
'''
    
    index_html += '''
            </div>
            
            <div class="footer">
                <p>ç”Ÿæˆæ—¶é—´: ''' + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + '''</p>
            </div>
        </div>
    </div>
</body>
</html>
'''
    
    with open(html_dir / 'index.html', 'w', encoding='utf-8') as f:
        f.write(index_html)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='UCIæ•°æ®é›†æ¨ç†æµ‹è¯•')
    parser.add_argument(
        '--model-dir',
        default='outputs/training_uci/models',
        help='æ¨¡å‹ç›®å½•'
    )
    parser.add_argument(
        '--test-data',
        default='data/uci/splits/test.csv',
        help='æµ‹è¯•æ•°æ®æ–‡ä»¶'
    )
    parser.add_argument(
        '--n-samples',
        type=int,
        default=100,
        help='ä½¿ç”¨å¤šå°‘æµ‹è¯•æ ·æœ¬ï¼ˆé»˜è®¤100ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        default=None,
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: outputs/inference/æ¨¡å‹å/æ—¶é—´ï¼‰'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œä½¿ç”¨ outputs/inference/æ¨¡å‹å/æ—¶é—´/ æ ¼å¼
    if args.output_dir is None:
        # ä»æ¨¡å‹ç›®å½•æå–æ¨¡å‹åç§°
        model_dir_path = Path(args.model_dir)
        if model_dir_path.parent.name == 'models':
            # å¦‚æœæ˜¯ xxx/modelsï¼Œå–ä¸Šä¸€çº§ç›®å½•å
            model_name = model_dir_path.parent.parent.name
        else:
            model_name = model_dir_path.parent.name
        
        timestamp = datetime.now().strftime('%y-%m-%d_%H-%M')
        args.output_dir = f'outputs/inference/{model_name}/{timestamp}'
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        logger.info("=" * 80)
        logger.info(" " * 30 + "æ¨ç†æµ‹è¯•å¼€å§‹")
        logger.info("=" * 80)
        
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        test_data = load_test_data(args.test_data, args.n_samples)
        true_values = test_data['EDP'].values
        
        # 2. åˆå§‹åŒ–æ¨ç†æµæ°´çº¿
        logger.info(f"\nåŠ è½½æ¨¡å‹: {args.model_dir}")
        pipeline = InferencePipeline(models_dir=args.model_dir)
        
        # 3. æ‰§è¡Œæ¨ç†
        logger.info(f"\nå¼€å§‹æ¨ç†...")
        results = pipeline.predict(test_data)
        
        # æ·»åŠ çœŸå®å€¼åˆ°ç»“æœä¸­
        results['true_values'] = true_values
        
        # 4. ç”Ÿæˆæ˜“è¯»æŠ¥å‘Š
        logger.info(f"\nç”Ÿæˆæ¨ç†æŠ¥å‘Š...")
        report = format_prediction_results(results)
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + report)
        
        # 5. ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = output_dir / 'inference_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # 6. ä¿å­˜è¯¦ç»†ç»“æœ
        detail_file = output_dir / 'inference_details.csv'
        df_results = save_detailed_results(results, detail_file)
        
        # 7. ä¿å­˜JSONæ ¼å¼ç»“æœ
        json_file = output_dir / 'inference_results.json'
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        true_vals_matched = true_values[:len(results['predictions'])]
        json_results = {
            'predictions': results['predictions'].tolist(),
            'true_values': true_vals_matched.tolist() if isinstance(true_vals_matched, np.ndarray) else list(true_vals_matched),
            'edp_states': list(results.get('edp_states', [])),
            'cam_clusters': [int(x) for x in results.get('cam_clusters', [])],
            'attention_types': list(results.get('attention_types', [])),
            'statistics': {
                'mae': float(np.abs(results['predictions'] - true_vals_matched).mean()),
                'rmse': float(np.sqrt(((results['predictions'] - true_vals_matched) ** 2).mean())),
                'n_samples': len(results['predictions'])
            }
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        logger.info(f"JSONç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
        # 8. ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ¨ ç”ŸæˆHTMLå¯è§†åŒ–æŠ¥å‘Š")
        logger.info("=" * 80)
        generate_html_reports(results, test_data, output_dir)
        
        logger.info("\n" + "=" * 80)
        logger.info(" " * 30 + "æ¨ç†æµ‹è¯•å®Œæˆ")
        logger.info("=" * 80)
        
        # 9. è¿”å›æ€§èƒ½æ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“Š æ€§èƒ½æ‘˜è¦")
        print("=" * 80)
        print(f"MAE:  {json_results['statistics']['mae']:.4f} kW")
        print(f"RMSE: {json_results['statistics']['rmse']:.4f} kW")
        print(f"æ ·æœ¬æ•°: {json_results['statistics']['n_samples']}")
        print("=" * 80)
        print(f"\nğŸ’¡ æŸ¥çœ‹HTMLå¯è§†åŒ–: {output_dir}/html_reports/index.html")
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
