"""
UCIæ•°æ®æ¨ç†æµ‹è¯•
ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ç”Ÿæˆæ˜“è¯»çš„ç»“æœæŠ¥å‘Š
"""
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime
from pathlib import Path

from src.pipeline.inference_pipeline import InferencePipeline

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_test_data(test_file, n_samples=100):
    """
    åŠ è½½æµ‹è¯•é›†æ•°æ®
    
    Args:
        test_file: æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„
        n_samples: ä½¿ç”¨å¤šå°‘æ ·æœ¬è¿›è¡Œæµ‹è¯•
    """
    logger.info(f"åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    df = pd.read_csv(test_file)
    
    # åªå–å‰n_samplesä¸ªæ ·æœ¬ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦
    if len(df) > n_samples:
        df = df.iloc[:n_samples].copy()
        logger.info(f"ä½¿ç”¨å‰{n_samples}ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†")
    
    # å‡†å¤‡ç‰¹å¾
    feature_cols = ['Global_reactive_power', 'Voltage', 'Global_intensity']
    target_col = 'Global_active_power'
    
    # é‡å‘½åç›®æ ‡åˆ—ä¸ºEDPï¼ˆå…¼å®¹è®­ç»ƒæ—¶çš„å‘½åï¼‰
    df_test = df[feature_cols + [target_col]].copy()
    df_test = df_test.rename(columns={target_col: 'EDP'})
    
    logger.info(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {df_test.shape}")
    logger.info(f"EDPèŒƒå›´: [{df_test['EDP'].min():.2f}, {df_test['EDP'].max():.2f}]")
    
    return df_test


def format_prediction_results(results):
    """
    å°†é¢„æµ‹ç»“æœæ ¼å¼åŒ–ä¸ºæ˜“è¯»çš„æŠ¥å‘Š
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
    """
    report = []
    report.append("=" * 80)
    report.append(" " * 25 + "ğŸ“Š æ¨ç†ç»“æœæŠ¥å‘Š")
    report.append("=" * 80)
    
    # 1. åŸºæœ¬ä¿¡æ¯
    report.append("\nã€1. åŸºæœ¬ä¿¡æ¯ã€‘")
    report.append(f"  æµ‹è¯•æ ·æœ¬æ•°: {len(results['predictions'])} ä¸ª")
    report.append(f"  é¢„æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 2. é¢„æµ‹ç»Ÿè®¡
    predictions = results['predictions']
    true_values = results.get('true_values', [])
    
    report.append("\nã€2. é¢„æµ‹ç»Ÿè®¡ã€‘")
    report.append(f"  é¢„æµ‹EDPèŒƒå›´: [{predictions.min():.3f}, {predictions.max():.3f}] kW")
    report.append(f"  é¢„æµ‹EDPå‡å€¼: {predictions.mean():.3f} kW")
    report.append(f"  é¢„æµ‹EDPæ ‡å‡†å·®: {predictions.std():.3f} kW")
    
    if len(true_values) > 0:
        # ç¡®ä¿ true_values ä¸ predictions é•¿åº¦åŒ¹é…
        true_values = np.array(true_values[:len(predictions)])
        report.append(f"\n  çœŸå®EDPèŒƒå›´: [{true_values.min():.3f}, {true_values.max():.3f}] kW")
        report.append(f"  çœŸå®EDPå‡å€¼: {true_values.mean():.3f} kW")
        
        # è®¡ç®—è¯¯å·®æŒ‡æ ‡
        errors = predictions - true_values
        mae = np.abs(errors).mean()
        rmse = np.sqrt((errors ** 2).mean())
        mape = np.mean(np.abs(errors / (true_values + 1e-8))) * 100
        
        report.append(f"\n  ã€æ€§èƒ½æŒ‡æ ‡ã€‘")
        report.append(f"    MAE (å¹³å‡ç»å¯¹è¯¯å·®): {mae:.4f} kW")
        report.append(f"    RMSE (å‡æ–¹æ ¹è¯¯å·®): {rmse:.4f} kW")
        report.append(f"    MAPE (å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®): {mape:.2f}%")
    
    # 3. çŠ¶æ€åˆ†å¸ƒ
    if 'edp_states' in results:
        states = results['edp_states']
        state_counts = pd.Series(states).value_counts()
        
        report.append("\nã€3. EDPçŠ¶æ€åˆ†å¸ƒã€‘")
        total = len(states)
        for state, count in state_counts.items():
            percentage = count / total * 100
            bar = "â–ˆ" * int(percentage / 2)
            report.append(f"  {state:8s}: {count:4d} ({percentage:5.1f}%) {bar}")
    
    # 4. DLPç‰¹å¾åˆ†å¸ƒ
    report.append("\nã€4. æ·±åº¦å­¦ä¹ å‚æ•°(DLP)ç‰¹å¾ã€‘")
    
    if 'cam_clusters' in results:
        cam_clusters = results['cam_clusters']
        cam_counts = pd.Series(cam_clusters).value_counts().sort_index()
        
        report.append("  CAMèšç±»åˆ†å¸ƒ:")
        for cluster, count in cam_counts.items():
            percentage = count / len(cam_clusters) * 100
            report.append(f"    Cluster {cluster}: {count:4d} ({percentage:5.1f}%)")
    
    if 'attention_types' in results:
        attention_types = results['attention_types']
        attention_counts = pd.Series(attention_types).value_counts()
        
        report.append("\n  Attentionç±»å‹åˆ†å¸ƒ:")
        for att_type, count in attention_counts.items():
            percentage = count / len(attention_types) * 100
            report.append(f"    {att_type:10s}: {count:4d} ({percentage:5.1f}%)")
    
    # 5. æ ·æœ¬æ¡ˆä¾‹å±•ç¤º
    report.append("\nã€5. å…¸å‹æ ·æœ¬æ¡ˆä¾‹ã€‘")
    
    # é€‰æ‹©3ä¸ªä»£è¡¨æ€§æ ·æœ¬
    n_samples = len(predictions)
    indices = [0, n_samples // 2, n_samples - 1]
    
    for i, idx in enumerate(indices):
        if idx >= n_samples:
            continue
            
        report.append(f"\n  æ ·æœ¬ {i+1} (ç´¢å¼• {idx}):")
        report.append(f"    é¢„æµ‹EDP: {predictions[idx]:.3f} kW")
        
        if len(true_values) > 0 and idx < len(true_values):
            error = predictions[idx] - true_values[idx]
            report.append(f"    çœŸå®EDP: {true_values[idx]:.3f} kW")
            report.append(f"    è¯¯å·®: {error:+.3f} kW ({error/true_values[idx]*100:+.1f}%)")
        
        if 'edp_states' in results and idx < len(results['edp_states']):
            report.append(f"    çŠ¶æ€: {results['edp_states'][idx]}")
        
        if 'cam_clusters' in results and idx < len(results['cam_clusters']):
            report.append(f"    CAMèšç±»: {results['cam_clusters'][idx]}")
        
        if 'attention_types' in results and idx < len(results['attention_types']):
            report.append(f"    Attention: {results['attention_types'][idx]}")
    
    # 6. é¢„æµ‹åŒºé—´ç»Ÿè®¡
    report.append("\nã€6. é¢„æµ‹åŒºé—´ç»Ÿè®¡ã€‘")
    
    bins = [0, 0.5, 1.0, 1.5, 2.0, 3.0, 10.0]
    labels = ['0-0.5', '0.5-1.0', '1.0-1.5', '1.5-2.0', '2.0-3.0', '>3.0']
    
    pred_binned = pd.cut(predictions, bins=bins, labels=labels, include_lowest=True)
    bin_counts = pred_binned.value_counts().sort_index()
    
    report.append("  é¢„æµ‹EDPåˆ†å¸ƒ (kW):")
    for label, count in bin_counts.items():
        percentage = count / len(predictions) * 100
        bar = "â–“" * int(percentage / 2)
        report.append(f"    {label:10s}: {count:4d} ({percentage:5.1f}%) {bar}")
    
    report.append("\n" + "=" * 80)
    
    return "\n".join(report)


def save_detailed_results(results, output_file):
    """
    ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSVæ–‡ä»¶
    
    Args:
        results: æ¨ç†ç»“æœå­—å…¸
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # æ„å»ºè¯¦ç»†ç»“æœDataFrame
    data = {
        'index': range(len(results['predictions'])),
        'predicted_edp': results['predictions'],
    }
    
    if 'true_values' in results and len(results['true_values']) > 0:
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        true_vals = np.array(results['true_values'][:len(results['predictions'])])
        data['true_edp'] = true_vals
        data['error'] = results['predictions'] - true_vals
        data['abs_error'] = np.abs(data['error'])
        data['relative_error'] = data['error'] / (true_vals + 1e-8)
    
    if 'edp_states' in results:
        data['state'] = results['edp_states']
    
    if 'cam_clusters' in results:
        data['cam_cluster'] = results['cam_clusters']
    
    if 'attention_types' in results:
        data['attention_type'] = results['attention_types']
    
    df_results = pd.DataFrame(data)
    
    # ä¿å­˜åˆ°CSV
    df_results.to_csv(output_file, index=False)
    logger.info(f"è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    return df_results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='UCIæ•°æ®é›†æ¨ç†æµ‹è¯•')
    parser.add_argument(
        '--model-dir',
        default='outputs/training_uci/models',
        help='æ¨¡å‹ç›®å½•'
    )
    parser.add_argument(
        '--test-data',
        default='data/uci/splits/test.csv',
        help='æµ‹è¯•æ•°æ®æ–‡ä»¶'
    )
    parser.add_argument(
        '--n-samples',
        type=int,
        default=100,
        help='ä½¿ç”¨å¤šå°‘æµ‹è¯•æ ·æœ¬ï¼ˆé»˜è®¤100ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        default='outputs/inference_uci',
        help='è¾“å‡ºç›®å½•'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        logger.info("=" * 80)
        logger.info(" " * 30 + "æ¨ç†æµ‹è¯•å¼€å§‹")
        logger.info("=" * 80)
        
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        test_data = load_test_data(args.test_data, args.n_samples)
        true_values = test_data['EDP'].values
        
        # 2. åˆå§‹åŒ–æ¨ç†æµæ°´çº¿
        logger.info(f"\nåŠ è½½æ¨¡å‹: {args.model_dir}")
        pipeline = InferencePipeline(models_dir=args.model_dir)
        
        # 3. æ‰§è¡Œæ¨ç†
        logger.info(f"\nå¼€å§‹æ¨ç†...")
        results = pipeline.predict(test_data)
        
        # æ·»åŠ çœŸå®å€¼åˆ°ç»“æœä¸­
        results['true_values'] = true_values
        
        # 4. ç”Ÿæˆæ˜“è¯»æŠ¥å‘Š
        logger.info(f"\nç”Ÿæˆæ¨ç†æŠ¥å‘Š...")
        report = format_prediction_results(results)
        
        # æ‰“å°åˆ°æ§åˆ¶å°
        print("\n" + report)
        
        # 5. ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = output_dir / 'inference_report.txt'
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        logger.info(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # 6. ä¿å­˜è¯¦ç»†ç»“æœ
        detail_file = output_dir / 'inference_details.csv'
        df_results = save_detailed_results(results, detail_file)
        
        # 7. ä¿å­˜JSONæ ¼å¼ç»“æœ
        json_file = output_dir / 'inference_results.json'
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        true_vals_matched = true_values[:len(results['predictions'])]
        json_results = {
            'predictions': results['predictions'].tolist(),
            'true_values': true_vals_matched.tolist() if isinstance(true_vals_matched, np.ndarray) else list(true_vals_matched),
            'edp_states': list(results.get('edp_states', [])),
            'cam_clusters': [int(x) for x in results.get('cam_clusters', [])],
            'attention_types': list(results.get('attention_types', [])),
            'statistics': {
                'mae': float(np.abs(results['predictions'] - true_vals_matched).mean()),
                'rmse': float(np.sqrt(((results['predictions'] - true_vals_matched) ** 2).mean())),
                'n_samples': len(results['predictions'])
            }
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, indent=2, ensure_ascii=False)
        logger.info(f"JSONç»“æœå·²ä¿å­˜åˆ°: {json_file}")
        
        logger.info("\n" + "=" * 80)
        logger.info(" " * 30 + "æ¨ç†æµ‹è¯•å®Œæˆ")
        logger.info("=" * 80)
        
        # 8. è¿”å›æ€§èƒ½æ‘˜è¦
        print("\n" + "=" * 80)
        print("ğŸ“Š æ€§èƒ½æ‘˜è¦")
        print("=" * 80)
        print(f"MAE:  {json_results['statistics']['mae']:.4f} kW")
        print(f"RMSE: {json_results['statistics']['rmse']:.4f} kW")
        print(f"æ ·æœ¬æ•°: {json_results['statistics']['n_samples']}")
        print("=" * 80)
        
    except Exception as e:
        logger.error(f"\nâŒ æ¨ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
