"""
è°ƒè¯•è„šæœ¬ï¼šæ£€æŸ¥å¹¶è¡Œå’Œä¸²è¡Œæ¨¡å‹çš„æ¶æ„å·®å¼‚
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import tensorflow as tf
from src.models.predictor import ParallelCNNLSTMAttention
from src.models.baseline_models import SerialCNNLSTM, SerialCNNLSTMAttention

np.random.seed(42)
tf.random.set_seed(42)

print("=" * 80)
print("æ¨¡å‹æ¶æ„å¯¹æ¯”åˆ†æ")
print("=" * 80)

# åˆ›å»ºç¤ºä¾‹è¾“å…¥
input_shape = (80, 7)
sample_input = np.random.randn(1, 80, 7).astype(np.float32)

# 1. ä¸²è”CNN-LSTM (Baseline)
print("\n[1] ä¸²è”CNN-LSTM (Baseline)")
print("-" * 80)
serial = SerialCNNLSTM(input_shape=input_shape, cnn_filters=64, lstm_units=128, dense_units=[64, 32])
serial.model.summary()

# æµ‹è¯•ç»´åº¦å˜åŒ–
print("\nç»´åº¦å˜åŒ–è¿½è¸ª:")
print(f"è¾“å…¥: {sample_input.shape}")

# æ‰‹åŠ¨è¿½è¸ªCNNè¾“å‡º
from tensorflow.keras import layers
test_input = layers.Input(shape=input_shape)
x = layers.Conv1D(64, 3, padding='same', activation='relu')(test_input)
print(f"Conv1D(64) å: {x.shape}")
x = layers.MaxPooling1D(2)(x)
print(f"MaxPool(2) å: {x.shape}")
x = layers.Conv1D(128, 3, padding='same', activation='relu')(x)
print(f"Conv1D(128) å: {x.shape}")
x = layers.MaxPooling1D(2)(x)
print(f"MaxPool(2) å (CNNè¾“å‡º): {x.shape}")

lstm_out = layers.LSTM(128, return_sequences=False)(x)
print(f"LSTM(128, return_sequences=False) å: {lstm_out.shape}")

# 2. ä¸²è”CNN-LSTM-Attention
print("\n" + "=" * 80)
print("[2] ä¸²è”CNN-LSTM-Attention")
print("-" * 80)
serial_att = SerialCNNLSTMAttention(input_shape=input_shape, cnn_filters=64, lstm_units=128, attention_units=64, dense_units=[64, 32])
serial_att.model.summary()

print("\nç»´åº¦å˜åŒ–è¿½è¸ª:")
print(f"è¾“å…¥: {sample_input.shape}")
print(f"CNNè¾“å‡º: (batch, 20, 128)")
lstm_out_seq = layers.LSTM(128, return_sequences=True)(x)
print(f"LSTM(128, return_sequences=True) å: {lstm_out_seq.shape}")
print(f"Attention å: (batch, 128)")

# 3. å¹¶è¡ŒCNN-LSTM-Attention
print("\n" + "=" * 80)
print("[3] å¹¶è¡ŒCNN-LSTM-Attention")
print("-" * 80)
parallel = ParallelCNNLSTMAttention(input_shape=input_shape, cnn_filters=64, lstm_units=128, attention_units=64, dense_units=[64, 32])
parallel.model.summary()

print("\nç»´åº¦å˜åŒ–è¿½è¸ª:")
print(f"è¾“å…¥: {sample_input.shape}")
print("\nCNNåˆ†æ”¯:")
print(f"  CNNè¾“å‡º: (batch, 20, 128)")
print(f"  Flattenå: (batch, 2560)")
print("\nLSTMåˆ†æ”¯:")
print(f"  è¾“å…¥: (batch, 80, 7)  â† æ³¨æ„ï¼ä»åŸå§‹è¾“å…¥")
lstm_from_raw = layers.LSTM(128, return_sequences=True)(test_input)
print(f"  LSTM(128, return_sequences=True) å: {lstm_from_raw.shape}")
print(f"  Attention å: (batch, 128)")
print("\nç‰¹å¾èåˆ:")
print(f"  Concatenate([2560, 128]) = (batch, 2688)")

# 4. å…³é”®å·®å¼‚åˆ†æ
print("\n" + "=" * 80)
print("å…³é”®å·®å¼‚åˆ†æ")
print("=" * 80)

print("\nğŸ” **é—®é¢˜å‘ç°**ï¼š")
print("\nä¸²è”æ¨¡å‹:")
print("  LSTMè¾“å…¥: CNNå¤„ç†åçš„ç‰¹å¾ (batch, 20, 128)")
print("  - åºåˆ—é•¿åº¦: 20 (å·²è¢«CNNæ± åŒ–ç¼©çŸ­)")
print("  - ç‰¹å¾ç»´åº¦: 128 (CNNæå–çš„é«˜çº§ç‰¹å¾)")
print("  - LSTMå­¦ä¹ : CNNç‰¹å¾ä¹‹é—´çš„æ—¶åºå…³ç³»")

print("\nå¹¶è¡Œæ¨¡å‹:")
print("  LSTMè¾“å…¥: åŸå§‹è¾“å…¥ (batch, 80, 7)")
print("  - åºåˆ—é•¿åº¦: 80 (å®Œæ•´çš„æ—¶é—´æ­¥)")
print("  - ç‰¹å¾ç»´åº¦: 7 (åŸå§‹ç‰¹å¾)")
print("  - LSTMå­¦ä¹ : åŸå§‹æ•°æ®çš„æ—¶åºå…³ç³»")

print("\nâŒ **æ½œåœ¨é—®é¢˜**ï¼š")
print("1. LSTMå¤„ç†çš„åºåˆ—é•¿åº¦ä¸åŒï¼š")
print("   - ä¸²è”: 20æ­¥ (æ›´å®¹æ˜“å­¦ä¹ çŸ­æœŸä¾èµ–)")
print("   - å¹¶è¡Œ: 80æ­¥ (æ›´éš¾å­¦ä¹ é•¿æœŸä¾èµ–)")

print("\n2. LSTMè¾“å…¥çš„ä¿¡æ¯å¯†åº¦ä¸åŒï¼š")
print("   - ä¸²è”: 128ç»´é«˜çº§ç‰¹å¾ (CNNå·²æå–æ¨¡å¼)")
print("   - å¹¶è¡Œ: 7ç»´åŸå§‹ç‰¹å¾ (éœ€è¦LSTMè‡ªå·±æå–)")

print("\n3. å‚æ•°åˆ©ç”¨æ•ˆç‡ï¼š")
print("   - ä¸²è”LSTM: åœ¨CNNç‰¹å¾åŸºç¡€ä¸Šå·¥ä½œï¼Œæ›´é«˜æ•ˆ")
print("   - å¹¶è¡ŒLSTM: éœ€è¦ä»å¤´å­¦ä¹ ï¼Œä»»åŠ¡æ›´é‡")

print("\nğŸ’¡ **è®ºæ–‡çš„çœŸæ­£æ„å›¾å¯èƒ½æ˜¯**ï¼š")
print("   å¹¶è¡Œç»“æ„åº”è¯¥è®©CNNå’ŒLSTMåœ¨**ä¸åŒæŠ½è±¡å±‚æ¬¡**ä¸Šå·¥ä½œï¼Œ")
print("   è€Œä¸æ˜¯è®©å®ƒä»¬å¤„ç†**å®Œå…¨ä¸åŒçš„è¾“å…¥é•¿åº¦å’Œç‰¹å¾**ã€‚")
print("=" * 80)
