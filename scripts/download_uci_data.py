"""
ä½¿ç”¨ucimlrepo Python APIä¸‹è½½UCI Householdæ•°æ®é›†

æ•°æ®é›†ä¿¡æ¯:
- åç§°: Individual Household Electric Power Consumption
- ID: 235
- æ ·æœ¬æ•°: 2,075,259
- æ—¶é—´èŒƒå›´: 2006/12/16 - 2010/11/26
"""
import os
import sys
import logging
import pandas as pd
import numpy as np
from pathlib import Path

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def download_with_progress(url, output_path):
    """å¸¦è¿›åº¦æ¡çš„ä¸‹è½½"""
    import urllib.request
    
    def progress_hook(count, block_size, total_size):
        if total_size > 0:
            percent = min(100, count * block_size * 100 / total_size)
            downloaded = count * block_size / (1024 * 1024)
            total = total_size / (1024 * 1024)
            print(f"\rä¸‹è½½è¿›åº¦: {percent:.1f}% ({downloaded:.1f}/{total:.1f} MB)", end='', flush=True)
        else:
            downloaded = count * block_size / (1024 * 1024)
            print(f"\rå·²ä¸‹è½½: {downloaded:.1f} MB", end='', flush=True)
    
    try:
        urllib.request.urlretrieve(url, output_path, progress_hook)
        print()  # æ¢è¡Œ
    except Exception as e:
        print()  # æ¢è¡Œ
        raise


def download_uci_dataset(output_dir='data/raw', method='direct'):
    """
    ä¸‹è½½UCIæ•°æ®é›†
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        method: 'api' ä½¿ç”¨ucimlrepo, 'direct' ç›´æ¥ä¸‹è½½ZIP
    
    Returns:
        DataFrame: åŠ è½½çš„æ•°æ®é›†
    """
    os.makedirs(output_dir, exist_ok=True)
    
    if method == 'api':
        logger.info("ä½¿ç”¨ucimlrepo Python APIä¸‹è½½...")
        
        try:
            from ucimlrepo import fetch_ucirepo
        except ImportError:
            logger.error("âŒ ucimlrepoæœªå®‰è£…")
            logger.info("å®‰è£…å‘½ä»¤: pip install ucimlrepo")
            sys.exit(1)
        
        # è·å–æ•°æ®é›†
        logger.info("æ­£åœ¨ä»UCIä»“åº“è·å–æ•°æ®é›† ID=235...")
        dataset = fetch_ucirepo(id=235)
        
        # æå–æ•°æ®
        logger.info("âœ… æ•°æ®é›†ä¸‹è½½æˆåŠŸ")
        
        # æ˜¾ç¤ºå…ƒæ•°æ®
        logger.info("\n" + "="*70)
        logger.info("æ•°æ®é›†å…ƒæ•°æ®")
        logger.info("="*70)
        if hasattr(dataset, 'metadata'):
            for key, value in dataset.metadata.items():
                if key in ['name', 'num_instances', 'num_features', 'area', 'task']:
                    logger.info(f"{key}: {value}")
        
        # ç»„åˆç‰¹å¾å’Œç›®æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
        X = dataset.data.features
        
        logger.info(f"\nç‰¹å¾æ•°æ®å½¢çŠ¶: {X.shape}")
        logger.info(f"åˆ—å: {X.columns.tolist()}")
        
        # ä¿å­˜åŸå§‹æ•°æ®
        output_path = os.path.join(output_dir, 'uci_household_raw.csv')
        X.to_csv(output_path, index=False)
        logger.info(f"âœ… ä¿å­˜åŸå§‹æ•°æ®: {output_path}")
        
        return X
        
    elif method == 'direct':
        logger.info("ä½¿ç”¨ç›´æ¥ä¸‹è½½æ–¹å¼...")
        import zipfile
        
        # ä¸‹è½½URL
        url = 'https://archive.ics.uci.edu/static/public/235/individual+household+electric+power+consumption.zip'
        zip_path = os.path.join(output_dir, 'uci_household.zip')
        txt_path = os.path.join(output_dir, 'household_power_consumption.txt')
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(txt_path):
            logger.info(f"âœ… æ•°æ®æ–‡ä»¶å·²å­˜åœ¨: {txt_path}")
            return load_txt_dataset(txt_path)
        
        # ä¸‹è½½
        logger.info(f"å¼€å§‹ä¸‹è½½: {url}")
        logger.info("æ–‡ä»¶å¤§å°: ~126.8 MBï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        try:
            download_with_progress(url, zip_path)
            logger.info(f"âœ… ä¸‹è½½å®Œæˆ: {zip_path}")
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            logger.info("æç¤º: å¦‚æœä¸‹è½½è¶…æ—¶ï¼Œå¯ä»¥æ‰‹åŠ¨ä»ä»¥ä¸‹åœ°å€ä¸‹è½½:")
            logger.info(f"  {url}")
            logger.info(f"  ç„¶åæ”¾åˆ°: {zip_path}")
            raise
        
        # è§£å‹
        logger.info("è§£å‹æ–‡ä»¶...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(output_dir)
        logger.info(f"âœ… è§£å‹å®Œæˆ")
        
        # åˆ é™¤zip
        os.remove(zip_path)
        logger.info("æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        
        # åŠ è½½æ•°æ®
        return load_txt_dataset(txt_path)


def load_txt_dataset(filepath):
    """åŠ è½½TXTæ ¼å¼çš„UCIæ•°æ®é›†"""
    logger.info(f"åŠ è½½æ•°æ®: {filepath}")
    
    # è¯»å–æ•°æ®ï¼ˆåˆ†å·åˆ†éš”ï¼Œé—®å·è¡¨ç¤ºç¼ºå¤±å€¼ï¼‰
    df = pd.read_csv(
        filepath,
        sep=';',
        na_values=['?'],
        low_memory=False
    )
    
    # è§£ææ—¥æœŸæ—¶é—´
    df['datetime'] = pd.to_datetime(
        df['Date'] + ' ' + df['Time'],
        format='%d/%m/%Y %H:%M:%S'
    )
    
    # åˆ é™¤åŸå§‹Dateå’ŒTimeåˆ—
    df = df.drop(['Date', 'Time'], axis=1)
    
    # é‡æ–°æ’åºåˆ—ï¼ˆdatetimeæ”¾ç¬¬ä¸€åˆ—ï¼‰
    cols = ['datetime'] + [col for col in df.columns if col != 'datetime']
    df = df[cols]
    
    logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: {df.shape}")
    
    return df


def analyze_dataset(df):
    """åˆ†ææ•°æ®é›†åŸºæœ¬ä¿¡æ¯"""
    logger.info("\n" + "="*70)
    logger.info("æ•°æ®é›†åˆ†æ")
    logger.info("="*70)
    
    logger.info(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
    logger.info(f"  æ ·æœ¬æ•°: {len(df):,}")
    logger.info(f"  ç‰¹å¾æ•°: {len(df.columns)}")
    
    # æ£€æŸ¥æ—¶é—´èŒƒå›´
    if 'datetime' in df.columns:
        logger.info(f"  æ—¶é—´èŒƒå›´: {df['datetime'].min()} ~ {df['datetime'].max()}")
        time_span = df['datetime'].max() - df['datetime'].min()
        logger.info(f"  æ—¶é•¿: {time_span.days} å¤© ({time_span.days/30.5:.1f} ä¸ªæœˆ)")
    
    # ç¼ºå¤±å€¼ç»Ÿè®¡
    missing = df.isnull().sum()
    total_missing = missing.sum()
    missing_pct = total_missing / df.size * 100
    
    logger.info(f"\nâš ï¸  ç¼ºå¤±å€¼:")
    logger.info(f"  æ€»è®¡: {total_missing:,} ({missing_pct:.2f}%)")
    if total_missing > 0:
        logger.info(f"  å„åˆ—ç¼ºå¤±:")
        for col, count in missing[missing > 0].items():
            pct = count / len(df) * 100
            logger.info(f"    {col}: {count:,} ({pct:.2f}%)")
    
    # æ•°å€¼å‹ç‰¹å¾ç»Ÿè®¡
    logger.info(f"\nğŸ“ˆ æ•°å€¼ç»Ÿè®¡:")
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    stats = df[numeric_cols].describe()
    logger.info(f"\n{stats.to_string()}")
    
    # æ˜¾ç¤ºå‰å‡ è¡Œ
    logger.info(f"\nğŸ“‹ å‰5è¡Œæ•°æ®:")
    logger.info(f"\n{df.head().to_string()}")


def preprocess_for_training(df, output_path='data/processed/uci_household_clean.csv'):
    """
    é¢„å¤„ç†æ•°æ®ç”¨äºè®­ç»ƒ
    
    å¤„ç†æ­¥éª¤:
    1. å¤„ç†ç¼ºå¤±å€¼
    2. é‡é‡‡æ ·åˆ°15åˆ†é’Ÿï¼ˆè®ºæ–‡ä¸­ä½¿ç”¨çš„é¢‘ç‡ï¼‰
    3. ç‰¹å¾å·¥ç¨‹
    4. ä¿å­˜æ¸…æ´—åçš„æ•°æ®
    """
    logger.info("\n" + "="*70)
    logger.info("æ•°æ®é¢„å¤„ç†")
    logger.info("="*70)
    
    df_clean = df.copy()
    
    # 1. å¤„ç†ç¼ºå¤±å€¼
    logger.info("\n1ï¸âƒ£ å¤„ç†ç¼ºå¤±å€¼...")
    initial_missing = df_clean.isnull().sum().sum()
    
    # å‰å‘å¡«å……
    df_clean = df_clean.fillna(method='ffill')
    # åå‘å¡«å……ï¼ˆå¤„ç†å¼€å¤´çš„ç¼ºå¤±ï¼‰
    df_clean = df_clean.fillna(method='bfill')
    
    remaining_missing = df_clean.isnull().sum().sum()
    logger.info(f"  å¤„ç†å‰: {initial_missing:,} ç¼ºå¤±å€¼")
    logger.info(f"  å¤„ç†å: {remaining_missing:,} ç¼ºå¤±å€¼")
    
    # 2. è®¾ç½®æ—¶é—´ç´¢å¼•å¹¶é‡é‡‡æ ·
    if 'datetime' in df_clean.columns:
        logger.info("\n2ï¸âƒ£ é‡é‡‡æ ·åˆ°15åˆ†é’Ÿ...")
        df_clean = df_clean.set_index('datetime')
        
        # é‡é‡‡æ ·ï¼ˆè®ºæ–‡ä¸­ä½¿ç”¨15åˆ†é’Ÿï¼‰
        df_clean = df_clean.resample('15T').mean()
        logger.info(f"  é‡é‡‡æ ·åæ ·æœ¬æ•°: {len(df_clean):,}")
        
        # é‡ç½®ç´¢å¼•
        df_clean = df_clean.reset_index()
    
    # 3. ç‰¹å¾å·¥ç¨‹
    logger.info("\n3ï¸âƒ£ ç‰¹å¾å·¥ç¨‹...")
    if 'datetime' in df_clean.columns:
        df_clean['hour'] = df_clean['datetime'].dt.hour
        df_clean['day_of_week'] = df_clean['datetime'].dt.dayofweek
        df_clean['month'] = df_clean['datetime'].dt.month
        df_clean['is_weekend'] = df_clean['day_of_week'].isin([5, 6]).astype(int)
        logger.info(f"  æ·»åŠ æ—¶é—´ç‰¹å¾: hour, day_of_week, month, is_weekend")
    
    # 4. ä¿å­˜
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df_clean.to_csv(output_path, index=False)
    logger.info(f"\nâœ… ä¿å­˜æ¸…æ´—æ•°æ®: {output_path}")
    logger.info(f"  æœ€ç»ˆå½¢çŠ¶: {df_clean.shape}")
    
    return df_clean


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¸‹è½½UCI Householdæ•°æ®é›†')
    parser.add_argument(
        '--method',
        choices=['api', 'direct'],
        default='api',
        help='ä¸‹è½½æ–¹å¼: api=ucimlrepoåŒ…, direct=ç›´æ¥ä¸‹è½½ZIP'
    )
    parser.add_argument(
        '--output-dir',
        default='data/raw',
        help='åŸå§‹æ•°æ®è¾“å‡ºç›®å½•'
    )
    parser.add_argument(
        '--preprocess',
        action='store_true',
        help='æ˜¯å¦è¿›è¡Œé¢„å¤„ç†'
    )
    parser.add_argument(
        '--processed-output',
        default='data/processed/uci_household_clean.csv',
        help='é¢„å¤„ç†åçš„æ•°æ®è¾“å‡ºè·¯å¾„'
    )
    
    args = parser.parse_args()
    
    try:
        # ä¸‹è½½æ•°æ®
        df = download_uci_dataset(
            output_dir=args.output_dir,
            method=args.method
        )
        
        # åˆ†ææ•°æ®
        analyze_dataset(df)
        
        # é¢„å¤„ç†
        if args.preprocess:
            df_clean = preprocess_for_training(
                df,
                output_path=args.processed_output
            )
            
            logger.info("\n" + "="*70)
            logger.info("âœ… å…¨éƒ¨å®Œæˆï¼")
            logger.info("="*70)
            logger.info(f"åŸå§‹æ•°æ®: {args.output_dir}/uci_household_raw.csv")
            logger.info(f"æ¸…æ´—æ•°æ®: {args.processed_output}")
            logger.info("\nå¯ä»¥ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®è¿›è¡Œè®­ç»ƒ:")
            logger.info(f"  python scripts/run_training.py --data {args.processed_output}")
        
    except Exception as e:
        logger.error(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
