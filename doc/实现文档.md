# 能源需求预测的因果可解释AI系统 - 实现文档

## 1. 项目结构

```
energy-causal-ai/
├── README.md                    # 项目说明
├── requirements.txt             # 依赖包
├── config/
│   ├── config.yaml             # 主配置文件
│   ├── domain_config.yaml      # 领域知识配置
│   └── model_params.yaml       # 模型超参数
├── data/
│   ├── raw/                    # 原始数据
│   ├── processed/              # 预处理后数据
│   └── interim/                # 中间数据
├── src/
│   ├── __init__.py
│   ├── data/
│   │   ├── __init__.py
│   │   ├── data_loader.py      # 数据加载
│   │   ├── preprocessor.py     # 数据预处理
│   │   └── discretizer.py      # 离散化模块
│   ├── models/
│   │   ├── __init__.py
│   │   ├── predictor.py        # 预测模型（CNN+LSTM+Att）
│   │   ├── state_classifier.py # 状态分类（Sn）
│   │   ├── clustering.py       # CAM/Attention聚类
│   │   ├── association.py      # 关联规则挖掘
│   │   └── bayesian_net.py     # 贝叶斯网络
│   ├── inference/
│   │   ├── __init__.py
│   │   ├── causal_inference.py # 因果推断
│   │   └── recommendation.py   # 建议生成
│   ├── utils/
│   │   ├── __init__.py
│   │   ├── metrics.py          # 评估指标
│   │   ├── visualization.py    # 可视化
│   │   └── logger.py           # 日志
│   └── pipeline/
│       ├── __init__.py
│       ├── train_pipeline.py   # 训练流水线
│       └── infer_pipeline.py   # 推理流水线
├── notebooks/
│   ├── 01_data_exploration.ipynb
│   ├── 02_model_training.ipynb
│   └── 03_causal_analysis.ipynb
├── tests/
│   ├── test_data.py
│   ├── test_models.py
│   └── test_inference.py
├── models/                      # 保存的模型
│   ├── predictor.h5
│   ├── bn_peak.pkl
│   ├── bn_normal.pkl
│   └── bn_lower.pkl
├── outputs/                     # 输出结果
│   ├── predictions/
│   ├── explanations/
│   └── visualizations/
└── docs/                        # 文档
    ├── 项目设计文档.md
    ├── 实现文档.md（本文档）
    └── API文档.md
```

---

## 2. 核心模块详细实现

### 2.1 数据预处理模块

#### 文件: `src/data/preprocessor.py`

```python
"""
数据预处理模块
功能：
1. 数据清洗
2. 时间特征提取
3. 滑动窗口构造
4. 标准化
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from typing import Tuple, List
import logging

logger = logging.getLogger(__name__)


class EnergyDataPreprocessor:
    """能源数据预处理器"""
    
    def __init__(self, config: dict):
        """
        参数:
            config: 配置字典，包含：
                - window_size: 窗口大小（默认80）
                - pred_horizon: 预测步长（默认1）
                - time_resolution: 时间分辨率（'15min', '30min', '1h', '1d'）
                - feature_cols: 特征列名列表
        """
        self.window_size = config.get('window_size', 80)
        self.pred_horizon = config.get('pred_horizon', 1)
        self.time_resolution = config.get('time_resolution', '15min')
        self.feature_cols = config.get('feature_cols', [])
        
        self.scaler = StandardScaler()
        self.is_fitted = False
        
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        数据清洗
        
        步骤:
        1. 处理缺失值（前向填充 + 后向填充）
        2. 移除异常值（使用IQR方法）
        3. 确保时间戳连续
        
        输入:
            df: 原始数据框，必须包含时间戳列
        
        输出:
            清洗后的数据框
        """
        logger.info("开始数据清洗...")
        
        # 1. 确保时间戳为datetime类型
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df = df.set_index('timestamp')
        
        # 2. 重采样到指定时间分辨率（确保时间连续）
        df = df.resample(self.time_resolution).mean()
        
        # 3. 处理缺失值
        # 先前向填充，再后向填充
        df = df.fillna(method='ffill').fillna(method='bfill')
        
        # 如果还有缺失值，用列均值填充
        df = df.fillna(df.mean())
        
        # 4. 移除异常值（保守策略：仅移除极端异常）
        for col in self.feature_cols:
            if col in df.columns:
                # 计算IQR
                Q1 = df[col].quantile(0.01)  # 使用1%和99%分位点（更保守）
                Q3 = df[col].quantile(0.99)
                IQR = Q3 - Q1
                
                # 定义异常值边界
                lower_bound = Q1 - 3 * IQR  # 3倍IQR（更宽松）
                upper_bound = Q3 + 3 * IQR
                
                # 截断（clip）而非删除
                df[col] = df[col].clip(lower=lower_bound, upper=upper_bound)
        
        logger.info(f"数据清洗完成。最终数据量: {len(df)}")
        return df
    
    def extract_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        提取时间特征
        
        特征:
        - Date: 日期（1-31）
        - Day: 星期几（0-6，0为周一）
        - Month: 月份（1-12）
        - Season: 季节（0-3，0=春，1=夏，2=秋，3=冬）
        - Weekend: 是否周末（0或1）
        - Hour: 小时（如果分辨率 < 1天）
        
        输入:
            df: 带时间索引的数据框
        
        输出:
            添加了时间特征的数据框
        """
        logger.info("提取时间特征...")
        
        # 基础时间特征
        df['Date'] = df.index.day
        df['Day'] = df.index.dayofweek  # 0=周一, 6=周日
        df['Month'] = df.index.month
        
        # 季节特征（北半球）
        # 春:3-5, 夏:6-8, 秋:9-11, 冬:12-2
        df['Season'] = ((df['Month'] % 12 + 3) // 3) % 4
        
        # 周末标识
        df['Weekend'] = (df['Day'] >= 5).astype(int)  # 5=周六, 6=周日
        
        # 如果分辨率小于1天，添加小时特征
        if self.time_resolution in ['15min', '30min', '1h']:
            df['Hour'] = df.index.hour
        
        logger.info(f"时间特征提取完成。新增特征: Date, Day, Month, Season, Weekend")
        return df
    
    def create_sequences(self, 
                         df: pd.DataFrame,
                         target_col: str = 'Global_active_power'
                         ) -> Tuple[np.ndarray, np.ndarray]:
        """
        创建滑动窗口序列
        
        输入:
            df: 预处理后的数据框
            target_col: 目标变量列名
        
        输出:
            X: 形状为 [样本数, 窗口大小, 特征数] 的输入序列
            y: 形状为 [样本数,] 的目标值
        
        示例:
            window_size=80, pred_horizon=1
            样本0: X[0] = df[0:80], y[0] = df[80][target_col]
            样本1: X[1] = df[1:81], y[1] = df[81][target_col]
        """
        logger.info("创建滑动窗口序列...")
        
        # 选择特征列
        feature_df = df[self.feature_cols].copy()
        target_series = df[target_col].values
        
        # 标准化特征
        if not self.is_fitted:
            feature_values = self.scaler.fit_transform(feature_df)
            self.is_fitted = True
        else:
            feature_values = self.scaler.transform(feature_df)
        
        # 构造序列
        X_list = []
        y_list = []
        
        for i in range(len(feature_values) - self.window_size - self.pred_horizon + 1):
            # 输入窗口: [i, i+window_size)
            X_window = feature_values[i:i+self.window_size]
            
            # 目标值: i+window_size+pred_horizon-1
            y_target = target_series[i + self.window_size + self.pred_horizon - 1]
            
            X_list.append(X_window)
            y_list.append(y_target)
        
        X = np.array(X_list)  # [样本数, 窗口, 特征]
        y = np.array(y_list)  # [样本数,]
        
        logger.info(f"序列创建完成。X形状: {X.shape}, y形状: {y.shape}")
        return X, y
    
    def split_data(self, 
                   X: np.ndarray, 
                   y: np.ndarray,
                   train_ratio: float = 0.7,
                   val_ratio: float = 0.15
                   ) -> Tuple:
        """
        分割训练集、验证集、测试集
        
        注意: 时间序列不能打乱，必须按时间顺序分割
        
        输入:
            X, y: 完整数据集
            train_ratio: 训练集比例
            val_ratio: 验证集比例（测试集 = 1 - train_ratio - val_ratio）
        
        输出:
            (X_train, y_train, X_val, y_val, X_test, y_test)
        """
        n_samples = len(X)
        
        train_end = int(n_samples * train_ratio)
        val_end = int(n_samples * (train_ratio + val_ratio))
        
        X_train = X[:train_end]
        y_train = y[:train_end]
        
        X_val = X[train_end:val_end]
        y_val = y[train_end:val_end]
        
        X_test = X[val_end:]
        y_test = y[val_end:]
        
        logger.info(f"数据分割完成:")
        logger.info(f"  训练集: {len(X_train)} 样本")
        logger.info(f"  验证集: {len(X_val)} 样本")
        logger.info(f"  测试集: {len(X_test)} 样本")
        
        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def preprocess_pipeline(self, 
                            df: pd.DataFrame,
                            target_col: str = 'Global_active_power'
                            ) -> Tuple:
        """
        完整预处理流水线
        
        步骤:
        1. 数据清洗
        2. 时间特征提取
        3. 创建序列
        4. 数据分割
        
        输入:
            df: 原始数据框
            target_col: 目标列名
        
        输出:
            训练集、验证集、测试集的X和y
        """
        # 1. 清洗
        df_clean = self.clean_data(df)
        
        # 2. 时间特征
        df_features = self.extract_time_features(df_clean)
        
        # 3. 创建序列
        X, y = self.create_sequences(df_features, target_col)
        
        # 4. 分割
        return self.split_data(X, y)


# 使用示例
if __name__ == "__main__":
    # 配置
    config = {
        'window_size': 80,
        'pred_horizon': 1,
        'time_resolution': '15min',
        'feature_cols': [
            'Global_active_power',
            'Global_reactive_power',
            'Voltage',
            'Global_intensity',
            'Sub_metering_1',  # 厨房
            'Sub_metering_2',  # 洗衣
            'Sub_metering_3',  # 空调
            # 时间特征将在extract_time_features中添加
        ]
    }
    
    # 加载数据
    df = pd.read_csv('data/raw/household_power_consumption.txt',
                     sep=';',
                     parse_dates={'timestamp': ['Date', 'Time']},
                     na_values=['?'])
    
    # 预处理
    preprocessor = EnergyDataPreprocessor(config)
    X_train, y_train, X_val, y_val, X_test, y_test = \
        preprocessor.preprocess_pipeline(df)
    
    print(f"预处理完成！")
    print(f"X_train.shape: {X_train.shape}")
    print(f"y_train.shape: {y_train.shape}")
```

---

### 2.2 预测模型实现

#### 文件: `src/models/predictor.py`

```python
"""
并行CNN-LSTM-Attention预测模型
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
import numpy as np
from typing import Tuple, Dict
import logging

logger = logging.getLogger(__name__)


class ParallelCNNLSTMAttention(Model):
    """
    并行CNN-LSTM-Attention架构
    
    结构:
    - CNN分支: 提取局部时序模式
    - LSTM+Attention分支: 建模长期依赖并定位关键时间
    - 融合: 拼接后通过MLP回归
    """
    
    def __init__(self, 
                 window_size: int = 80,
                 n_features: int = 10,
                 cnn_filters: list = [64, 128],
                 lstm_units: int = 128,
                 mlp_units: list = [256, 128],
                 dropout_rate: float = 0.3):
        """
        参数:
            window_size: 输入窗口大小
            n_features: 特征数量
            cnn_filters: CNN各层的卷积核数量
            lstm_units: LSTM隐藏单元数
            mlp_units: MLP各层神经元数
            dropout_rate: Dropout比例
        """
        super(ParallelCNNLSTMAttention, self).__init__()
        
        self.window_size = window_size
        self.n_features = n_features
        
        # ===== CNN分支 =====
        self.conv1 = layers.Conv1D(filters=cnn_filters[0],
                                    kernel_size=3,
                                    activation='relu',
                                    padding='same',
                                    name='conv1')
        self.pool1 = layers.MaxPooling1D(pool_size=2, name='pool1')
        
        self.conv2 = layers.Conv1D(filters=cnn_filters[1],
                                    kernel_size=3,
                                    activation='relu',
                                    padding='same',
                                    name='conv2')
        self.pool2 = layers.MaxPooling1D(pool_size=2, name='pool2')
        
        # 全局平均池化（用于生成CAM）
        self.global_avg_pool = layers.GlobalAveragePooling1D(name='gap')
        
        # ===== LSTM+Attention分支 =====
        self.lstm = layers.LSTM(units=lstm_units,
                                return_sequences=True,  # 返回所有时间步
                                name='lstm')
        
        # 注意力层（自定义）
        self.attention = AttentionLayer(name='attention')
        
        # ===== 融合与回归 =====
        self.flatten = layers.Flatten(name='flatten')
        
        self.fc1 = layers.Dense(mlp_units[0], activation='relu', name='fc1')
        self.dropout1 = layers.Dropout(dropout_rate, name='dropout1')
        
        self.fc2 = layers.Dense(mlp_units[1], activation='relu', name='fc2')
        self.dropout2 = layers.Dropout(dropout_rate, name='dropout2')
        
        self.output_layer = layers.Dense(1, activation='linear', name='output')
        
    def call(self, inputs, training=False, return_attention=False):
        """
        前向传播
        
        输入:
            inputs: [batch, window_size, n_features]
            training: 是否训练模式
            return_attention: 是否返回注意力权重（用于解释）
        
        输出:
            如果return_attention=False: 预测值 [batch, 1]
            如果return_attention=True: (预测值, CAM, attention_weights)
        """
        # ===== CNN分支 =====
        x_cnn = self.conv1(inputs)  # [batch, window_size, 64]
        x_cnn = self.pool1(x_cnn)   # [batch, window_size/2, 64]
        
        x_cnn = self.conv2(x_cnn)   # [batch, window_size/4, 128]
        x_cnn_pooled = self.pool2(x_cnn)  # [batch, window_size/4, 128]
        
        # 生成CAM（在最后一个卷积层）
        # CAM = 每个时间步在各特征维度的激活强度
        cam = tf.reduce_mean(x_cnn, axis=-1)  # [batch, window_size/4]
        
        # 全局池化
        cnn_features = self.global_avg_pool(x_cnn_pooled)  # [batch, 128]
        
        # ===== LSTM+Attention分支 =====
        lstm_out = self.lstm(inputs, training=training)  # [batch, window, 128]
        
        # 注意力
        context, attention_weights = self.attention(lstm_out)  # context: [batch, 128]
        
        # ===== 融合 =====
        # 拼接CNN特征和LSTM上下文
        merged = tf.concat([cnn_features, context], axis=-1)  # [batch, 256]
        
        # MLP回归
        x = self.fc1(merged)
        x = self.dropout1(x, training=training)
        
        x = self.fc2(x)
        x = self.dropout2(x, training=training)
        
        prediction = self.output_layer(x)  # [batch, 1]
        
        if return_attention:
            return prediction, cam, attention_weights
        else:
            return prediction
    
    def get_config(self):
        """保存模型配置（用于序列化）"""
        return {
            'window_size': self.window_size,
            'n_features': self.n_features
        }


class AttentionLayer(layers.Layer):
    """
    自定义注意力层
    
    计算公式:
    score_k = fc(h_k)
    a_k = exp(score_k) / Σexp(score_k)  # softmax
    context = Σ(a_k × h_k)
    """
    
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)
        
    def build(self, input_shape):
        """
        输入形状: [batch, sequence_length, hidden_dim]
        """
        self.hidden_dim = input_shape[-1]
        
        # 注意力权重计算的全连接层
        self.W = self.add_weight(name='attention_weight',
                                 shape=(self.hidden_dim, 1),
                                 initializer='glorot_uniform',
                                 trainable=True)
        self.b = self.add_weight(name='attention_bias',
                                 shape=(1,),
                                 initializer='zeros',
                                 trainable=True)
        
        super(AttentionLayer, self).build(input_shape)
    
    def call(self, inputs):
        """
        输入:
            inputs: LSTM输出 [batch, sequence, hidden]
        
        输出:
            context: 上下文向量 [batch, hidden]
            attention_weights: 注意力权重 [batch, sequence]
        """
        # 计算注意力分数
        # [batch, sequence, hidden] × [hidden, 1] = [batch, sequence, 1]
        score = tf.matmul(inputs, self.W) + self.b
        score = tf.squeeze(score, axis=-1)  # [batch, sequence]
        
        # Softmax归一化
        attention_weights = tf.nn.softmax(score, axis=-1)  # [batch, sequence]
        
        # 加权求和
        # [batch, sequence, 1] × [batch, sequence, hidden] = [batch, sequence, hidden]
        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)
        context = tf.reduce_sum(inputs * attention_weights_expanded, axis=1)  # [batch, hidden]
        
        return context, attention_weights
    
    def get_config(self):
        return super(AttentionLayer, self).get_config()


# ===== 模型构建与训练 =====
def build_model(config: dict) -> ParallelCNNLSTMAttention:
    """
    根据配置构建模型
    
    配置示例:
    {
        'window_size': 80,
        'n_features': 10,
        'cnn_filters': [64, 128],
        'lstm_units': 128,
        'mlp_units': [256, 128],
        'dropout_rate': 0.3
    }
    """
    model = ParallelCNNLSTMAttention(**config)
    return model


def train_model(model: ParallelCNNLSTMAttention,
                X_train: np.ndarray,
                y_train: np.ndarray,
                X_val: np.ndarray,
                y_val: np.ndarray,
                epochs: int = 100,
                batch_size: int = 64,
                learning_rate: float = 0.001) -> Dict:
    """
    训练模型
    
    输入:
        model: 模型实例
        X_train, y_train: 训练数据
        X_val, y_val: 验证数据
        epochs: 训练轮数
        batch_size: 批大小
        learning_rate: 学习率
    
    输出:
        训练历史字典
    """
    # 编译模型
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='mse',
        metrics=['mae', 'mse']
    )
    
    # 回调函数
    callbacks = [
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=15,
            restore_best_weights=True
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6
        ),
        keras.callbacks.ModelCheckpoint(
            filepath='models/predictor_best.h5',
            monitor='val_loss',
            save_best_only=True
        )
    ]
    
    # 训练
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1
    )
    
    return history.history


# 使用示例
if __name__ == "__main__":
    # 模型配置
    config = {
        'window_size': 80,
        'n_features': 10,
        'cnn_filters': [64, 128],
        'lstm_units': 128,
        'mlp_units': [256, 128],
        'dropout_rate': 0.3
    }
    
    # 构建模型
    model = build_model(config)
    
    # 假设已有数据
    # X_train, y_train, X_val, y_val = ...
    
    # 训练
    # history = train_model(model, X_train, y_train, X_val, y_val)
    
    # 预测（带解释）
    # predictions, cam, attention = model(X_test, return_attention=True)
    
    print("模型构建完成！")
    model.summary()
```

---

### 2.3 状态分类模块（Sn估计）

#### 文件: `src/models/state_classifier.py`

```python
"""
基于Sn尺度估计的状态分类器
"""

import numpy as np
from typing import Tuple, Union
import logging

logger = logging.getLogger(__name__)


class SnStateClassifier:
    """
    Sn鲁棒尺度估计器
    用于将连续预测值映射为3类状态
    """
    
    def __init__(self, 
                 peak_threshold: float = 2.0,
                 lower_threshold: float = -2.0):
        """
        参数:
            peak_threshold: Peak状态的Z分数阈值
            lower_threshold: Lower状态的Z分数阈值
        """
        self.peak_threshold = peak_threshold
        self.lower_threshold = lower_threshold
        
        # 校正因子（使Sn在正态分布下无偏）
        self.correction_factor = 1.1926
        
    def sn_scale(self, data: np.ndarray) -> float:
        """
        计算Sn尺度估计
        
        Sn = c × median_i{ median_j(|x_i - x_j|) }
        
        优点:
        - 比标准差更鲁棒（breakdown point = 50%）
        - 对异常值不敏感
        - 适合高噪声数据
        
        输入:
            data: 一维数组
        
        输出:
            Sn尺度值
        """
        n = len(data)
        
        # 计算所有成对差值的中位数
        pairwise_medians = []
        for i in range(n):
            # 第i个点与所有点的差值的绝对值
            diffs = np.abs(data - data[i])
            pairwise_medians.append(np.median(diffs))
        
        # 对所有中位数再取中位数
        sn = self.correction_factor * np.median(pairwise_medians)
        
        return sn
    
    def robust_z_score(self, 
                       value: float,
                       reference_data: np.ndarray) -> float:
        """
        计算鲁棒Z分数
        
        Z = (value - median) / Sn
        
        输入:
            value: 待判断的值（通常是预测值）
            reference_data: 参考数据（历史窗口）
        
        输出:
            鲁棒Z分数
        """
        median = np.median(reference_data)
        sn = self.sn_scale(reference_data)
        
        # 防止除零
        if sn < 1e-8:
            return 0.0
        
        z_score = (value - median) / sn
        return z_score
    
    def classify(self, 
                 prediction: float,
                 historical_window: np.ndarray) -> str:
        """
        分类单个预测值
        
        输入:
            prediction: 预测的下一时刻用电值
            historical_window: 历史窗口数据（用于计算参考分布）
        
        输出:
            状态标签: 'Peak' / 'Normal' / 'Lower'
        """
        z = self.robust_z_score(prediction, historical_window)
        
        if z > self.peak_threshold:
            return 'Peak'
        elif z < self.lower_threshold:
            return 'Lower'
        else:
            return 'Normal'
    
    def classify_batch(self,
                       predictions: np.ndarray,
                       historical_windows: np.ndarray) -> np.ndarray:
        """
        批量分类
        
        输入:
            predictions: [n_samples,] 预测值数组
            historical_windows: [n_samples, window_size] 对应的历史窗口
        
        输出:
            [n_samples,] 状态标签数组
        """
        states = []
        for pred, window in zip(predictions, historical_windows):
            state = self.classify(pred, window)
            states.append(state)
        
        return np.array(states)
    
    def get_state_statistics(self, states: np.ndarray) -> dict:
        """
        统计各状态的分布
        
        输入:
            states: 状态标签数组
        
        输出:
            统计字典
        """
        unique, counts = np.unique(states, return_counts=True)
        total = len(states)
        
        stats = {
            'total': total,
            'counts': dict(zip(unique, counts)),
            'ratios': {s: c/total for s, c in zip(unique, counts)}
        }
        
        logger.info("状态分布:")
        for state in ['Peak', 'Normal', 'Lower']:
            if state in stats['counts']:
                count = stats['counts'][state]
                ratio = stats['ratios'][state]
                logger.info(f"  {state}: {count} ({ratio:.2%})")
        
        return stats


# 使用示例
if __name__ == "__main__":
    # 创建分类器
    classifier = SnStateClassifier(
        peak_threshold=2.0,
        lower_threshold=-2.0
    )
    
    # 模拟数据
    np.random.seed(42)
    
    # 历史窗口（正常用电）
    historical = np.random.normal(loc=3.0, scale=0.5, size=80)
    
    # 测试用例
    test_cases = [
        ("正常预测", 3.2, historical),
        ("峰值预测", 5.0, historical),
        ("偏低预测", 1.5, historical)
    ]
    
    for desc, pred, window in test_cases:
        state = classifier.classify(pred, window)
        z_score = classifier.robust_z_score(pred, window)
        print(f"{desc}: 预测值={pred:.2f}, Z分数={z_score:.2f}, 状态={state}")
```

---

**待续...**

由于实现文档内容较长，我将继续补充其他模块的实现。让我在下一部分继续。

---

### 2.4 离散化模块

#### 文件: `src/data/discretizer.py`

```python
"""
连续变量离散化模块
"""

import pandas as pd
import numpy as np
from typing import Dict, List
import pickle
import logging

logger = logging.getLogger(__name__)


class QuantileDiscretizer:
    """
    基于分位数的离散化器
    
    将连续变量映射为有限等级（Low, Medium, High, VeryHigh）
    """
    
    def __init__(self, n_bins: int = 4, labels: List[str] = None):
        """
        参数:
            n_bins: 离散化级别数
            labels: 标签列表
        """
        self.n_bins = n_bins
        self.labels = labels or ['Low', 'Medium', 'High', 'VeryHigh']
        
        assert len(self.labels) == n_bins, \
            f"标签数量 ({len(self.labels)}) 必须等于bins数量 ({n_bins})"
        
        # 保存每个变量的分位点（用于transform）
        self.quantile_dict_ = {}
        self.is_fitted = False
    
    def fit(self, data: pd.DataFrame, columns: List[str]):
        """
        拟合离散化器（计算分位点）
        
        输入:
            data: 数据框
            columns: 需要离散化的列名列表
        """
        logger.info(f"开始拟合离散化器，变量数: {len(columns)}")
        
        quantiles = np.linspace(0, 1, self.n_bins + 1)
        
        for col in columns:
            if col not in data.columns:
                logger.warning(f"列 '{col}' 不存在，跳过")
                continue
            
            # 计算分位点
            bins = data[col].quantile(quantiles).values
            
            # 确保边界唯一（避免重复边界导致的错误）
            bins = np.unique(bins)
            if len(bins) < len(quantiles):
                logger.warning(f"列 '{col}' 的唯一分位点少于预期，可能有重复值")
            
            self.quantile_dict_[col] = bins
        
        self.is_fitted = True
        logger.info("离散化器拟合完成")
    
    def transform(self, data: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """
        转换数据（离散化）
        
        输入:
            data: 数据框
            columns: 需要离散化的列名
        
        输出:
            离散化后的数据框（原列被替换）
        """
        if not self.is_fitted:
            raise ValueError("离散化器未拟合，请先调用fit()")
        
        data_discrete = data.copy()
        
        for col in columns:
            if col not in self.quantile_dict_:
                logger.warning(f"列 '{col}' 未在拟合时出现，跳过")
                continue
            
            bins = self.quantile_dict_[col]
            
            # 使用pd.cut进行离散化
            data_discrete[col] = pd.cut(
                data[col],
                bins=bins,
                labels=self.labels[:len(bins)-1],  # 标签数 = bins数 - 1
                include_lowest=True,
                duplicates='drop'
            )
        
        return data_discrete
    
    def fit_transform(self, data: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """拟合并转换"""
        self.fit(data, columns)
        return self.transform(data, columns)
    
    def save(self, filepath: str):
        """保存离散化器"""
        with open(filepath, 'wb') as f:
            pickle.dump(self.quantile_dict_, f)
        logger.info(f"离散化器已保存到 {filepath}")
    
    def load(self, filepath: str):
        """加载离散化器"""
        with open(filepath, 'rb') as f:
            self.quantile_dict_ = pickle.load(f)
        self.is_fitted = True
        logger.info(f"离散化器已从 {filepath} 加载")


# 使用示例
if __name__ == "__main__":
    # 模拟数据
    np.random.seed(42)
    data = pd.DataFrame({
        'power': np.random.normal(3, 1, 1000),
        'voltage': np.random.normal(240, 5, 1000),
        'current': np.random.normal(10, 2, 1000)
    })
    
    # 离散化
    discretizer = QuantileDiscretizer(n_bins=4)
    
    columns_to_discretize = ['power', 'voltage', 'current']
    data_discrete = discretizer.fit_transform(data, columns_to_discretize)
    
    print("离散化后的前5行:")
    print(data_discrete.head())
    
    print("\n各变量的分布:")
    for col in columns_to_discretize:
        print(f"\n{col}:")
        print(data_discrete[col].value_counts().sort_index())
```

---

### 2.5 CAM/Attention聚类模块

#### 文件: `src/models/clustering.py`

```python
"""
CAM和Attention向量聚类模块
"""

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import pickle
import logging

logger = logging.getLogger(__name__)


class DLPClusterer:
    """
    深度学习参数（Deep Learning Parameters）聚类器
    
    用于:
    1. CAM（类激活图）聚类
    2. Attention权重向量聚类
    """
    
    def __init__(self, 
                 n_clusters: int = None,
                 max_clusters: int = 10,
                 random_state: int = 42):
        """
        参数:
            n_clusters: 聚类数量（None则自动选择）
            max_clusters: 自动选择时的最大聚类数
            random_state: 随机种子
        """
        self.n_clusters = n_clusters
        self.max_clusters = max_clusters
        self.random_state = random_state
        
        self.kmeans = None
        self.cluster_centers_ = None
        self.labels_ = None
    
    def find_optimal_k(self, data: np.ndarray) -> int:
        """
        使用肘部法则（Elbow Method）自动确定最优K
        
        输入:
            data: [n_samples, n_features]
        
        输出:
            最优K值
        """
        logger.info("使用肘部法则寻找最优K值...")
        
        inertias = []
        silhouette_scores = []
        K_range = range(2, min(self.max_clusters + 1, len(data) // 10 + 1))
        
        for k in K_range:
            kmeans = KMeans(n_clusters=k, 
                            random_state=self.random_state,
                            n_init=10)
            kmeans.fit(data)
            
            inertias.append(kmeans.inertia_)
            
            if k < len(data):
                score = silhouette_score(data, kmeans.labels_)
                silhouette_scores.append(score)
            else:
                silhouette_scores.append(0)
        
        # 使用轮廓系数最大的K
        optimal_k = K_range[np.argmax(silhouette_scores)]
        
        logger.info(f"最优K值: {optimal_k}")
        logger.info(f"对应轮廓系数: {max(silhouette_scores):.3f}")
        
        # 可视化（可选）
        # self._plot_elbow(K_range, inertias, silhouette_scores, optimal_k)
        
        return optimal_k
    
    def fit(self, data: np.ndarray):
        """
        拟合聚类器
        
        输入:
            data: [n_samples, n_features]
        """
        if self.n_clusters is None:
            self.n_clusters = self.find_optimal_k(data)
        
        logger.info(f"开始K-means聚类，K={self.n_clusters}")
        
        self.kmeans = KMeans(n_clusters=self.n_clusters,
                             random_state=self.random_state,
                             n_init=20,
                             max_iter=300)
        self.labels_ = self.kmeans.fit_predict(data)
        self.cluster_centers_ = self.kmeans.cluster_centers_
        
        logger.info("聚类完成")
        self._print_cluster_info(data)
    
    def transform(self, data: np.ndarray) -> np.ndarray:
        """
        将新数据映射到聚类标签
        
        输入:
            data: [n_samples, n_features]
        
        输出:
            聚类标签 [n_samples,]
        """
        if self.kmeans is None:
            raise ValueError("聚类器未拟合")
        
        return self.kmeans.predict(data)
    
    def fit_transform(self, data: np.ndarray) -> np.ndarray:
        """拟合并转换"""
        self.fit(data)
        return self.labels_
    
    def _print_cluster_info(self, data: np.ndarray):
        """打印聚类统计信息"""
        unique, counts = np.unique(self.labels_, return_counts=True)
        
        logger.info("聚类分布:")
        for cluster_id, count in zip(unique, counts):
            ratio = count / len(data)
            logger.info(f"  簇{cluster_id}: {count} 样本 ({ratio:.1%})")
    
    def save(self, filepath: str):
        """保存聚类器"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'kmeans': self.kmeans,
                'n_clusters': self.n_clusters,
                'cluster_centers': self.cluster_centers_
            }, f)
        logger.info(f"聚类器已保存到 {filepath}")
    
    def load(self, filepath: str):
        """加载聚类器"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.kmeans = data['kmeans']
            self.n_clusters = data['n_clusters']
            self.cluster_centers_ = data['cluster_centers']
        logger.info(f"聚类器已从 {filepath} 加载")


class AttentionClusterer(DLPClusterer):
    """
    Attention向量聚类器（专用）
    
    额外功能:
    - 累积Attention向量处理
    - 识别Early/Late/Other模式
    """
    
    def preprocess_attention(self, attention_weights: np.ndarray) -> np.ndarray:
        """
        预处理Attention权重向量
        
        步骤:
        1. 归一化（确保和为1）
        2. 计算累积和
        
        输入:
            attention_weights: [n_samples, sequence_length]
        
        输出:
            累积attention向量 [n_samples, sequence_length]
        """
        # 归一化
        normalized = attention_weights / (attention_weights.sum(axis=1, keepdims=True) + 1e-8)
        
        # 累积和
        cumulative = np.cumsum(normalized, axis=1)
        
        return cumulative
    
    def fit(self, attention_weights: np.ndarray):
        """
        拟合（自动预处理）
        
        输入:
            attention_weights: [n_samples, sequence_length]
        """
        processed = self.preprocess_attention(attention_weights)
        super().fit(processed)
        
        # 为聚类命名（基于累积模式）
        self._name_clusters(processed)
    
    def _name_clusters(self, cumulative_attention: np.ndarray):
        """
        根据累积Attention模式命名聚类
        
        策略:
        - Early: 前50%时间累积到70%以上
        - Late: 前50%时间累积到30%以下
        - Other: 其他
        """
        self.cluster_names_ = {}
        
        mid_point = cumulative_attention.shape[1] // 2
        
        for cluster_id in range(self.n_clusters):
            # 该簇的样本
            mask = self.labels_ == cluster_id
            cluster_samples = cumulative_attention[mask]
            
            # 计算中点的平均累积值
            mid_cumulative = cluster_samples[:, mid_point].mean()
            
            if mid_cumulative > 0.7:
                name = 'Early'
            elif mid_cumulative < 0.3:
                name = 'Late'
            else:
                name = 'Other'
            
            self.cluster_names_[cluster_id] = name
        
        logger.info("Attention簇命名:")
        for cid, name in self.cluster_names_.items():
            logger.info(f"  簇{cid} -> {name}")
    
    def transform_to_names(self, attention_weights: np.ndarray) -> np.ndarray:
        """
        转换为命名标签
        
        输入:
            attention_weights: [n_samples, sequence_length]
        
        输出:
            命名标签 [n_samples,] (例如 ['Early', 'Late', ...])
        """
        processed = self.preprocess_attention(attention_weights)
        cluster_ids = super().transform(processed)
        
        return np.array([self.cluster_names_[cid] for cid in cluster_ids])


# 使用示例
if __name__ == "__main__":
    # === CAM聚类 ===
    np.random.seed(42)
    
    # 模拟CAM数据（例如从CNN最后一层提取）
    cam_data = np.random.rand(1000, 20)  # 1000样本，20个时间步
    
    cam_clusterer = DLPClusterer(n_clusters=None)  # 自动选择K
    cam_labels = cam_clusterer.fit_transform(cam_data)
    
    print(f"CAM聚类完成，K={cam_clusterer.n_clusters}")
    print(f"前10个样本的聚类标签: {cam_labels[:10]}")
    
    # === Attention聚类 ===
    # 模拟Attention权重（前期高 vs 后期高）
    attention_early = np.random.beta(2, 5, size=(500, 80))  # 前期高
    attention_late = np.random.beta(5, 2, size=(500, 80))   # 后期高
    attention_data = np.vstack([attention_early, attention_late])
    
    att_clusterer = AttentionClusterer(n_clusters=3)
    att_clusterer.fit(attention_data)
    
    # 转换为命名标签
    att_names = att_clusterer.transform_to_names(attention_data[:10])
    print(f"\n前10个样本的Attention类型: {att_names}")
```

---

以上是核心模块的实现代码。由于篇幅限制，后续模块（关联规则、贝叶斯网络、因果推断）我将在下一部分继续编写。

你现在可以看到：
1. **项目设计文档**已创建（完整的架构设计）
2. **实现文档**正在创建（包含详细代码）

这些文档都位于 [doc/](doc/)目录下。接下来我会继续完成实现文档并开始搭建项目代码结构。

---

### 2.6 关联规则挖掘模块

#### 文件: `src/models/association.py`

```python
"""
关联规则挖掘模块（Apriori算法）
用于提取离散变量间的高频关联模式
"""

import pandas as pd
import numpy as np
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)


class AssociationRuleMiner:
    """
    关联规则挖掘器
    
    基于Apriori算法提取候选因果关系
    """
    
    def __init__(self, 
                 min_support: float = 0.1,
                 min_confidence: float = 0.6,
                 min_lift: float = 1.0):
        """
        参数:
            min_support: 最小支持度阈值
            min_confidence: 最小置信度阈值
            min_lift: 最小提升度阈值
        """
        self.min_support = min_support
        self.min_confidence = min_confidence
        self.min_lift = min_lift
        
        self.frequent_itemsets_ = None
        self.rules_ = None
    
    def prepare_data(self, df: pd.DataFrame, target_col: str = 'EDP') -> pd.DataFrame:
        """
        准备one-hot编码数据
        
        输入:
            df: 离散化后的数据框
            target_col: 目标列名（EDP状态）
        
        输出:
            One-hot编码的数据框
        """
        logger.info("准备关联规则挖掘数据...")
        
        # 对每一列创建 "列名_值" 的组合
        transactions = []
        
        for _, row in df.iterrows():
            items = []
            for col in df.columns:
                if pd.notna(row[col]):
                    # 创建 "变量=值" 的项
                    item = f"{col}_{row[col]}"
                    items.append(item)
            transactions.append(items)
        
        # One-hot编码
        te = TransactionEncoder()
        te_ary = te.fit(transactions).transform(transactions)
        df_encoded = pd.DataFrame(te_ary, columns=te.columns_)
        
        logger.info(f"编码后特征数: {len(df_encoded.columns)}")
        return df_encoded
    
    def mine_frequent_itemsets(self, df_encoded: pd.DataFrame):
        """
        挖掘频繁项集
        
        输入:
            df_encoded: One-hot编码后的数据框
        """
        logger.info(f"挖掘频繁项集 (min_support={self.min_support})...")
        
        self.frequent_itemsets_ = apriori(
            df_encoded,
            min_support=self.min_support,
            use_colnames=True,
            low_memory=True
        )
        
        logger.info(f"找到 {len(self.frequent_itemsets_)} 个频繁项集")
        return self.frequent_itemsets_
    
    def generate_rules(self, target_suffix: str = 'EDP'):
        """
        生成关联规则
        
        输入:
            target_suffix: 目标变量后缀（用于过滤规则）
        
        输出:
            规则数据框
        """
        if self.frequent_itemsets_ is None:
            raise ValueError("请先调用 mine_frequent_itemsets()")
        
        logger.info("生成关联规则...")
        
        # 生成所有规则
        all_rules = association_rules(
            self.frequent_itemsets_,
            metric="confidence",
            min_threshold=self.min_confidence
        )
        
        # 过滤：只保留后件包含目标变量的规则
        def has_target_in_consequent(consequents):
            return any(target_suffix in str(item) for item in consequents)
        
        self.rules_ = all_rules[
            all_rules['consequents'].apply(has_target_in_consequent)
        ]
        
        # 进一步过滤lift
        self.rules_ = self.rules_[self.rules_['lift'] >= self.min_lift]
        
        # 按confidence排序
        self.rules_ = self.rules_.sort_values(
            by='confidence',
            ascending=False
        ).reset_index(drop=True)
        
        logger.info(f"生成 {len(self.rules_)} 条有效规则")
        return self.rules_
    
    def filter_rules_for_state(self, state: str) -> pd.DataFrame:
        """
        过滤特定EDP状态的规则
        
        输入:
            state: 'Peak', 'Normal', 或 'Lower'
        
        输出:
            该状态的规则子集
        """
        if self.rules_ is None:
            raise ValueError("请先调用 generate_rules()")
        
        state_key = f"EDP_{state}"
        
        filtered = self.rules_[
            self.rules_['consequents'].apply(
                lambda x: state_key in str(x)
            )
        ]
        
        logger.info(f"状态 '{state}' 的规则数: {len(filtered)}")
        return filtered
    
    def get_top_rules(self, n: int = 10, state: str = None) -> pd.DataFrame:
        """
        获取Top-N规则
        
        输入:
            n: 返回规则数量
            state: 特定状态（None则返回所有）
        
        输出:
            Top规则数据框
        """
        if state is not None:
            rules = self.filter_rules_for_state(state)
        else:
            rules = self.rules_
        
        return rules.head(n)
    
    def rules_to_constraints(self, state: str) -> List[tuple]:
        """
        将规则转换为贝叶斯网络的候选边
        
        输入:
            state: EDP状态
        
        输出:
            候选边列表 [(from_var, to_var), ...]
        """
        state_rules = self.filter_rules_for_state(state)
        
        edges = []
        target_node = 'EDP'
        
        for _, rule in state_rules.iterrows():
            # 从前件中提取变量名（去掉值部分）
            for antecedent in rule['antecedents']:
                # 格式: "VariableName_Value"
                var_name = str(antecedent).split('_')[0]
                
                # 添加边: 变量 → EDP
                if var_name != target_node:
                    edges.append((var_name, target_node))
        
        # 去重
        edges = list(set(edges))
        
        logger.info(f"状态 '{state}' 提取 {len(edges)} 条候选边")
        return edges
    
    def print_rules_summary(self, n: int = 5):
        """
        打印规则摘要
        
        输入:
            n: 每个状态显示的规则数
        """
        if self.rules_ is None or len(self.rules_) == 0:
            logger.warning("没有找到规则")
            return
        
        print("\n" + "=" * 80)
        print("关联规则摘要")
        print("=" * 80)
        
        for state in ['Peak', 'Normal', 'Lower']:
            print(f"\n--- 状态: {state} ---")
            state_rules = self.filter_rules_for_state(state)
            
            if len(state_rules) == 0:
                print("  (无规则)")
                continue
            
            for idx, rule in state_rules.head(n).iterrows():
                antecedents_str = ', '.join([str(x) for x in rule['antecedents']])
                consequents_str = ', '.join([str(x) for x in rule['consequents']])
                
                print(f"\n  规则 {idx + 1}:")
                print(f"    IF {antecedents_str}")
                print(f"    THEN {consequents_str}")
                print(f"    支持度: {rule['support']:.3f}")
                print(f"    置信度: {rule['confidence']:.3f}")
                print(f"    提升度: {rule['lift']:.3f}")
        
        print("\n" + "=" * 80)


# 使用示例
if __name__ == "__main__":
    # 模拟离散化数据
    np.random.seed(42)
    
    data = pd.DataFrame({
        'ClimateControl': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'Kitchen': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'Laundry': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'GlobalActivePower': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'Season': np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], 1000),
        'Weekend': np.random.choice(['Yes', 'No'], 1000),
        'EDP': np.random.choice(['Peak', 'Normal', 'Lower'], 1000, p=[0.2, 0.6, 0.2])
    })
    
    # 创建规则挖掘器
    miner = AssociationRuleMiner(
        min_support=0.05,
        min_confidence=0.5,
        min_lift=1.2
    )
    
    # 数据准备
    df_encoded = miner.prepare_data(data)
    
    # 挖掘频繁项集
    miner.mine_frequent_itemsets(df_encoded)
    
    # 生成规则
    rules = miner.generate_rules()
    
    # 打印摘要
    miner.print_rules_summary(n=3)
    
    # 提取候选边
    peak_edges = miner.rules_to_constraints('Peak')
    print(f"\nPeak状态候选边: {peak_edges}")
```

---

### 2.7 贝叶斯网络模块

#### 文件: `src/models/bayesian_net.py`

```python
"""
贝叶斯网络构建与推断模块
"""

import pandas as pd
import numpy as np
from pgmpy.models import BayesianNetwork
from pgmpy.estimators import (
    HillClimbSearch, 
    BicScore, 
    MaximumLikelihoodEstimator,
    BayesianEstimator
)
from pgmpy.inference import VariableElimination
import networkx as nx
import pickle
import logging

logger = logging.getLogger(__name__)


class CausalBayesianNetwork:
    """
    因果贝叶斯网络
    
    结合领域知识约束的结构学习
    """
    
    def __init__(self, domain_knowledge: dict = None):
        """
        参数:
            domain_knowledge: 领域知识约束字典
                {
                    'themes': {  # 主题分组
                        'physical_env': [变量列表],
                        'appliances': [变量列表],
                        'consumption': [变量列表],
                        'dlp': [变量列表],
                        'target': [目标变量]
                    },
                    'allow_directions': [  # 允许的方向
                        ('theme_from', 'theme_to'),
                        ...
                    ]
                }
        """
        self.domain_knowledge = domain_knowledge or self._default_domain_knowledge()
        
        self.model = None
        self.inference = None
        
        # 白名单和黑名单
        self.white_list = []
        self.black_list = []
    
    def _default_domain_knowledge(self) -> dict:
        """
        默认领域知识（基于论文表2）
        """
        return {
            'themes': {
                'physical_env': ['Date', 'Day', 'Month', 'Season', 'Weekend'],
                'appliances': ['Kitchen', 'Laundry', 'ClimateControl', 'Other'],
                'consumption': ['GlobalActivePower', 'GlobalReactivePower', 
                                'Voltage', 'GlobalIntensity'],
                'dlp': ['CAM_type', 'ATT_type'],
                'target': ['EDP']
            },
            'allow_directions': [
                ('physical_env', 'appliances'),
                ('appliances', 'consumption'),
                ('consumption', 'target'),
                ('dlp', 'target')  # DLP直接影响目标
            ]
        }
    
    def _build_constraints(self, variables: List[str]):
        """
        根据领域知识构建白名单和黑名单
        
        输入:
            variables: 数据中实际存在的变量列表
        """
        logger.info("构建领域知识约束...")
        
        # 映射变量到主题
        var_to_theme = {}
        for theme, vars_list in self.domain_knowledge['themes'].items():
            for var in vars_list:
                if var in variables:
                    var_to_theme[var] = theme
        
        # 生成白名单（允许的边）
        for from_theme, to_theme in self.domain_knowledge['allow_directions']:
            from_vars = [v for v, t in var_to_theme.items() if t == from_theme]
            to_vars = [v for v, t in var_to_theme.items() if t == to_theme]
            
            for from_var in from_vars:
                for to_var in to_vars:
                    if from_var != to_var:
                        self.white_list.append((from_var, to_var))
        
        # 生成黑名单（禁止的边）
        # 规则1: 目标变量不能指向其他变量
        target_vars = self.domain_knowledge['themes']['target']
        for target in target_vars:
            if target in variables:
                for var in variables:
                    if var != target:
                        self.black_list.append((target, var))
        
        # 规则2: 反向因果（consumption → appliances, appliances → physical_env）
        consumption_vars = [v for v, t in var_to_theme.items() 
                            if t == 'consumption']
        appliance_vars = [v for v, t in var_to_theme.items() 
                          if t == 'appliances']
        physical_vars = [v for v, t in var_to_theme.items() 
                         if t == 'physical_env']
        
        for cons_var in consumption_vars:
            for app_var in appliance_vars:
                self.black_list.append((cons_var, app_var))
        
        for app_var in appliance_vars:
            for phy_var in physical_vars:
                self.black_list.append((app_var, phy_var))
        
        logger.info(f"白名单边数: {len(self.white_list)}")
        logger.info(f"黑名单边数: {len(self.black_list)}")
    
    def learn_structure(self, data: pd.DataFrame, scoring_method='bic'):
        """
        结构学习（带领域约束）
        
        输入:
            data: 离散化后的数据
            scoring_method: 评分方法 ('bic', 'k2', 'bdeu')
        
        输出:
            学习到的DAG结构
        """
        logger.info("开始贝叶斯网络结构学习...")
        
        variables = list(data.columns)
        self._build_constraints(variables)
        
        # 选择评分函数
        if scoring_method == 'bic':
            scoring = BicScore(data)
        else:
            raise ValueError(f"不支持的评分方法: {scoring_method}")
        
        # Hill-Climbing搜索
        hc = HillClimbSearch(data)
        
        best_model = hc.estimate(
            scoring_method=scoring,
            white_list=self.white_list if len(self.white_list) > 0 else None,
            black_list=self.black_list if len(self.black_list) > 0 else None,
            max_indegree=5,  # 限制入度，避免过于复杂
            max_iter=int(1e4)
        )
        
        logger.info(f"结构学习完成。边数: {len(best_model.edges())}")
        return best_model
    
    def learn_parameters(self, data: pd.DataFrame, structure=None, method='mle'):
        """
        参数学习（CPT估计）
        
        输入:
            data: 离散化数据
            structure: DAG结构（None则使用已学习的）
            method: 'mle' (最大似然) 或 'bayes' (贝叶斯估计)
        
        输出:
            完整的贝叶斯网络模型
        """
        if structure is None and self.model is None:
            raise ValueError("请先进行结构学习或提供结构")
        
        if structure is not None:
            self.model = BayesianNetwork(structure.edges())
        
        logger.info(f"参数学习 (方法: {method})...")
        
        if method == 'mle':
            estimator = MaximumLikelihoodEstimator
        elif method == 'bayes':
            estimator = BayesianEstimator
        else:
            raise ValueError(f"未知方法: {method}")
        
        self.model.fit(data, estimator=estimator)
        
        # 初始化推断引擎
        self.inference = VariableElimination(self.model)
        
        logger.info("参数学习完成")
        return self.model
    
    def query(self, variables: List[str], evidence: dict = None) -> dict:
        """
        贝叶斯推断
        
        输入:
            variables: 查询变量列表
            evidence: 观测证据 {'var': 'value', ...}
        
        输出:
            后验概率分布
        """
        if self.inference is None:
            raise ValueError("模型未拟合，无法推断")
        
        result = self.inference.query(
            variables=variables,
            evidence=evidence
        )
        
        return result
    
    def map_query(self, variables: List[str], evidence: dict = None) -> dict:
        """
        最大后验估计（MAP）
        
        输入:
            variables: 查询变量
            evidence: 证据
        
        输出:
            最可能的状态
        """
        if self.inference is None:
            raise ValueError("模型未拟合")
        
        result = self.inference.map_query(
            variables=variables,
            evidence=evidence
        )
        
        return result
    
    def get_cpd(self, variable: str):
        """获取条件概率表"""
        return self.model.get_cpds(variable)
    
    def save(self, filepath: str):
        """保存模型"""
        with open(filepath, 'wb') as f:
            pickle.dump({
                'model': self.model,
                'domain_knowledge': self.domain_knowledge
            }, f)
        logger.info(f"模型已保存到 {filepath}")
    
    def load(self, filepath: str):
        """加载模型"""
        with open(filepath, 'rb') as f:
            data = pickle.load(f)
            self.model = data['model']
            self.domain_knowledge = data['domain_knowledge']
            self.inference = VariableElimination(self.model)
        logger.info(f"模型已从 {filepath} 加载")
    
    def visualize(self, filename: str = None):
        """可视化DAG结构"""
        if self.model is None:
            raise ValueError("模型未构建")
        
        import matplotlib.pyplot as plt
        
        G = nx.DiGraph(self.model.edges())
        pos = nx.spring_layout(G, k=2, iterations=50)
        
        plt.figure(figsize=(14, 10))
        nx.draw(G, pos,
                with_labels=True,
                node_color='lightblue',
                node_size=3000,
                font_size=10,
                font_weight='bold',
                arrows=True,
                arrowsize=20,
                edge_color='gray')
        
        plt.title("Bayesian Network Structure", fontsize=16)
        
        if filename:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            logger.info(f"网络结构图已保存到 {filename}")
        else:
            plt.show()


# 使用示例
if __name__ == "__main__":
    # 模拟数据
    np.random.seed(42)
    data = pd.DataFrame({
        'Season': np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], 1000),
        'Weekend': np.random.choice(['Yes', 'No'], 1000),
        'ClimateControl': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'Kitchen': np.random.choice(['Low', 'Medium', 'High'], 1000),
        'GlobalActivePower': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'CAM_type': np.random.choice(['Type1', 'Type2'], 1000),
        'ATT_type': np.random.choice(['Early', 'Late', 'Other'], 1000),
        'EDP': np.random.choice(['Peak', 'Normal', 'Lower'], 1000)
    })
    
    # 创建BN
    bn = CausalBayesianNetwork()
    
    # 结构学习
    structure = bn.learn_structure(data)
    
    # 参数学习
    bn.learn_parameters(data)
    
    # 推断
    evidence = {
        'Season': 'Summer',
        'ClimateControl': 'VeryHigh',
        'ATT_type': 'Late'
    }
    
    result = bn.query(variables=['EDP'], evidence=evidence)
    print("\n推断结果:")
    print(result)
    
    # 可视化
    bn.visualize('bn_structure.png')
```

---


### 2.8 因果推断模块

#### 文件: `src/inference/causal_inference.py`

```python
"""
因果推断和敏感性分析模块
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import logging

logger = logging.getLogger(__name__)


class CausalInferenceEngine:
    """
    因果推断引擎
    
    基于贝叶斯网络进行因果推断和敏感性分析
    """
    
    def __init__(self, bayesian_network):
        """
        参数:
            bayesian_network: CausalBayesianNetwork实例
        """
        self.bn = bayesian_network
        
        if self.bn.inference is None:
            raise ValueError("贝叶斯网络未拟合")
    
    def causal_effect(self, 
                     intervention: Dict[str, str],
                     target: str,
                     evidence: Dict[str, str] = None) -> pd.DataFrame:
        """
        计算do-calculus下的因果效应
        
        输入:
            intervention: 干预变量及其值 {'var': 'value'}
            target: 目标变量
            evidence: 观测证据 (可选)
        
        输出:
            目标变量的后验分布
        """
        logger.info(f"计算因果效应: do({intervention}) -> {target}")
        
        # 合并干预和证据
        combined_evidence = evidence.copy() if evidence else {}
        combined_evidence.update(intervention)
        
        # 查询目标变量
        result = self.bn.query(
            variables=[target],
            evidence=combined_evidence
        )
        
        return result
    
    def sensitivity_analysis(self,
                            target: str,
                            variables: List[str],
                            baseline: Dict[str, str] = None,
                            n_samples: int = 100) -> pd.DataFrame:
        """
        敏感性分析（单变量扰动）
        
        输入:
            target: 目标变量
            variables: 待分析的变量列表
            baseline: 基线观测（None则使用空证据）
            n_samples: 每个变量的采样数
        
        输出:
            敏感性分数DataFrame
        """
        logger.info(f"敏感性分析: 目标={target}, 变量数={len(variables)}")
        
        baseline = baseline or {}
        
        sensitivity_scores = {}
        
        for var in variables:
            # 获取变量的所有可能值
            var_cpd = self.bn.get_cpd(var)
            var_states = var_cpd.state_names[var]
            
            # 计算每个值下目标的分布
            distributions = []
            
            for state in var_states:
                # 干预此变量
                evidence = baseline.copy()
                evidence[var] = state
                
                try:
                    result = self.bn.query(
                        variables=[target],
                        evidence=evidence
                    )
                    
                    # 提取概率分布
                    probs = result.values
                    distributions.append(probs)
                    
                except Exception as e:
                    logger.warning(f"查询失败: {var}={state}, 错误: {e}")
                    distributions.append(None)
            
            # 计算分布的方差（作为敏感性指标）
            valid_dists = [d for d in distributions if d is not None]
            
            if len(valid_dists) > 1:
                # 计算各状态下Peak/Normal/Lower概率的标准差
                dist_array = np.array(valid_dists)
                sensitivity = np.mean(np.std(dist_array, axis=0))
            else:
                sensitivity = 0.0
            
            sensitivity_scores[var] = sensitivity
        
        # 转为DataFrame并排序
        df = pd.DataFrame.from_dict(
            sensitivity_scores,
            orient='index',
            columns=['Sensitivity']
        ).sort_values(by='Sensitivity', ascending=False)
        
        logger.info(f"敏感性分析完成，Top变量: {df.head(3).index.tolist()}")
        
        return df
    
    def tornado_chart(self, 
                     sensitivity_df: pd.DataFrame,
                     top_n: int = 10,
                     filename: str = None):
        """
        绘制Tornado图（敏感性可视化）
        
        输入:
            sensitivity_df: 敏感性分析结果
            top_n: 显示Top-N变量
            filename: 保存路径（None则显示）
        """
        top_vars = sensitivity_df.head(top_n)
        
        plt.figure(figsize=(10, 6))
        
        y_pos = np.arange(len(top_vars))
        plt.barh(y_pos, top_vars['Sensitivity'], color='steelblue')
        
        plt.yticks(y_pos, top_vars.index)
        plt.xlabel('Sensitivity Score', fontsize=12)
        plt.title('Tornado Chart: Variable Sensitivity', fontsize=14, fontweight='bold')
        plt.gca().invert_yaxis()  # 最高的在顶部
        
        plt.tight_layout()
        
        if filename:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            logger.info(f"Tornado图已保存到 {filename}")
        else:
            plt.show()
    
    def counterfactual_analysis(self,
                               factual_evidence: Dict[str, str],
                               intervention: Dict[str, str],
                               target: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        反事实分析
        
        比较:
          - 事实情况: P(Target | Evidence)
          - 反事实情况: P(Target | do(Intervention), Evidence)
        
        输入:
            factual_evidence: 事实观测
            intervention: 反事实干预
            target: 目标变量
        
        输出:
            (事实分布, 反事实分布)
        """
        logger.info("反事实分析...")
        
        # 事实推断
        factual_result = self.bn.query(
            variables=[target],
            evidence=factual_evidence
        )
        
        # 反事实推断
        counterfactual_evidence = factual_evidence.copy()
        counterfactual_evidence.update(intervention)
        
        counterfactual_result = self.bn.query(
            variables=[target],
            evidence=counterfactual_evidence
        )
        
        logger.info("反事实分析完成")
        
        return factual_result, counterfactual_result
    
    def compare_distributions(self,
                            dist1: pd.DataFrame,
                            dist2: pd.DataFrame,
                            labels: Tuple[str, str] = ('Factual', 'Counterfactual'),
                            filename: str = None):
        """
        可视化对比两个分布
        
        输入:
            dist1, dist2: 概率分布
            labels: 分布标签
            filename: 保存路径
        """
        states = dist1.state_names[dist1.variables[0]]
        
        x = np.arange(len(states))
        width = 0.35
        
        fig, ax = plt.subplots(figsize=(8, 5))
        
        ax.bar(x - width/2, dist1.values, width, label=labels[0], color='skyblue')
        ax.bar(x + width/2, dist2.values, width, label=labels[1], color='salmon')
        
        ax.set_xlabel('State', fontsize=12)
        ax.set_ylabel('Probability', fontsize=12)
        ax.set_title('Distribution Comparison', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(states)
        ax.legend()
        
        plt.tight_layout()
        
        if filename:
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            logger.info(f"分布对比图已保存到 {filename}")
        else:
            plt.show()
    
    def explain_state(self,
                     target: str,
                     target_state: str,
                     evidence: Dict[str, str],
                     top_n: int = 5) -> pd.DataFrame:
        """
        解释特定状态的关键因素
        
        输入:
            target: 目标变量
            target_state: 目标状态（如'Peak'）
            evidence: 当前观测
            top_n: 返回Top-N关键因素
        
        输出:
            关键因素DataFrame
        """
        logger.info(f"解释状态: {target}={target_state}")
        
        # 获取所有父节点
        parents = list(self.bn.model.get_parents(target))
        
        if len(parents) == 0:
            logger.warning(f"{target}没有父节点，无法解释")
            return pd.DataFrame()
        
        # 计算每个父节点的贡献
        contributions = {}
        
        # 基线：无任何证据
        baseline_prob = self.bn.query(
            variables=[target],
            evidence={}
        ).values
        
        # 获取target_state的索引
        target_cpd = self.bn.get_cpd(target)
        target_states = target_cpd.state_names[target]
        state_idx = list(target_states).index(target_state)
        baseline_target_prob = baseline_prob[state_idx]
        
        # 逐个证据计算贡献
        for var in parents:
            if var in evidence:
                # 计算有此证据时的概率
                single_evidence = {var: evidence[var]}
                
                prob_with_evidence = self.bn.query(
                    variables=[target],
                    evidence=single_evidence
                ).values[state_idx]
                
                # 贡献 = P(Target=state|Var) - P(Target=state)
                contribution = prob_with_evidence - baseline_target_prob
                
                contributions[var] = {
                    'Value': evidence[var],
                    'Contribution': contribution,
                    'P(Target|Var)': prob_with_evidence
                }
        
        # 转为DataFrame
        df = pd.DataFrame.from_dict(contributions, orient='index')
        df = df.sort_values(by='Contribution', ascending=False).head(top_n)
        
        logger.info(f"Top {top_n} 关键因素已识别")
        
        return df


# 使用示例
if __name__ == "__main__":
    from bayesian_net import CausalBayesianNetwork
    
    # 构建示例BN
    np.random.seed(42)
    data = pd.DataFrame({
        'Season': np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], 1000),
        'ClimateControl': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'GlobalActivePower': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'ATT_type': np.random.choice(['Early', 'Late', 'Other'], 1000),
        'EDP': np.random.choice(['Peak', 'Normal', 'Lower'], 1000)
    })
    
    bn = CausalBayesianNetwork()
    structure = bn.learn_structure(data)
    bn.learn_parameters(data)
    
    # 创建因果推断引擎
    engine = CausalInferenceEngine(bn)
    
    # 敏感性分析
    sensitivity = engine.sensitivity_analysis(
        target='EDP',
        variables=['Season', 'ClimateControl', 'GlobalActivePower', 'ATT_type']
    )
    print(sensitivity)
    
    # 绘制Tornado图
    engine.tornado_chart(sensitivity, filename='tornado.png')
    
    # 反事实分析
    factual_ev = {
        'Season': 'Summer',
        'ClimateControl': 'VeryHigh'
    }
    
    intervention = {
        'ClimateControl': 'Low'  # 反事实：如果使用低空调
    }
    
    factual, counterfactual = engine.counterfactual_analysis(
        factual_evidence=factual_ev,
        intervention=intervention,
        target='EDP'
    )
    
    print("\n事实分布:")
    print(factual)
    print("\n反事实分布:")
    print(counterfactual)
    
    # 可视化对比
    engine.compare_distributions(factual, counterfactual, filename='counterfactual.png')
```

---

### 2.9 推荐生成模块

#### 文件: `src/inference/recommendation.py`

```python
"""
可操作推荐生成模块
"""

import pandas as pd
import numpy as np
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)


class RecommendationGenerator:
    """
    基于因果推断的推荐生成器
    
    生成针对不同EDP状态的节能建议
    """
    
    def __init__(self, causal_engine):
        """
        参数:
            causal_engine: CausalInferenceEngine实例
        """
        self.engine = causal_engine
        self.bn = causal_engine.bn
        
        # 推荐模板（基于论文表7的actionable insights）
        self.templates = self._load_templates()
    
    def _load_templates(self) -> Dict:
        """
        加载推荐模板（基于论文中的示例）
        """
        return {
            'Peak': {
                'ClimateControl': {
                    'VeryHigh': "空调使用过高是峰值用电的主要原因。建议将空调温度设置提高2-3度，可降低{reduction}%的能耗。",
                    'High': "空调用量较高。建议在非高峰时段使用，或调整到节能模式。"
                },
                'Kitchen': {
                    'VeryHigh': "厨房电器集中使用导致峰值负荷。建议错峰使用烤箱、微波炉等大功率设备。",
                    'High': "厨房用电较高。建议分散使用高功率电器。"
                },
                'Laundry': {
                    'VeryHigh': "洗衣/烘干设备使用密集。建议在夜间低峰时段运行，可节省{reduction}%电费。",
                    'High': "洗衣设备用量较高。建议使用冷水洗涤模式，减少加热耗电。"
                },
                'Season': {
                    'Summer': "夏季高温导致空调负荷增加。建议使用遮阳窗帘，减少室内热量积累。",
                    'Winter': "冬季供暖需求高。建议检查门窗密封性，减少热量流失。"
                },
                'GlobalActivePower': {
                    'VeryHigh': "整体用电功率过高，存在过载风险。建议立即关闭非必要电器。"
                }
            },
            'Normal': {
                'ClimateControl': {
                    'Medium': "用电正常。继续保持当前空调设置即可。",
                    'High': "空调用量偏高。可考虑微调温度，进一步优化能耗。"
                },
                'Kitchen': {
                    'Medium': "厨房用电合理。建议使用高能效等级电器。"
                },
                'Laundry': {
                    'Medium': "洗衣设备使用正常。可选择快洗模式以节约时间和电能。"
                }
            },
            'Lower': {
                'ClimateControl': {
                    'Low': "能耗水平低，已达到节能目标。继续保持良好习惯。"
                },
                'Season': {
                    'Spring': "春季气候宜人，无需空调。建议开窗通风，享受自然通风。",
                    'Fall': "秋季温度适中。建议减少空调使用，利用自然温度调节。"
                },
                'Weekend': {
                    'No': "工作日外出时间长，用电自然较低。建议出门前检查所有电器是否关闭。"
                }
            }
        }
    
    def generate_recommendations(self,
                                current_state: str,
                                evidence: Dict[str, str],
                                top_n: int = 3) -> List[Dict]:
        """
        生成推荐列表
        
        输入:
            current_state: 当前EDP状态 ('Peak', 'Normal', 'Lower')
            evidence: 当前观测变量值
            top_n: 返回Top-N推荐
        
        输出:
            推荐列表 [{'variable': ..., 'action': ..., 'impact': ..., 'text': ...}, ...]
        """
        logger.info(f"生成推荐: 状态={current_state}, 证据数={len(evidence)}")
        
        recommendations = []
        
        # 1. 识别关键因素
        key_factors = self.engine.explain_state(
            target='EDP',
            target_state=current_state,
            evidence=evidence,
            top_n=top_n * 2  # 多取一些，后续筛选
        )
        
        # 2. 为每个关键因素生成推荐
        for var, row in key_factors.iterrows():
            var_value = row['Value']
            contribution = row['Contribution']
            
            # 查找模板
            recommendation_text = self._get_template(
                state=current_state,
                variable=var,
                value=var_value
            )
            
            if recommendation_text is None:
                # 无模板，生成通用推荐
                recommendation_text = self._generate_generic_recommendation(
                    state=current_state,
                    variable=var,
                    value=var_value,
                    contribution=contribution
                )
            
            # 计算干预影响
            impact = self._estimate_impact(
                variable=var,
                current_value=var_value,
                target_state=current_state,
                evidence=evidence
            )
            
            recommendations.append({
                'variable': var,
                'current_value': var_value,
                'contribution': contribution,
                'action': self._suggest_action(var, var_value, current_state),
                'impact': impact,
                'text': recommendation_text
            })
        
        # 3. 按影响排序，返回Top-N
        recommendations = sorted(
            recommendations,
            key=lambda x: abs(x['impact']),
            reverse=True
        )[:top_n]
        
        logger.info(f"生成 {len(recommendations)} 条推荐")
        
        return recommendations
    
    def _get_template(self, state: str, variable: str, value: str) -> str:
        """
        从模板库中获取推荐文本
        """
        try:
            return self.templates[state][variable][value]
        except KeyError:
            return None
    
    def _generate_generic_recommendation(self,
                                        state: str,
                                        variable: str,
                                        value: str,
                                        contribution: float) -> str:
        """
        生成通用推荐文本
        """
        if state == 'Peak':
            if contribution > 0.1:
                return f"{variable}当前为{value}，是导致峰值用电的重要因素。建议调整或减少使用。"
            else:
                return f"{variable}={value}对当前用电有一定影响，可考虑优化。"
        
        elif state == 'Lower':
            return f"{variable}={value}有助于保持低能耗。继续保持当前设置。"
        
        else:  # Normal
            return f"{variable}={value}，用电处于正常水平。"
    
    def _suggest_action(self, variable: str, value: str, state: str) -> str:
        """
        建议具体操作
        """
        action_map = {
            'Peak': {
                'ClimateControl': {
                    'VeryHigh': '调高空调温度2-3度',
                    'High': '切换到节能模式'
                },
                'Kitchen': {
                    'VeryHigh': '错峰使用大功率厨电',
                    'High': '减少同时使用多个厨电'
                },
                'Laundry': {
                    'VeryHigh': '延迟到夜间低峰运行',
                    'High': '使用冷水/快洗模式'
                }
            }
        }
        
        try:
            return action_map[state][variable][value]
        except KeyError:
            return '优化使用方式'
    
    def _estimate_impact(self,
                        variable: str,
                        current_value: str,
                        target_state: str,
                        evidence: Dict[str, str]) -> float:
        """
        估算干预影响（降低Peak概率的幅度）
        
        输出:
            影响系数（-1到1，负值表示降低Peak概率）
        """
        # 当前概率
        current_prob = self.bn.query(
            variables=['EDP'],
            evidence=evidence
        ).values
        
        # 获取Peak状态的索引
        edp_cpd = self.bn.get_cpd('EDP')
        peak_idx = list(edp_cpd.state_names['EDP']).index(target_state)
        current_peak_prob = current_prob[peak_idx]
        
        # 尝试改变此变量到最优值
        var_cpd = self.bn.get_cpd(variable)
        var_states = var_cpd.state_names[variable]
        
        best_improvement = 0.0
        
        for alt_value in var_states:
            if alt_value == current_value:
                continue
            
            # 干预证据
            intervention_evidence = evidence.copy()
            intervention_evidence[variable] = alt_value
            
            try:
                new_prob = self.bn.query(
                    variables=['EDP'],
                    evidence=intervention_evidence
                ).values[peak_idx]
                
                improvement = current_peak_prob - new_prob
                
                if improvement > best_improvement:
                    best_improvement = improvement
                    
            except Exception:
                continue
        
        return best_improvement
    
    def format_recommendations(self, recommendations: List[Dict]) -> str:
        """
        格式化推荐为可读文本
        
        输入:
            recommendations: 推荐列表
        
        输出:
            格式化的文本字符串
        """
        if len(recommendations) == 0:
            return "当前用电模式良好，无需特别调整。"
        
        output = "=== 节能建议 ===\n\n"
        
        for i, rec in enumerate(recommendations, 1):
            output += f"{i}. {rec['text']}\n"
            output += f"   操作: {rec['action']}\n"
            output += f"   预期影响: {rec['impact']:.1%}\n"
            output += "\n"
        
        return output
    
    def save_recommendations(self, 
                           recommendations: List[Dict],
                           filename: str):
        """
        保存推荐到文件
        """
        text = self.format_recommendations(recommendations)
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(text)
        
        logger.info(f"推荐已保存到 {filename}")


# 使用示例
if __name__ == "__main__":
    from bayesian_net import CausalBayesianNetwork
    from causal_inference import CausalInferenceEngine
    
    # 构建BN
    np.random.seed(42)
    data = pd.DataFrame({
        'Season': np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], 1000),
        'ClimateControl': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'Kitchen': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'Laundry': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'GlobalActivePower': np.random.choice(['Low', 'Medium', 'High', 'VeryHigh'], 1000),
        'EDP': np.random.choice(['Peak', 'Normal', 'Lower'], 1000, p=[0.2, 0.6, 0.2])
    })
    
    bn = CausalBayesianNetwork()
    structure = bn.learn_structure(data)
    bn.learn_parameters(data)
    
    # 创建推荐生成器
    engine = CausalInferenceEngine(bn)
    recommender = RecommendationGenerator(engine)
    
    # 生成推荐
    current_evidence = {
        'Season': 'Summer',
        'ClimateControl': 'VeryHigh',
        'Kitchen': 'High',
        'Laundry': 'Medium',
        'GlobalActivePower': 'VeryHigh'
    }
    
    recommendations = recommender.generate_recommendations(
        current_state='Peak',
        evidence=current_evidence,
        top_n=3
    )
    
    # 打印推荐
    print(recommender.format_recommendations(recommendations))
    
    # 保存推荐
    recommender.save_recommendations(recommendations, 'recommendations.txt')
```

---


### 2.10 训练流水线

#### 文件: `src/pipeline/train_pipeline.py`

```python
"""
完整训练流水线
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from pathlib import Path
import logging
import json
import pickle

logger = logging.getLogger(__name__)


class TrainingPipeline:
    """
    端到端训练流水线
    
    包含：数据预处理 → 模型训练 → 状态分类 → 离散化 → 聚类 → 关联规则 → 贝叶斯网络
    """
    
    def __init__(self, config: dict):
        """
        参数:
            config: 配置字典
                {
                    'data_path': 数据文件路径,
                    'output_dir': 输出目录,
                    'model_params': {...},
                    'training_params': {...},
                    ...
                }
        """
        self.config = config
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # 各模块组件
        self.preprocessor = None
        self.predictor = None
        self.classifier = None
        self.discretizer = None
        self.cam_clusterer = None
        self.att_clusterer = None
        self.rule_miner = None
        self.bn = None
    
    def run(self):
        """
        执行完整训练流水线
        """
        logger.info("=" * 80)
        logger.info("开始完整训练流水线")
        logger.info("=" * 80)
        
        # 步骤1: 数据加载与预处理
        X_train, y_train, X_val, y_val, X_test, y_test = self.prepare_data()
        
        # 步骤2: 训练预测模型
        self.train_predictor(X_train, y_train, X_val, y_val)
        
        # 步骤3: 评估预测性能
        self.evaluate_predictor(X_test, y_test)
        
        # 步骤4: 状态分类（Sn scale）
        edp_states = self.classify_states(y_test)
        
        # 步骤5: 离散化
        discrete_data = self.discretize_features(X_test, edp_states)
        
        # 步骤6: DLP聚类
        discrete_data = self.cluster_dlp(X_test, discrete_data)
        
        # 步骤7: 关联规则挖掘
        self.mine_association_rules(discrete_data)
        
        # 步骤8: 贝叶斯网络构建
        self.build_bayesian_network(discrete_data)
        
        # 步骤9: 保存所有模型
        self.save_models()
        
        logger.info("=" * 80)
        logger.info("训练流水线完成!")
        logger.info("=" * 80)
    
    def prepare_data(self):
        """
        步骤1: 数据准备
        """
        logger.info("\n[步骤1] 数据加载与预处理...")
        
        from ..preprocessing.data_preprocessor import EnergyDataPreprocessor
        
        # 加载数据
        df = pd.read_csv(self.config['data_path'])
        logger.info(f"数据集大小: {df.shape}")
        
        # 初始化预处理器
        self.preprocessor = EnergyDataPreprocessor(
            sequence_length=self.config.get('sequence_length', 60),
            feature_cols=self.config.get('feature_cols'),
            target_col=self.config.get('target_col', 'GlobalActivePower')
        )
        
        # 预处理
        X, y = self.preprocessor.fit_transform(df)
        
        # 分割数据集
        train_ratio = self.config.get('train_ratio', 0.7)
        val_ratio = self.config.get('val_ratio', 0.15)
        
        train_size = int(len(X) * train_ratio)
        val_size = int(len(X) * val_ratio)
        
        X_train = X[:train_size]
        y_train = y[:train_size]
        
        X_val = X[train_size:train_size+val_size]
        y_val = y[train_size:train_size+val_size]
        
        X_test = X[train_size+val_size:]
        y_test = y[train_size+val_size:]
        
        logger.info(f"训练集: {len(X_train)}, 验证集: {len(X_val)}, 测试集: {len(X_test)}")
        
        return X_train, y_train, X_val, y_val, X_test, y_test
    
    def train_predictor(self, X_train, y_train, X_val, y_val):
        """
        步骤2: 训练并行CNN-LSTM-Attention模型
        """
        logger.info("\n[步骤2] 训练预测模型...")
        
        from ..models.predictor import ParallelCNNLSTMAttention
        
        # 模型参数
        input_shape = (X_train.shape[1], X_train.shape[2])
        model_params = self.config.get('model_params', {})
        
        # 构建模型
        self.predictor = ParallelCNNLSTMAttention(
            input_shape=input_shape,
            **model_params
        )
        
        # 编译
        self.predictor.model.compile(
            optimizer=tf.keras.optimizers.Adam(
                learning_rate=self.config.get('learning_rate', 0.001)
            ),
            loss='mse',
            metrics=['mae', 'mape']
        )
        
        # 回调
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ModelCheckpoint(
                filepath=str(self.output_dir / 'best_model.h5'),
                monitor='val_loss',
                save_best_only=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5
            )
        ]
        
        # 训练
        history = self.predictor.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=self.config.get('epochs', 100),
            batch_size=self.config.get('batch_size', 64),
            callbacks=callbacks,
            verbose=1
        )
        
        # 保存历史
        with open(self.output_dir / 'training_history.json', 'w') as f:
            json.dump(history.history, f, indent=2)
        
        logger.info("模型训练完成")
    
    def evaluate_predictor(self, X_test, y_test):
        """
        步骤3: 评估预测性能
        """
        logger.info("\n[步骤3] 评估预测模型...")
        
        # 预测
        y_pred = self.predictor.predict(X_test)
        
        # 计算指标
        mse = np.mean((y_test - y_pred) ** 2)
        mae = np.mean(np.abs(y_test - y_pred))
        rmse = np.sqrt(mse)
        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100
        
        metrics = {
            'MSE': float(mse),
            'MAE': float(mae),
            'RMSE': float(rmse),
            'MAPE': float(mape)
        }
        
        logger.info(f"预测性能: MSE={mse:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, MAPE={mape:.2f}%")
        
        # 保存指标
        with open(self.output_dir / 'prediction_metrics.json', 'w') as f:
            json.dump(metrics, f, indent=2)
        
        return metrics
    
    def classify_states(self, y_test):
        """
        步骤4: 使用Sn尺度估计器分类EDP状态
        """
        logger.info("\n[步骤4] 状态分类...")
        
        from ..models.state_classifier import SnStateClassifier
        
        # 初始化分类器
        self.classifier = SnStateClassifier(
            n_states=self.config.get('n_states', 3),
            state_names=self.config.get('state_names', ['Lower', 'Normal', 'Peak'])
        )
        
        # 拟合并预测
        edp_states = self.classifier.fit_predict(y_test.flatten())
        
        # 统计分布
        unique, counts = np.unique(edp_states, return_counts=True)
        distribution = dict(zip(unique, counts))
        
        logger.info(f"状态分布: {distribution}")
        
        return edp_states
    
    def discretize_features(self, X_test, edp_states):
        """
        步骤5: 特征离散化
        """
        logger.info("\n[步骤5] 特征离散化...")
        
        from ..models.discretizer import QuantileDiscretizer
        
        # 从序列数据中提取最后时刻的特征
        X_last = X_test[:, -1, :]
        
        # 初始化离散化器
        self.discretizer = QuantileDiscretizer(n_bins=4)
        
        # 离散化
        discrete_features = self.discretizer.fit_transform(X_last)
        
        # 组合为DataFrame
        feature_names = self.preprocessor.feature_cols
        
        discrete_data = pd.DataFrame(
            discrete_features,
            columns=feature_names
        )
        
        # 添加EDP状态
        discrete_data['EDP'] = edp_states
        
        logger.info(f"离散化后数据: {discrete_data.shape}")
        
        return discrete_data
    
    def cluster_dlp(self, X_test, discrete_data):
        """
        步骤6: DLP聚类
        """
        logger.info("\n[步骤6] DLP聚类...")
        
        from ..models.clustering import DLPClusterer, AttentionClusterer
        
        # 提取CAM
        cam_values = self.predictor.extract_cam(X_test)
        
        # CAM聚类
        self.cam_clusterer = DLPClusterer(
            n_clusters=self.config.get('cam_clusters', 3),
            dlp_type='CAM'
        )
        cam_labels = self.cam_clusterer.fit_predict(cam_values)
        
        discrete_data['CAM_type'] = [f'Type{i+1}' for i in cam_labels]
        
        # 提取Attention权重
        att_weights = self.predictor.extract_attention_weights(X_test)
        
        # Attention聚类
        self.att_clusterer = AttentionClusterer(
            n_clusters=self.config.get('att_clusters', 3)
        )
        att_labels = self.att_clusterer.fit_predict(att_weights)
        
        discrete_data['ATT_type'] = self.att_clusterer.cluster_names_[att_labels]
        
        logger.info("DLP聚类完成")
        
        return discrete_data
    
    def mine_association_rules(self, discrete_data):
        """
        步骤7: 关联规则挖掘
        """
        logger.info("\n[步骤7] 关联规则挖掘...")
        
        from ..models.association import AssociationRuleMiner
        
        self.rule_miner = AssociationRuleMiner(
            min_support=self.config.get('min_support', 0.05),
            min_confidence=self.config.get('min_confidence', 0.5),
            min_lift=self.config.get('min_lift', 1.2)
        )
        
        # 准备数据
        df_encoded = self.rule_miner.prepare_data(discrete_data)
        
        # 挖掘
        self.rule_miner.mine_frequent_itemsets(df_encoded)
        rules = self.rule_miner.generate_rules()
        
        # 保存规则
        rules.to_csv(self.output_dir / 'association_rules.csv', index=False)
        
        # 打印摘要
        self.rule_miner.print_rules_summary(n=3)
        
        logger.info(f"挖掘到 {len(rules)} 条规则")
    
    def build_bayesian_network(self, discrete_data):
        """
        步骤8: 贝叶斯网络构建
        """
        logger.info("\n[步骤8] 贝叶斯网络构建...")
        
        from ..models.bayesian_net import CausalBayesianNetwork
        
        # 初始化BN
        domain_knowledge = self.config.get('domain_knowledge', None)
        self.bn = CausalBayesianNetwork(domain_knowledge=domain_knowledge)
        
        # 结构学习
        structure = self.bn.learn_structure(discrete_data)
        logger.info(f"学习到 {len(structure.edges())} 条边")
        
        # 参数学习
        self.bn.learn_parameters(discrete_data)
        
        # 可视化
        self.bn.visualize(filename=str(self.output_dir / 'bn_structure.png'))
        
        logger.info("贝叶斯网络构建完成")
    
    def save_models(self):
        """
        步骤9: 保存所有模型
        """
        logger.info("\n[步骤9] 保存模型...")
        
        models_dir = self.output_dir / 'models'
        models_dir.mkdir(exist_ok=True)
        
        # 保存预处理器
        with open(models_dir / 'preprocessor.pkl', 'wb') as f:
            pickle.dump(self.preprocessor, f)
        
        # 保存预测模型
        self.predictor.save(str(models_dir / 'predictor.h5'))
        
        # 保存分类器
        with open(models_dir / 'classifier.pkl', 'wb') as f:
            pickle.dump(self.classifier, f)
        
        # 保存离散化器
        with open(models_dir / 'discretizer.pkl', 'wb') as f:
            pickle.dump(self.discretizer, f)
        
        # 保存聚类器
        with open(models_dir / 'cam_clusterer.pkl', 'wb') as f:
            pickle.dump(self.cam_clusterer, f)
        
        with open(models_dir / 'att_clusterer.pkl', 'wb') as f:
            pickle.dump(self.att_clusterer, f)
        
        # 保存规则挖掘器
        with open(models_dir / 'rule_miner.pkl', 'wb') as f:
            pickle.dump(self.rule_miner, f)
        
        # 保存贝叶斯网络
        self.bn.save(str(models_dir / 'bayesian_network.pkl'))
        
        logger.info(f"所有模型已保存到 {models_dir}")


# 使用示例
if __name__ == "__main__":
    # 配置
    config = {
        'data_path': 'data/household_power_consumption.txt',
        'output_dir': 'outputs/training',
        
        # 数据参数
        'sequence_length': 60,
        'feature_cols': [
            'GlobalActivePower', 'GlobalReactivePower',
            'Voltage', 'GlobalIntensity',
            'Kitchen', 'Laundry', 'ClimateControl'
        ],
        'target_col': 'GlobalActivePower',
        'train_ratio': 0.7,
        'val_ratio': 0.15,
        
        # 模型参数
        'model_params': {
            'cnn_filters': 64,
            'lstm_units': 128,
            'attention_units': 64,
            'dense_units': [64, 32]
        },
        
        # 训练参数
        'epochs': 100,
        'batch_size': 64,
        'learning_rate': 0.001,
        
        # 其他参数
        'n_states': 3,
        'state_names': ['Lower', 'Normal', 'Peak'],
        'cam_clusters': 3,
        'att_clusters': 3,
        'min_support': 0.05,
        'min_confidence': 0.5,
        'min_lift': 1.2
    }
    
    # 运行流水线
    pipeline = TrainingPipeline(config)
    pipeline.run()
```

---

### 2.11 推理流水线

#### 文件: `src/pipeline/inference_pipeline.py`

```python
"""
推理流水线（单样本预测+因果解释+推荐）
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
import logging

logger = logging.getLogger(__name__)


class InferencePipeline:
    """
    推理流水线
    
    输入：新样本 → 预测 → 因果解释 → 生成推荐
    """
    
    def __init__(self, models_dir: str):
        """
        参数:
            models_dir: 模型目录（包含所有训练好的模型）
        """
        self.models_dir = Path(models_dir)
        
        # 加载所有模型
        self.load_models()
    
    def load_models(self):
        """
        加载所有训练好的模型
        """
        logger.info("加载模型...")
        
        # 预处理器
        with open(self.models_dir / 'preprocessor.pkl', 'rb') as f:
            self.preprocessor = pickle.load(f)
        
        # 预测模型
        from ..models.predictor import ParallelCNNLSTMAttention
        self.predictor = ParallelCNNLSTMAttention.load(
            str(self.models_dir / 'predictor.h5')
        )
        
        # 状态分类器
        with open(self.models_dir / 'classifier.pkl', 'rb') as f:
            self.classifier = pickle.load(f)
        
        # 离散化器
        with open(self.models_dir / 'discretizer.pkl', 'rb') as f:
            self.discretizer = pickle.load(f)
        
        # 聚类器
        with open(self.models_dir / 'cam_clusterer.pkl', 'rb') as f:
            self.cam_clusterer = pickle.load(f)
        
        with open(self.models_dir / 'att_clusterer.pkl', 'rb') as f:
            self.att_clusterer = pickle.load(f)
        
        # 贝叶斯网络
        from ..models.bayesian_net import CausalBayesianNetwork
        self.bn = CausalBayesianNetwork()
        self.bn.load(str(self.models_dir / 'bayesian_network.pkl'))
        
        # 因果推断引擎
        from ..inference.causal_inference import CausalInferenceEngine
        self.causal_engine = CausalInferenceEngine(self.bn)
        
        # 推荐生成器
        from ..inference.recommendation import RecommendationGenerator
        self.recommender = RecommendationGenerator(self.causal_engine)
        
        logger.info("所有模型加载完成")
    
    def predict(self, raw_input: pd.DataFrame) -> dict:
        """
        完整推理流程
        
        输入:
            raw_input: 原始输入数据（可以是单条或多条）
        
        输出:
            推理结果字典
        """
        logger.info("开始推理...")
        
        # 1. 预处理
        X = self.preprocessor.transform(raw_input)
        
        # 2. 预测
        y_pred = self.predictor.predict(X)
        
        # 3. 状态分类
        edp_state = self.classifier.predict(y_pred.flatten())[-1]  # 取最后一个
        
        # 4. 提取DLP
        cam = self.predictor.extract_cam(X)
        att = self.predictor.extract_attention_weights(X)
        
        # 5. DLP聚类
        cam_label = self.cam_clusterer.predict(cam)[-1]
        att_label = self.att_clusterer.predict(att)[-1]
        
        cam_type = f'Type{cam_label + 1}'
        att_type = self.att_clusterer.cluster_names_[att_label]
        
        # 6. 离散化特征
        X_last = X[-1, -1, :]  # 最后一个样本的最后时刻
        discrete_features = self.discretizer.transform(X_last.reshape(1, -1))[0]
        
        # 7. 构建证据
        evidence = {}
        for i, col in enumerate(self.preprocessor.feature_cols):
            evidence[col] = discrete_features[i]
        
        evidence['CAM_type'] = cam_type
        evidence['ATT_type'] = att_type
        
        # 8. 因果推断
        query_result = self.bn.query(
            variables=['EDP'],
            evidence=evidence
        )
        
        # 9. 生成推荐
        recommendations = self.recommender.generate_recommendations(
            current_state=edp_state,
            evidence=evidence,
            top_n=3
        )
        
        # 10. 整合结果
        result = {
            'prediction': {
                'value': float(y_pred[-1]),
                'state': edp_state
            },
            'dlp': {
                'cam_type': cam_type,
                'attention_type': att_type
            },
            'causal_probability': {
                state: float(prob)
                for state, prob in zip(
                    query_result.state_names['EDP'],
                    query_result.values
                )
            },
            'evidence': evidence,
            'recommendations': recommendations,
            'recommendation_text': self.recommender.format_recommendations(recommendations)
        }
        
        logger.info(f"推理完成: 预测={y_pred[-1]:.4f}, 状态={edp_state}")
        
        return result
    
    def explain(self, raw_input: pd.DataFrame, detailed: bool = False) -> dict:
        """
        详细解释（包含敏感性分析、反事实等）
        
        输入:
            raw_input: 原始输入
            detailed: 是否包含详细分析
        
        输出:
            解释结果
        """
        # 基础推理
        result = self.predict(raw_input)
        
        if not detailed:
            return result
        
        logger.info("生成详细解释...")
        
        evidence = result['evidence']
        state = result['prediction']['state']
        
        # 敏感性分析
        sensitivity = self.causal_engine.sensitivity_analysis(
            target='EDP',
            variables=list(self.preprocessor.feature_cols),
            baseline=evidence
        )
        
        result['sensitivity'] = sensitivity.to_dict()
        
        # 关键因素解释
        key_factors = self.causal_engine.explain_state(
            target='EDP',
            target_state=state,
            evidence=evidence,
            top_n=5
        )
        
        result['key_factors'] = key_factors.to_dict()
        
        logger.info("详细解释生成完成")
        
        return result
    
    def batch_predict(self, raw_inputs: pd.DataFrame) -> list:
        """
        批量推理
        
        输入:
            raw_inputs: 多条原始输入
        
        输出:
            结果列表
        """
        results = []
        
        for i in range(len(raw_inputs)):
            sample = raw_inputs.iloc[i:i+1]
            result = self.predict(sample)
            results.append(result)
        
        return results


# 使用示例
if __name__ == "__main__":
    # 初始化流水线
    pipeline = InferencePipeline(models_dir='outputs/training/models')
    
    # 准备测试数据
    test_input = pd.DataFrame({
        'Date': ['2025-06-15 14:30:00'],
        'GlobalActivePower': [4.5],
        'GlobalReactivePower': [0.3],
        'Voltage': [240.0],
        'GlobalIntensity': [18.0],
        'Kitchen': [2.0],
        'Laundry': [1.5],
        'ClimateControl': [3.5]
    })
    
    # 推理
    result = pipeline.predict(test_input)
    
    print("\n=== 推理结果 ===")
    print(f"预测值: {result['prediction']['value']:.4f}")
    print(f"状态: {result['prediction']['state']}")
    print(f"\nDLP: CAM={result['dlp']['cam_type']}, Attention={result['dlp']['attention_type']}")
    print(f"\n因果概率: {result['causal_probability']}")
    print(f"\n{result['recommendation_text']}")
    
    # 详细解释
    detailed_result = pipeline.explain(test_input, detailed=True)
    
    print("\n=== 敏感性分析 ===")
    for var, score in detailed_result['sensitivity'].items():
        print(f"{var}: {score:.4f}")
```

---

## 3. 总结

### 3.1 模块间依赖关系

```
数据预处理 (preprocessing/data_preprocessor.py)
    ↓
预测模型 (models/predictor.py)
    ↓
状态分类 (models/state_classifier.py)
    ↓
离散化 (models/discretizer.py) + DLP聚类 (models/clustering.py)
    ↓
关联规则 (models/association.py) → 贝叶斯网络 (models/bayesian_net.py)
    ↓
因果推断 (inference/causal_inference.py)
    ↓
推荐生成 (inference/recommendation.py)
    ↓
流水线集成 (pipeline/train_pipeline.py, pipeline/inference_pipeline.py)
```

### 3.2 关键技术要点

1. **并行架构**: CNN和LSTM-Attention并行处理，通过融合层整合
2. **稳健分类**: Sn尺度估计器处理异常值
3. **DLP解释**: CAM和Attention权重聚类提供可解释性
4. **因果推断**: 贝叶斯网络结合领域知识约束
5. **一致性**: 解释结果高度稳定（论文中达到0.999+余弦相似度）

### 3.3 下一步工作

参考《项目设计文档.md》第11节"开发计划"，按照以下顺序实现：

1. ✅ **完成实现文档编写**（当前任务）
2. ⬜ 搭建项目目录结构
3. ⬜ 实现数据预处理模块
4. ⬜ 实现并行CNN-LSTM-Attention模型
5. ⬜ 实现状态分类和离散化
6. ⬜ 实现DLP聚类
7. ⬜ 实现关联规则和贝叶斯网络
8. ⬜ 实现因果推断和推荐生成
9. ⬜ 集成训练和推理流水线
10. ⬜ 单元测试和集成测试
11. ⬜ 数据集准备和模型训练
12. ⬜ 性能评估和调优

---

**文档版本**: v1.0  
**最后更新**: 2025-01-XX  
**作者**: Severin YE

