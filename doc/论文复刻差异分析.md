# è®ºæ–‡å¤åˆ»å·®å¼‚åˆ†ææŠ¥å‘Š

ç”Ÿæˆæ—¶é—´: 2026-02-02
è®ºæ–‡: Erlangga & Cho (2025) - Causally Explainable AI on Deep Learning Model for Energy Demand Prediction

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šå¯¹æ¯”äº†è®ºæ–‡ã€Šå› æœå¯è§£é‡Šäººå·¥æ™ºèƒ½ç”¨äºèƒ½æºéœ€æ±‚é¢„æµ‹ã€‹ä¸å½“å‰é¡¹ç›®å®ç°ä¹‹é—´çš„å·®å¼‚ã€‚é€šè¿‡è¯¦ç»†åˆ†æï¼Œè¯†åˆ«å‡º**å…³é”®å·®å¼‚ç‚¹**å’Œ**éœ€è¦æ”¹è¿›çš„åœ°æ–¹**ã€‚

### æ€»ä½“è¯„ä¼°
- âœ… **æ ¸å¿ƒæ¶æ„**: å·²æ­£ç¡®å®ç°å¹¶è¡ŒCNN-LSTM-Attention
- âœ… **å› æœè§£é‡Š**: è´å¶æ–¯ç½‘ç»œæ¡†æ¶å·²æ­å»º
- âš ï¸ **æ¨¡å‹ç»†èŠ‚**: éƒ¨åˆ†å®ç°ä¸è®ºæ–‡æœ‰å‡ºå…¥
- âš ï¸ **å®éªŒç»“æœ**: æ€§èƒ½æŒ‡æ ‡å­˜åœ¨æ˜¾è‘—å·®å¼‚
- âŒ **æ•°æ®é›†**: ä»…ä½¿ç”¨UCIæ•°æ®é›†ï¼Œæœªä½¿ç”¨REFIT

---

## 1. é¢„æµ‹æ¨¡å‹å®ç°å·®å¼‚

### 1.1 æ³¨æ„åŠ›æœºåˆ¶å®ç° âš ï¸

#### è®ºæ–‡åŸå§‹å…¬å¼ï¼ˆç¬¬3èŠ‚ï¼Œå…¬å¼1-2ï¼‰
```
è®ºæ–‡å…¬å¼(1): a_n = exp(fc(o_n, h_N)) / Î£_{k=1}^N exp(fc(o_k, h_N))
è®ºæ–‡å…¬å¼(2): c_N = Î£_{k=1}^N a_k Â· h_k
```

#### å½“å‰é¡¹ç›®å®ç°
```python
# src/models/predictor.py L48-61
score = tf.nn.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)
attention_weights = tf.nn.softmax(tf.tensordot(score, self.u, axes=1), axis=1)
context_vector = tf.reduce_sum(inputs * tf.expand_dims(attention_weights, -1), axis=1)
```

#### å·®å¼‚åˆ†æ
| ç»´åº¦ | è®ºæ–‡ | é¡¹ç›®å®ç° | å·®å¼‚ |
|------|------|----------|------|
| **è¾“å…¥** | `(o_n, h_N)` - å½“å‰è¾“å‡ºä¸æœ€ç»ˆéšè—çŠ¶æ€ | `inputs` - LSTMå…¨éƒ¨éšè—çŠ¶æ€ | âŒ æœªä½¿ç”¨æœ€ç»ˆéšè—çŠ¶æ€h_N |
| **è¯„åˆ†å‡½æ•°** | `fc(o_n, h_N)` - æ˜¾å¼å»ºæ¨¡ç›¸å…³æ€§ | `tanh(WÂ·h + b)Â·u` - è‡ªæ³¨æ„åŠ› | âš ï¸ ç¼ºå°‘ä¸h_Nçš„å¯¹æ¯” |
| **ä¸Šä¸‹æ–‡å‘é‡** | `Î£ a_kÂ·h_k` åŠ æƒæ±‚å’Œ | `Î£ a_kÂ·h_k` åŠ æƒæ±‚å’Œ | âœ… ç›¸åŒ |

**å…³é”®é—®é¢˜**: 
- è®ºæ–‡å¼ºè°ƒä½¿ç”¨**æœ€ç»ˆéšè—çŠ¶æ€h_N**ä½œä¸ºæŸ¥è¯¢å‘é‡ï¼ˆqueryï¼‰ï¼Œæ¥è¡¡é‡æ¯ä¸ªæ—¶é—´æ­¥o_nçš„é‡è¦æ€§
- å½“å‰å®ç°ä½¿ç”¨çš„æ˜¯æ ‡å‡†è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœªä½“ç°"ä¸æœ€ç»ˆçŠ¶æ€å¯¹æ¯”"çš„æ€æƒ³
- **å½±å“**: å¯èƒ½å¯¼è‡´æ³¨æ„åŠ›æƒé‡çš„åˆ†å¸ƒä¸è®ºæ–‡ä¸åŒ

**å»ºè®®ä¿®æ­£**:
```python
# åº”è¯¥æ”¹ä¸º:
lstm_output = self.lstm(inputs)  # [batch, seq, units]
final_hidden_state = lstm_output[:, -1, :]  # h_N

# è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥ä¸h_Nçš„ç›¸å…³æ€§
score = fc([lstm_output, final_hidden_state])  # è®ºæ–‡ä¸­çš„fc(o_n, h_N)
attention_weights = softmax(score)
context = sum(attention_weights * lstm_output)
```

---

### 1.2 CAMæå–å®ç° âœ…

#### è®ºæ–‡æè¿°ï¼ˆç¬¬3.3èŠ‚ï¼Œå…¬å¼10ï¼‰
```
Activation Map = Ï†^{Layer1}_{MLP}(concat(flattened(Ï†^l_{Conv}), 0_N))
```

#### å½“å‰å®ç°
```python
# src/models/predictor.py L193-207
cnn_conv2_output = self.model.get_layer('cnn_conv2').output
cam_output = self.cam_model.predict(X)
cam = np.mean(cam_output, axis=-1)  # å…¨å±€å¹³å‡æ± åŒ–
```

#### åˆ†æ
- âœ… æ­£ç¡®ä½¿ç”¨å·ç§¯å±‚è¾“å‡º
- âœ… ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–ï¼ˆGAPï¼‰
- âš ï¸ è®ºæ–‡æè¿°çš„0_Nï¼ˆé›¶å‘é‡ï¼‰æ›¿ä»£æ–¹æ¡ˆæœªå®Œå…¨å®ç°
- **å»ºè®®**: å¯è€ƒè™‘å®ç°è®ºæ–‡ä¸­çš„MLP Layer1æ–¹æ³•ï¼Œä½†å½“å‰GAPæ–¹æ³•ä¹Ÿåˆç†

---

### 1.3 æ¨¡å‹è¶…å‚æ•°å¯¹æ¯”

| å‚æ•° | è®ºæ–‡ | é¡¹ç›®å®ç° | å·®å¼‚ |
|------|------|----------|------|
| **çª—å£é•¿åº¦** | Ï‰=80 | 100 | âš ï¸ ä¸ä¸€è‡´ |
| **é¢„æµ‹æ­¥é•¿** | l=1 | 1 | âœ… ä¸€è‡´ |
| **CNNå·ç§¯æ ¸** | 64, 128 | 64, 32 | âŒ ç¬¬äºŒå±‚ä¸åŒ |
| **LSTMå•å…ƒ** | 128 | 64 | âŒ å•å…ƒæ•°å‡åŠ |
| **Attentionå•å…ƒ** | 64 | 25 | âŒ æ˜¾è‘—å‡å°‘ |
| **å­¦ä¹ ç‡** | æœªæ˜ç¡® | 0.001 | - |
| **æ‰¹å¤§å°** | æœªæ˜ç¡® | 32 | - |

**å…³é”®é—®é¢˜**:
1. **çª—å£é•¿åº¦**: è®ºæ–‡æ˜ç¡®ä½¿ç”¨80ï¼Œé¡¹ç›®ç”¨100 â†’ æ—¶é—´ç»´åº¦ä¸ä¸€è‡´
2. **æ¨¡å‹å®¹é‡**: CNNã€LSTMã€Attentionå•å…ƒæ•°æ™®éåå° â†’ å¯èƒ½å½±å“ç‰¹å¾æå–èƒ½åŠ›

---

## 2. çŠ¶æ€åˆ†ç±»å®ç°å·®å¼‚

### 2.1 Snå°ºåº¦ä¼°è®¡å™¨ âš ï¸

#### è®ºæ–‡å…¬å¼ï¼ˆå…¬å¼5-7ï¼‰
```
Sn = c Â· median_m { median_n {|x_m - x_n|} }
c = 1.1926

Z_{Sn}(x_i) = |x_i - MED(X)| / (Î± Ã— Sn)
Î± = 1.4285
```

#### å½“å‰å®ç°
```python
# src/models/state_classifier.py L38-55
diffs = []
for i in range(min(n, 1000)):  # âš ï¸ é™åˆ¶ä¸º1000
    diffs.append(np.median(np.abs(data[i] - data)))
sn = np.median(diffs)
c = 1.1926
return c * sn
```

#### å·®å¼‚åˆ†æ
| é¡¹ç›® | è®ºæ–‡ | å®ç° | å½±å“ |
|------|------|------|------|
| **è®¡ç®—èŒƒå›´** | å…¨éƒ¨æ•°æ® | min(n, 1000) | âš ï¸ å¤§æ•°æ®é›†æ—¶è¿‘ä¼¼ |
| **ä¿®æ­£å› å­c** | 1.1926 | 1.1926 | âœ… ä¸€è‡´ |
| **Î±ç³»æ•°** | 1.4285 | æœªæ‰¾åˆ° | âŒ Zåˆ†æ•°è®¡ç®—å¯èƒ½æœ‰è¯¯ |

**å…³é”®é—®é¢˜**: 
- è®ºæ–‡ä¸­Zåˆ†æ•°éœ€é™¤ä»¥Î±Ã—Snï¼Œå½“å‰ä»£ç æœªè§Î±=1.4285çš„ä½¿ç”¨
- çŠ¶æ€åˆ†ç±»æ”¹ç”¨K-meansè€Œéé˜ˆå€¼åˆ¤å®š â†’ ä¸è®ºæ–‡æ–¹æ³•ä¸åŒ

---

### 2.2 çŠ¶æ€åˆ¤å®šé€»è¾‘ âŒ

#### è®ºæ–‡æ–¹æ³•ï¼ˆå…¬å¼7ï¼‰
```python
if Z_Sn(x_i) > threshold and Z_Sn(x_i) > m_x:
    state = "Peak"
elif Z_Sn(x_i) > threshold and Z_Sn(x_i) < m_x:
    state = "Lower"
else:
    state = "Normal"
```

#### å½“å‰å®ç°
```python
# src/models/state_classifier.py L68-77
# ä½¿ç”¨K-meansèšç±»ä»£æ›¿é˜ˆå€¼åˆ¤å®š
data_normalized = (data - self.median_) / self.sn_scale_
self.kmeans.fit(data_normalized.reshape(-1, 1))
```

**å·®å¼‚**: 
- âŒ å®Œå…¨ä¸åŒçš„åˆ†ç±»ç­–ç•¥
- è®ºæ–‡: åŸºäºé²æ£’Zåˆ†æ•°+é˜ˆå€¼ï¼ˆæ˜ç¡®çš„ç»Ÿè®¡åˆ¤å®šï¼‰
- é¡¹ç›®: åŸºäºK-meansèšç±»ï¼ˆæ•°æ®é©±åŠ¨ï¼‰

**å½±å“**: 
- çŠ¶æ€åˆ†ç±»ç»“æœå¯èƒ½ä¸ä¸€è‡´
- è®ºæ–‡ä¸­çš„thresholdã€m_xå‚æ•°æœªåœ¨ä»£ç ä¸­ä½“ç°

---

## 3. è´å¶æ–¯ç½‘ç»œå®ç°å·®å¼‚

### 3.1 ç»“æ„å­¦ä¹ çº¦æŸ âœ…

#### è®ºæ–‡æè¿°ï¼ˆç¬¬3.3èŠ‚å›¾2ï¼‰
- ç™½åå•: é¢†åŸŸçŸ¥è¯†è¾¹ + å…³è”è§„åˆ™è¾¹
- é»‘åå•: ç¦æ­¢æ—¶é—´å€’æµã€ç¦æ­¢å­èŠ‚ç‚¹â†’çˆ¶èŠ‚ç‚¹
- ä¸»é¢˜åˆ†ç»„: ç‰©ç†ç¯å¢ƒâ†’ç”µå™¨ä½¿ç”¨â†’ç”µåŠ›æ¶ˆè€—â†’å³°å€¼éœ€æ±‚

#### å½“å‰å®ç°
```python
# src/models/bayesian_net.py L76-91
white_list = self.domain_edges + candidate_edges
black_list = self.forbidden_edges
```

âœ… **ç¬¦åˆè®ºæ–‡è¦æ±‚**ï¼Œæ”¯æŒé¢†åŸŸçŸ¥è¯†çº¦æŸ

---

### 3.2 æ·±åº¦å­¦ä¹ å‚æ•°ï¼ˆDLPï¼‰é›†æˆ âœ…

#### è®ºæ–‡æ–¹æ³•ï¼ˆç¬¬3.3èŠ‚ï¼‰
- CAMèšç±» â†’ SCAMç¬¦å·
- Attentionèšç±» â†’ SATTç¬¦å·
- DLPä¸EDPç›´æ¥è¿æ¥

#### å½“å‰å®ç°
```python
# src/models/clustering.py
class DLPClusterer:  # CAMèšç±»
class AttentionClusterer:  # Attentionèšç±»
```

âœ… **ç¬¦åˆè®ºæ–‡æ¡†æ¶**

---

### 3.3 å› æœæ¨æ–­ âœ…

#### è®ºæ–‡æåŠ
- Do-æ¼”ç®—: P(Y | do(X))
- åäº‹å®åˆ†æ
- æ•æ„Ÿæ€§åˆ†æï¼ˆé¾™å·é£å›¾ï¼‰

#### å½“å‰å®ç°
```python
# src/models/bayesian_net.py L218-269
def do_calculus(intervention, query_var, evidence):
    # ç§»é™¤æŒ‡å‘å¹²é¢„å˜é‡çš„è¾¹
    for parent in intervened_model.get_parents(var):
        intervened_model.remove_edge(parent, var)
    # æ¨æ–­
```

âœ… **æ­£ç¡®å®ç°do-æ¼”ç®—**

---

## 4. å®éªŒç»“æœå¯¹æ¯”

### 4.1 æ€§èƒ½æŒ‡æ ‡å¯¹æ¯” âŒ

#### è®ºæ–‡æŠ¥å‘Šçš„æ€§èƒ½æå‡
- **UCIæ•°æ®é›†**: ç›¸æ¯”ä¸²è”CNN-LSTMå¹³å‡æå‡ **34.84%**
- **REFITæ•°æ®é›†**: ç›¸æ¯”ä¸²è”CNN-LSTMå¹³å‡æå‡ **13.63%**

#### å½“å‰å®éªŒç»“æœ
```
outputs/inference/26-01-18/26-01-18_09-58/inference_report.txt:
MAE: 0.6919 kW
RMSE: 0.8842 kW
MAPE: 151.17%
```

#### é—®é¢˜
1. **æ— å¯¹ç…§å®éªŒ**: æœªæä¾›ä¸²è”CNN-LSTMçš„baselineç»“æœ
2. **MAPEå¼‚å¸¸é«˜**: 151.17%è¿œè¶…æ­£å¸¸èŒƒå›´ï¼ˆè®ºæ–‡æœªæä¾›MAPEï¼Œä½†é€šå¸¸åº”<20%ï¼‰
3. **æµ‹è¯•æ ·æœ¬å°‘**: ä»…80ä¸ªæ ·æœ¬ï¼Œç»Ÿè®¡æ˜¾è‘—æ€§ä¸è¶³

---

### 4.2 å¯è§£é‡Šæ€§è¯„ä¼°å¯¹æ¯”

#### è®ºæ–‡æŒ‡æ ‡ï¼ˆè¡¨5ï¼‰
```
è§£é‡Šä¸€è‡´æ€§ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰:
- Peak: 0.99940
- Normal: 0.99983
- Lower: 0.99974

ä¼˜äºå¯¹æ¯”æ–¹æ³•:
- SHAP: 0.95-0.96
- LIME: 0.70-0.75
- PD Variance: 0.81-0.96
```

#### å½“å‰å®ç°
- âŒ æœªæ‰¾åˆ°è§£é‡Šä¸€è‡´æ€§è¯„ä¼°ä»£ç 
- âŒ æœªå®ç°ä¸SHAP/LIMEçš„å¯¹æ¯”å®éªŒ

---

## 5. æ•°æ®é›†ä½¿ç”¨å·®å¼‚ âŒ

### è®ºæ–‡ä½¿ç”¨æ•°æ®é›†
1. **UCI Individual Household Dataset**
   - 2,075,259æ¡è®°å½•
   - 1åˆ†é’Ÿåˆ†è¾¨ç‡
   - 8ä¸ªå±æ€§
   
2. **REFIT Dataset**
   - 5,733,526æ¡è®°å½•
   - 8ç§’åˆ†è¾¨ç‡
   - 11ä¸ªå±æ€§

### å½“å‰é¡¹ç›®
- âœ… UCIæ•°æ®é›†å·²ä½¿ç”¨
- âŒ REFITæ•°æ®é›†æœªå®ç°
- âš ï¸ æ—¶é—´åˆ†è¾¨ç‡: è®ºæ–‡15åˆ†é’Ÿï¼Œé¡¹ç›®æœªæ˜ç¡®

---

## 6. å…³é”®å·®å¼‚æ€»ç»“è¡¨

| æ¨¡å— | è®ºæ–‡ | å®ç°çŠ¶æ€ | ä¼˜å…ˆçº§ |
|------|------|----------|--------|
| æ³¨æ„åŠ›æœºåˆ¶ | fc(o_n, h_N)ç›¸å…³æ€§ | è‡ªæ³¨æ„åŠ›æœºåˆ¶ | ğŸ”´ é«˜ |
| çª—å£é•¿åº¦ | 80 | 100 | ğŸŸ¡ ä¸­ |
| æ¨¡å‹å®¹é‡ | CNN(64,128), LSTM(128) | CNN(64,32), LSTM(64) | ğŸŸ¡ ä¸­ |
| çŠ¶æ€åˆ†ç±» | Sné˜ˆå€¼åˆ¤å®š | K-meansèšç±» | ğŸ”´ é«˜ |
| Î±ç³»æ•° | 1.4285 | æœªä½¿ç”¨ | ğŸŸ¡ ä¸­ |
| å¯¹ç…§å®éªŒ | å¤šç§baseline | æ—  | ğŸ”´ é«˜ |
| REFITæ•°æ®é›† | å·²ä½¿ç”¨ | æœªä½¿ç”¨ | ğŸŸ¢ ä½ |
| è§£é‡Šä¸€è‡´æ€§è¯„ä¼° | å·²å®ç° | æœªå®ç° | ğŸŸ¡ ä¸­ |

---

## 7. å®éªŒç»“æœå·®å¼‚åŸå› åˆ†æ

### 7.1 ä¸ºä»€ä¹ˆMAPEè¾¾åˆ°151.17%ï¼Ÿ

#### å¯èƒ½åŸå› 
1. **æ•°æ®æ³„éœ²**: é¢„æµ‹å€¼ä¸å®é™…å€¼é‡çº§ä¸åŒ¹é…
   ```
   é¢„æµ‹èŒƒå›´: [0.391, 2.862] kW
   å®é™…èŒƒå›´: [0.174, 2.391] kW
   éƒ¨åˆ†æ ·æœ¬è¯¯å·®: +532.0%ï¼ˆæ ·æœ¬79ï¼‰
   ```

2. **å½’ä¸€åŒ–é—®é¢˜**: 
   - å¯èƒ½è®­ç»ƒæ—¶å½’ä¸€åŒ–ï¼Œæ¨ç†æ—¶æœªæ­£ç¡®åå½’ä¸€åŒ–
   - æˆ–ä½¿ç”¨äº†é”™è¯¯çš„å½’ä¸€åŒ–å‚æ•°

3. **æ¨¡å‹è¿‡æ‹Ÿåˆ**: 
   - æµ‹è¯•é›†ä»…80ä¸ªæ ·æœ¬
   - ä¸ªåˆ«æç«¯æ ·æœ¬å½±å“è¿‡å¤§

4. **çª—å£é•¿åº¦ä¸åŒ¹é…**: 
   - è®ºæ–‡80ï¼Œå®ç°100
   - å¯¼è‡´æ—¶é—´ä¾èµ–å»ºæ¨¡é”™è¯¯

---

### 7.2 ä¸ºä»€ä¹ˆæœªè¾¾åˆ°è®ºæ–‡æ€§èƒ½æå‡ï¼Ÿ

#### æ¨æµ‹åŸå› 
1. **æ³¨æ„åŠ›æœºåˆ¶å·®å¼‚**: 
   - æœªä½¿ç”¨æœ€ç»ˆéšè—çŠ¶æ€h_N
   - å…³é”®æ—¶é—´ç‚¹è¯†åˆ«ä¸å‡†ç¡®

2. **æ¨¡å‹å®¹é‡ä¸è¶³**: 
   - LSTMå•å…ƒæ•°å‡åŠï¼ˆ128â†’64ï¼‰
   - Attentionå•å…ƒæ˜¾è‘—å‡å°‘ï¼ˆ64â†’25ï¼‰
   - ç‰¹å¾æå–èƒ½åŠ›å—é™

3. **çŠ¶æ€åˆ†ç±»æ–¹æ³•æ”¹å˜**: 
   - K-means vs è®ºæ–‡çš„é˜ˆå€¼æ³•
   - å¯èƒ½å¯¼è‡´Peak/Normal/Loweråˆ†å¸ƒä¸åŒ

4. **è¶…å‚æ•°æœªè°ƒä¼˜**: 
   - å­¦ä¹ ç‡ã€æ‰¹å¤§å°ç­‰æœªåŒ¹é…è®ºæ–‡
   - è®­ç»ƒepochæ•°å¯èƒ½ä¸è¶³ï¼ˆé¡¹ç›®ä»…10ï¼‰

---

## 8. æ”¹è¿›å»ºè®®

### 8.1 ç´§æ€¥ä¿®å¤é¡¹ï¼ˆä¼˜å…ˆçº§ğŸ”´ï¼‰

1. **ä¿®æ­£æ³¨æ„åŠ›æœºåˆ¶**
   ```python
   # ä¿®æ”¹ src/models/predictor.py
   def call(self, inputs):
       lstm_out = self.lstm(inputs)  # [B, T, D]
       h_final = lstm_out[:, -1, :]  # æœ€ç»ˆéšè—çŠ¶æ€
       
       # æ¯ä¸ªæ—¶é—´æ­¥ä¸h_finalçš„ç›¸å…³æ€§
       scores = tf.matmul(lstm_out, tf.expand_dims(h_final, -1))
       attention_weights = tf.nn.softmax(scores, axis=1)
       context = tf.reduce_sum(lstm_out * attention_weights, axis=1)
   ```

2. **æ¢å¤çŠ¶æ€åˆ†ç±»é˜ˆå€¼æ³•**
   ```python
   # ä¿®æ”¹ src/models/state_classifier.py
   def classify_state(self, y_pred, window_data):
       median = np.median(window_data)
       sn = self.compute_sn_scale(window_data)
       alpha = 1.4285  # è®ºæ–‡å‚æ•°
       z_score = abs(y_pred - median) / (alpha * sn)
       
       if z_score > threshold and y_pred > median:
           return "Peak"
       elif z_score > threshold and y_pred < median:
           return "Lower"
       else:
           return "Normal"
   ```

3. **å¢åŠ Baselineå¯¹æ¯”å®éªŒ**
   - å®ç°ä¸²è”CNN-LSTMæ¨¡å‹
   - è®¡ç®—34.84%æå‡æ˜¯å¦å¯é‡ç°

---

### 8.2 é‡è¦ä¼˜åŒ–é¡¹ï¼ˆä¼˜å…ˆçº§ğŸŸ¡ï¼‰

1. **è°ƒæ•´æ¨¡å‹è¶…å‚æ•°**
   ```json
   {
     "sequence_length": 80,  // æ”¹ä¸ºè®ºæ–‡çš„80
     "cnn_filters": [64, 128],  // ç¬¬äºŒå±‚æ”¹ä¸º128
     "lstm_units": 128,  // æ”¹ä¸º128
     "attention_units": 64,  // æ”¹ä¸º64
     "epochs": 50,  // å¢åŠ è®­ç»ƒè½®æ•°
     "batch_size": 64  // å¯å°è¯•æ›´å¤§æ‰¹æ¬¡
   }
   ```

2. **å®ç°è§£é‡Šä¸€è‡´æ€§è¯„ä¼°**
   ```python
   # æ–°å¢ src/evaluation/consistency.py
   def evaluate_consistency(bn_model, train_data, test_data):
       train_explanations = extract_feature_importance(train_data)
       test_explanations = extract_feature_importance(test_data)
       
       cosine_sim = cosine_similarity(train_explanations, test_explanations)
       return cosine_sim
   ```

3. **ä¿®å¤å½’ä¸€åŒ–æµç¨‹**
   - æ£€æŸ¥è®­ç»ƒæ—¶çš„Scaleræ˜¯å¦æ­£ç¡®ä¿å­˜
   - æ¨ç†æ—¶ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„Scaler

---

### 8.3 æ‰©å±•é¡¹ï¼ˆä¼˜å…ˆçº§ğŸŸ¢ï¼‰

1. **æ·»åŠ REFITæ•°æ®é›†æ”¯æŒ**
   - ä¸‹è½½REFITæ•°æ®é›†
   - ç¼–å†™æ•°æ®åŠ è½½å™¨
   - å¤ç°13.63%æå‡

2. **å®ç°ä¸XAIæ–¹æ³•çš„å¯¹æ¯”**
   - é›†æˆSHAPåº“
   - é›†æˆLIMEåº“
   - å¯¹æ¯”è§£é‡Šè´¨é‡

---

## 9. ä»£ç ä¿®æ”¹æ¸…å•

### 9.1 å¿…é¡»ä¿®æ”¹çš„æ–‡ä»¶
1. `src/models/predictor.py`
   - Line 48-61: ä¿®æ­£æ³¨æ„åŠ›è®¡ç®—
   - å‚æ•°: è°ƒæ•´LSTMå•å…ƒã€CNNå·ç§¯æ ¸

2. `src/models/state_classifier.py`
   - Line 68-77: æ”¹ä¸ºé˜ˆå€¼åˆ¤å®šæ³•
   - æ·»åŠ Î±=1.4285å¸¸é‡

3. `scripts/run_training.py`
   - æ›´æ–°è¶…å‚æ•°é…ç½®

### 9.2 éœ€è¦æ–°å¢çš„æ–‡ä»¶
1. `src/evaluation/consistency.py` - è§£é‡Šä¸€è‡´æ€§è¯„ä¼°
2. `src/models/baseline_models.py` - ä¸²è”CNN-LSTMå¯¹ç…§
3. `scripts/run_ablation_study.py` - æ¶ˆèå®éªŒè„šæœ¬

---

## 10. å®éªŒéªŒè¯è®¡åˆ’

### é˜¶æ®µ1: ä¿®æ­£æ ¸å¿ƒå·®å¼‚
- [ ] ä¿®æ”¹æ³¨æ„åŠ›æœºåˆ¶
- [ ] ä¿®æ”¹çŠ¶æ€åˆ†ç±»æ–¹æ³•
- [ ] è°ƒæ•´æ¨¡å‹è¶…å‚æ•°è‡³è®ºæ–‡é…ç½®
- [ ] é‡æ–°è®­ç»ƒæ¨¡å‹

### é˜¶æ®µ2: æ€§èƒ½å¯¹æ¯”
- [ ] å®ç°ä¸²è”CNN-LSTM baseline
- [ ] åœ¨UCIæ•°æ®é›†ä¸Šå¤ç°34.84%æå‡
- [ ] è®°å½•è¯¦ç»†çš„è®­ç»ƒæ—¥å¿—

### é˜¶æ®µ3: å®Œæ•´éªŒè¯
- [ ] å®ç°è§£é‡Šä¸€è‡´æ€§è¯„ä¼°
- [ ] ä¸SHAP/LIMEå¯¹æ¯”
- [ ] ç”Ÿæˆè®ºæ–‡å›¾è¡¨å¤ç°

---

## 11. ç»“è®º

### ä¸»è¦å‘ç°
1. âœ… **æ•´ä½“æ¶æ„æ­£ç¡®**: å¹¶è¡ŒCNN-LSTM-Attention + è´å¶æ–¯ç½‘ç»œ
2. âš ï¸ **ç»†èŠ‚å®ç°åå·®**: æ³¨æ„åŠ›æœºåˆ¶ã€çŠ¶æ€åˆ†ç±»ä¸è®ºæ–‡ä¸ä¸€è‡´
3. âš ï¸ **æ¨¡å‹å®¹é‡ä¸è¶³**: å„å±‚å•å…ƒæ•°æ™®éåå°
4. âŒ **å®éªŒä¸å®Œæ•´**: ç¼ºå°‘baselineå¯¹æ¯”ã€REFITæ•°æ®é›†ã€ä¸€è‡´æ€§è¯„ä¼°

### æ€§èƒ½å·®è·åŸå› 
1. **æ³¨æ„åŠ›æœºåˆ¶**: æœªä½¿ç”¨h_Nå¯¼è‡´å…³é”®æ—¶é—´ç‚¹è¯†åˆ«ä¸å‡†
2. **çŠ¶æ€åˆ†ç±»**: K-means vs é˜ˆå€¼æ³•ï¼Œåˆ†ç±»é€»è¾‘ä¸åŒ
3. **è¶…å‚æ•°**: çª—å£é•¿åº¦ã€æ¨¡å‹å®¹é‡ä¸è®ºæ–‡ä¸åŒ¹é…
4. **å®éªŒè®¾è®¡**: æµ‹è¯•æ ·æœ¬è¿‡å°‘ï¼ˆ80ä¸ªï¼‰ï¼Œç»Ÿè®¡ä¸æ˜¾è‘—

### ä¼˜å…ˆæ”¹è¿›æ–¹å‘
1. ğŸ”´ **ä¿®æ­£æ³¨æ„åŠ›æœºåˆ¶** - å¯¹å‡†è®ºæ–‡å…¬å¼
2. ğŸ”´ **æ¢å¤é˜ˆå€¼åˆ¤å®š** - ä½¿ç”¨Sn+Î±ç³»æ•°
3. ğŸ”´ **å¢åŠ å¯¹ç…§å®éªŒ** - éªŒè¯34.84%æå‡
4. ğŸŸ¡ **è°ƒæ•´è¶…å‚æ•°** - åŒ¹é…è®ºæ–‡é…ç½®

### é¢„æœŸæ•ˆæœ
å®Œæˆä¸Šè¿°ä¿®æ”¹åï¼Œé¢„è®¡ï¼š
- MAE/RMSEæŒ‡æ ‡å°†æ˜¾è‘—æ”¹å–„
- MAPEé™è‡³åˆç†èŒƒå›´ï¼ˆ<20%ï¼‰
- å¯å¤ç°è®ºæ–‡æ€§èƒ½æå‡ï¼ˆæ¥è¿‘34.84%ï¼‰
- è§£é‡Šä¸€è‡´æ€§è¾¾åˆ°0.99+æ°´å¹³

---

**æŠ¥å‘Šç”Ÿæˆäºº**: GitHub Copilot  
**å®¡æ ¸çŠ¶æ€**: å¾…äººå·¥éªŒè¯  
**ä¸‹ä¸€æ­¥è¡ŒåŠ¨**: æŒ‰ä¼˜å…ˆçº§å®æ–½ä¿®æ”¹å»ºè®®
