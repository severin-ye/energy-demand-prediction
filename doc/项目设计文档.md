# 能源需求预测的因果可解释AI系统 - 项目设计文档

## 1. 项目概述

### 1.1 项目目标
复现论文《Causally explainable artificial intelligence on deep learning model for energy demand prediction》（Erlangga & Cho, 2025），构建一个不仅能准确预测能源需求，还能提供因果解释和可执行建议的AI系统。

**论文出处**: Engineering Applications of Artificial Intelligence, Vol. 162, 2025

### 1.2 核心特点
- **预测能力**：基于并行CNN-LSTM-Attention架构的高精度时序预测
  - 相比串联CNN-LSTM改进：UCI数据集平均**34.84%**，REFIT数据集平均**13.63%**
- **因果解释**：通过贝叶斯网络提供因果级别的解释（而非简单的特征重要性）
  - 解释一致性（余弦相似度）: Peak=0.99940, Normal=0.99983, Lower=0.99974
  - 显著优于SHAP (0.95-0.96), LIME (0.70-0.75), PD Variance (0.81-0.96)
- **可执行建议**：输出用户可直接采纳的行动建议
  - 基于敏感性分析，量化干预效果

### 1.3 应用价值
- 家庭/住户用电需求预测
- 峰值预警与负荷管理
- 节能建议与用电优化
- 可迁移至其他领域（医学诊断、风险评估等）

---

## 2. 系统架构设计

### 2.1 整体架构（双流水线设计）

```
输入数据（时间序列）
    |
    ├─────────────────┬─────────────────┐
    |                 |                 |
    v                 v                 v
[预测流水线]      [特征提取]      [时间解释]
 CNN分支           CAM            LSTM+Attention
    |                 |                 |
    └─────────────────┴─────────────────┘
                      |
                      v
            [数值预测 ŷ + 解释特征]
                      |
                      v
            ┌─────────┴─────────┐
            v                   v
    [状态判定(Sn)]         [离散化]
            |                   |
            v                   v
    [Peak/Normal/Lower]    [符号集合]
            |                   |
            └─────────┬─────────┘
                      v
            ┌─────────┴─────────┐
            v                   v
        [聚类分析]         [关联规则]
    (CAM/ATT类型化)      (Apriori)
            |                   |
            └─────────┬─────────┘
                      v
            [贝叶斯网络构建]
         (结构学习+领域约束)
                      |
                      v
            ┌─────────┴─────────┐
            v                   v
    [因果推断]           [反事实分析]
            |                   |
            └─────────┬─────────┘
                      v
            [行动建议生成]
                      |
                      v
                [最终输出]
```

---

## 3. 模块设计详述

### 3.1 数据预处理模块

#### 功能
将原始时间序列数据转换为模型可用的格式

#### 输入
```python
原始数据：
- 时间戳
- 总有功功率 (Global_active_power)
- 总无功功率 (Global_reactive_power)
- 电压 (Voltage)
- 电流强度 (Global_intensity)
- 分项电器用电（厨房、洗衣、空调、其他）
```

#### 处理步骤
1. **数据清洗**：处理缺失值、异常值
2. **时间特征提取**：
   - 日期 (Date)
   - 星期几 (Day)
   - 月份 (Month)
   - 季节 (Season)
   - 是否周末 (Weekend)
3. **滑动窗口构造**：
   - 窗口大小 ω = 80个时间点
   - 预测步长 l = 1
4. **数据标准化**：对连续变量进行归一化

#### 输出
```python
训练样本：
- X: [样本数, 窗口长度=80, 特征数] 
- y: [样本数, 1]  # 下一时刻的真实用电值
```

---

### 3.2 预测模型模块（核心深度学习部分）

#### 3.2.1 并行CNN分支

**目标**：提取短期局部时序模式和跨特征组合

**结构**：
```python
输入: [batch, 窗口长度, 特征数]
  ↓
1D卷积层1: Conv1D(filters=64, kernel_size=3, activation='relu')
  ↓
最大池化1: MaxPooling1D(pool_size=2)
  ↓
1D卷积层2: Conv1D(filters=128, kernel_size=3, activation='relu')
  ↓
最大池化2: MaxPooling1D(pool_size=2)
  ↓
展平层: Flatten()
  ↓
输出: CNN特征向量 + CAM（类激活图）
```

**关键点**：
- CAM通过全局平均池化层获取，用于后续解释
- 关注"哪些电器在哪段时间组合变化"

#### 3.2.2 LSTM+Attention分支

**目标**：建模长期依赖并识别关键时间点

**结构**：
```python
输入: [batch, 窗口长度, 特征数]
  ↓
LSTM层: LSTM(units=128, return_sequences=True)
  # 输出所有时间步的隐藏状态 h₁, h₂, ..., hₙ
  # 以及cell state s₁, ..., sₙ 和 output o₁, ..., oₙ
  ↓
注意力层（根据论文公式1）:
  1. 计算注意力分数: score_n = fc(o_n, h_N)
  2. Softmax归一化: a_n = exp(fc(o_n, h_N)) / Σ_{k=1}^N exp(fc(o_k, h_N))
  3. 加权求和（公式2）: c_N = Σ_{k=1}^N a_k · h_k
  ↓
输出: 上下文向量 c_N + 注意力权重向量 a
```

**数学公式（来自论文第4页）**：
$$a_n = \frac{\exp(f_c(o_n, h_N))}{\sum_{k=1}^{N} f_c(o_k, h_N)} \quad \text{for } t = 1, \ldots, T$$

$$c_N = \sum_{k=1}^{N} a_k \cdot h_k$$

**关键点**：
- 注意力权重a用于后续解释（Early/Late/Other）
- 识别"哪个时间段最关键"
- N = 窗口长度（默认80）

#### 3.2.3 融合与回归

**结构**（根据论文公式3）：
```python
# 融合公式
X_combined = concat(flattened(φ^l_Conv), c_N)

CNN特征 concat 上下文向量c_N
  ↓
全连接层1: Dense(256, activation='relu')
  ↓
Dropout(0.3)
  ↓
全连接层2: Dense(128, activation='relu')
  ↓
输出层: Dense(1, activation='linear')
  ↓
预测值 ŷ (连续数值)
```

**数学表示**：
$$X_{combined} = \text{concat}(\text{flattened}(\phi^l_{Conv}), c_N)$$

其中：
- $\phi^l_{Conv}$: CNN最后一层的输出（经过flatten）
- $c_N$: LSTM+Attention的上下文向量

**损失函数**：均方误差（MSE）

**优化器**：Adam（学习率建议范围：0.0001-0.001）

**训练细节（来自论文）**：
- 窗口大小 ω = 80 个时间点
- 预测步长 l = 1（t+1单步预测）
- 批大小: 通常32-64
- 早停策略：验证集损失15个epoch无改善

---

### 3.3 状态判定模块（Sn鲁棒估计）

#### 功能
将连续预测值映射为3类离散状态

#### 算法：Sn尺度估计

```python
def sn_scale_estimator(data):
    """
    Sn = c × median_i{ median_j(|x_i - x_j|) }
    其中 c 是校正因子
    
    比标准差更鲁棒，对异常值不敏感
    """
    n = len(data)
    pairwise_diffs = []
    for i in range(n):
        diffs = [abs(data[i] - data[j]) for j in range(n)]
        pairwise_diffs.append(np.median(diffs))
    
    sn = 1.1926 * np.median(pairwise_diffs)  # 1.1926是校正因子
    return sn

def classify_state(y_pred, window_data):
    """
    输入: 
      - y_pred: 预测值
      - window_data: 历史窗口数据
    
    输出:
      - state ∈ {Peak, Normal, Lower}
    """
    median_val = np.median(window_data)
    sn = sn_scale_estimator(window_data)
    
    # 计算鲁棒Z分数
    z_score = (y_pred - median_val) / sn
    
    # 阈值判定（可根据数据调整）
    if z_score > 2.0:
        return "Peak"
    elif z_score < -2.0:
        return "Lower"
    else:
        return "Normal"
```

#### 输出
```python
用电状态（EDP）∈ {Peak, Normal, Lower}
```

---

### 3.4 离散化与符号化模块

#### 3.4.1 分位数离散化

**功能**：将连续变量映射为有限等级

**方法**：
```python
def quantile_discretize(data, n_bins=4):
    """
    输入: 连续变量序列
    输出: 离散等级 {Low, Medium, High, VeryHigh}
    
    基于分位点切分：
    - [0%, 25%): Low
    - [25%, 50%): Medium  
    - [50%, 75%): High
    - [75%, 100%]: VeryHigh
    """
    quantiles = [0, 0.25, 0.50, 0.75, 1.0]
    bins = np.quantile(data, quantiles)
    labels = ['Low', 'Medium', 'High', 'VeryHigh']
    
    return pd.cut(data, bins=bins, labels=labels, include_lowest=True)
```

**应用变量**：
- 总有功功率
- 总无功功率
- 电压
- 电流
- 各分项电器用电

#### 3.4.2 解释参数聚类

**目标**：将CNN的CAM和Attention向量类型化

**CAM聚类**：
```python
# 对所有样本的CAM进行K-means聚类
# K值通过肘部法则确定（论文中使用K=2）

聚类结果示例（来自论文表6）：
- CAM_type1: 洗衣、空调、其他设备在观察时间内呈上升模式
- CAM_type2: 洗衣、厨房、空调设备在观察时间内呈下降模式
```

**Attention聚类（累积聚类法）**：
```python
# 预处理步骤（论文公式15）
# 1. De-minning: 归一化（减去最小值）
# 2. 除以总和进行归一化
# 3. 计算累积和: C_A = Σ_{i=1}^N a_i

聚类结果示例（来自论文表6和图7）：
- Late Attention: 观察时间内整体能耗呈上升模式（后段权重大）
- Early Attention: 观察时间内整体能耗呈下降模式（前段权重大）
- Other Attention: 观察时间内整体能耗稳定或波动模式

判断标准（根据累积Attention）：
- Early: 前50%时间累积到70%以上 → 早期权重集中
- Late: 前50%时间累积到30%以下 → 后期权重集中
- Other: 其他情况
```

**累积聚类公式（论文公式15）**：
$$C_A = \sum_{i=1}^{N} a_i$$

**解释意义映射表（论文表6）**：

| DLP | 含义/解释 |
|-----|----------|
| CAM 1 | 洗衣、空调、其他设备在观察窗口内增加 |
| CAM 2 | 洗衣、厨房、空调设备在观察窗口内减少 |
| Late Attention | 整体能耗在观察窗口内呈上升趋势 |
| Early Attention | 整体能耗在观察窗口内呈下降趋势 |
| Other Attention | 整体能耗在观察窗口内稳定或波动 |

---

### 3.5 关联规则挖掘模块（Apriori）

#### 功能
从离散数据中提取候选因果关系

#### 算法
```python
from mlxtend.frequent_patterns import apriori, association_rules

def mine_association_rules(discrete_data, min_support=0.1, min_confidence=0.6):
    """
    输入: 离散化后的数据框（包含EDP状态）
    输出: 关联规则集合
    
    示例规则：
    {ClimateControl=VeryHigh, GAP=VeryHigh} → Peak
    置信度: 0.85
    提升度: 2.3
    """
    # 1. 频繁项集挖掘
    frequent_itemsets = apriori(discrete_data, 
                                 min_support=min_support,
                                 use_colnames=True)
    
    # 2. 生成规则
    rules = association_rules(frequent_itemsets, 
                               metric="confidence",
                               min_threshold=min_confidence)
    
    # 3. 过滤：只保留后件为EDP的规则
    rules = rules[rules['consequents'].apply(
        lambda x: 'EDP' in str(x)
    )]
    
    return rules
```

#### 输出
```python
规则列表：
[
  {
    antecedents: ['ClimateControl_VeryHigh', 'Season_Summer'],
    consequent: 'Peak',
    confidence: 0.87,
    lift: 2.5
  },
  ...
]
```

---

### 3.6 贝叶斯网络模块

#### 3.6.1 结构学习（带领域约束）

**节点分组（主题约束）**：

```python
节点主题分组：
1. 物理环境 (Physical Environment):
   - Date, Day, Month, Season, Weekend
   
2. 电器使用 (Appliance Usage):
   - Kitchen, Laundry, ClimateControl, Other
   
3. 电力消耗 (Electricity Consumption):
   - GlobalActivePower, GlobalReactivePower, Voltage, Intensity
   
4. 深度学习参数 (Deep Learning Parameters):
   - CAM_type, ATT_type
   
5. 能耗模式 (Energy Demand Pattern):
   - EDP (Peak/Normal/Lower)
```

**方向约束（领域知识）**：

```python
允许的因果方向：
  物理环境 → 电器使用 → 电力消耗 → 能耗模式
                    ↘
                     深度学习参数 → 能耗模式

禁止的方向（反常识）：
  ❌ 能耗模式 → 任何上游节点
  ❌ 电器使用 → 物理环境
  ❌ 电力消耗 → 电器使用（除非有物理依赖）
```

**结构学习算法**：

```python
from pgmpy.estimators import HillClimbSearch, BicScore

def learn_bn_structure(data, domain_constraints):
    """
    输入: 离散数据 + 领域约束
    输出: 贝叶斯网络结构（DAG）
    
    方法: Hill-Climbing + BIC评分 + 黑白名单约束
    """
    # 1. 根据主题约束构建黑白名单
    white_list = []  # 允许的边
    black_list = []  # 禁止的边
    
    # 物理环境只能指向电器使用
    for env_node in physical_env_nodes:
        for app_node in appliance_nodes:
            white_list.append((env_node, app_node))
        for edp_node in ['EDP']:
            black_list.append((edp_node, env_node))
    
    # ... 其他约束
    
    # 2. 结构搜索
    hc = HillClimbSearch(data)
    best_model = hc.estimate(
        scoring_method=BicScore(data),
        white_list=white_list,
        black_list=black_list
    )
    
    return best_model
```

#### 3.6.2 参数学习

**方法**：最大似然估计（频数统计）

```python
from pgmpy.estimators import MaximumLikelihoodEstimator

def learn_bn_parameters(structure, data):
    """
    输入: BN结构 + 数据
    输出: 条件概率表（CPT）
    
    示例CPT:
    P(EDP=Peak | ClimateControl=VeryHigh, Season=Summer) = 0.82
    """
    model = BayesianNetwork(structure.edges())
    model.fit(data, estimator=MaximumLikelihoodEstimator)
    
    return model
```

#### 3.6.3 三张条件化网络

**关键设计**：不是一张网络，而是三张

```python
# 根据EDP分别学习
data_peak = data[data['EDP'] == 'Peak']
data_lower = data[data['EDP'] == 'Lower']
data_normal = data[data['EDP'] == 'Normal']

bn_peak = learn_bn(data_peak, constraints)
bn_lower = learn_bn(data_lower, constraints)
bn_normal = learn_bn(data_normal, constraints)
```

**原因**：不同状态下的因果机制可能完全不同

---

### 3.7 因果推断与建议生成模块

#### 3.7.1 因果推断

```python
def causal_inference(bn_model, evidence):
    """
    输入: BN模型 + 当前观测证据
    输出: EDP的后验概率分布 + 各因素贡献
    
    示例:
    evidence = {
        'Season': 'Summer',
        'ClimateControl': 'VeryHigh',
        'ATT_type': 'Late'
    }
    
    输出:
    {
        'P(Peak)': 0.91,
        'P(Normal)': 0.07,
        'P(Lower)': 0.02,
        'factors': [
            ('ClimateControl', 0.43),  # 最关键
            ('ATT_type', 0.27),
            ('Season', 0.18)
        ]
    }
    """
    from pgmpy.inference import VariableElimination
    
    infer = VariableElimination(bn_model)
    result = infer.query(variables=['EDP'], evidence=evidence)
    
    # 敏感性分析（逐个移除因素看影响）
    sensitivity = compute_sensitivity(bn_model, evidence)
    
    return result, sensitivity
```

#### 3.7.2 反事实推断

```python
def counterfactual_analysis(bn_model, evidence, intervention):
    """
    输入: 
      - bn_model: 贝叶斯网络
      - evidence: 当前观测
      - intervention: 干预措施
    
    输出: 干预后的预期效果
    
    示例:
    evidence = {'ClimateControl': 'VeryHigh', ...}
    intervention = {'ClimateControl': 'Medium'}
    
    输出:
    {
        'original_P(Peak)': 0.91,
        'intervened_P(Peak)': 0.59,
        'reduction': 0.32  # 下降32%
    }
    """
    # 1. 原始推断
    original_prob = infer.query(
        variables=['EDP'], 
        evidence=evidence
    )
    
    # 2. 干预推断（do-运算思想）
    # 修改证据并重新推断
    evidence_intervened = evidence.copy()
    evidence_intervened.update(intervention)
    
    intervened_prob = infer.query(
        variables=['EDP'],
        evidence=evidence_intervened
    )
    
    return {
        'original': original_prob,
        'intervened': intervened_prob,
        'effect': original_prob - intervened_prob
    }
```

#### 3.7.3 行动建议生成

```python
def generate_recommendations(causal_result, counterfactual_results):
    """
    输入: 因果推断结果 + 多个反事实分析结果
    输出: 可执行的行动建议（自然语言）
    
    策略:
    1. 识别主要原因（贡献度最高的因素）
    2. 对每个可控因素进行反事实分析
    3. 选择效果最好的干预措施
    4. 翻译成用户可读的建议
    """
    recommendations = []
    
    # 示例：Peak状态
    if current_state == 'Peak':
        # 找到最关键且可控的因素
        top_factor = causal_result['factors'][0]  # 例如 ClimateControl
        
        # 计算干预效果
        effect = counterfactual_results[top_factor]['reduction']
        
        # 生成建议
        recommendation = f"""
        ⚠️ 用电峰值预警（概率 {causal_result['P(Peak)']:.0%}）
        
        主要原因：
        - {top_factor} 处于 VeryHigh 水平
        - 最近时间段用电快速上升
        
        建议措施：
        - 将 {top_factor} 从 VeryHigh 调整至 Medium
          → 峰值风险预计下降约 {effect:.0%}
        - 短时间内避免同时开启其他大功率设备
        """
        
        recommendations.append(recommendation)
    
    return recommendations
```

---

## 4. 数据流设计

### 4.1 训练阶段数据流

```
原始数据（CSV）
  ↓
[数据预处理]
  - 清洗、时间特征提取
  - 滑动窗口构造（ω=80, l=1）
  - 数据集说明：
    * UCI: 2,075,259条记录，1分钟分辨率，8个属性
    * REFIT: 5,733,526条记录，8秒分辨率，11个属性
  ↓
训练集 / 验证集 / 测试集
  ↓
[深度学习模型训练]
  - CNN+LSTM+Attention
  - 输出: 模型权重 + CAM + Attention
  - 训练参数：
    * 窗口大小 ω = 80
    * 预测步长 l = 1
    * 时间分辨率：15min/30min/1h/1d（论文测试了4种）
  ↓
[预测 + 状态判定]
  - 对训练集/测试集预测
  - Sn状态分类（Peak/Normal/Lower）
  ↓
[离散化 + 聚类]
  - 分位数离散化（4个等级）
  - CAM聚类（K=2）
  - Attention累积聚类（K=3）
  ↓
[关联规则挖掘]
  - Apriori算法
  - 最小支持度/置信度设置
  ↓
[贝叶斯网络学习]
  - 结构学习（3张网络）
    * 使用Genie Academic v4.1
    * 结构学习时间：约433.61秒
  - 参数学习（最大似然估计）
  - 领域知识约束（DK Restriction）
  ↓
保存所有模型和配置
```

### 4.2 推理阶段数据流

```
新的输入窗口（80个时间点）
  ↓
[预处理]
  - 标准化
  - 时间特征提取
  ↓
[深度模型预测]
  - 输出 ŷ + CAM + Attention
  ↓
[状态判定]
  - Sn → EDP
  ↓
[离散化]
  - 连续值 → 符号
  ↓
[聚类映射]
  - CAM → CAM_type
  - Attention → ATT_type
  ↓
[选择对应的BN]
  - 根据EDP选择 bn_peak/lower/normal
  ↓
[因果推断]
  - 计算概率 + 因素贡献
  ↓
[反事实分析]
  - 测试多种干预方案
  ↓
[生成建议]
  - 输出可执行建议
  ↓
最终输出（JSON + 自然语言）
```

---

## 5. 技术栈选型

### 5.1 编程语言与框架

```python
语言: Python 3.8+

深度学习框架:
- TensorFlow 2.x / Keras  # 预测模型
- PyTorch (可选)           # 如果需要更灵活的控制

贝叶斯网络:
- pgmpy                    # 贝叶斯网络建模与推断

数据处理:
- pandas                   # 数据处理
- numpy                    # 数值计算
- scikit-learn             # 离散化、评估指标

关联规则:
- mlxtend                  # Apriori实现

聚类:
- scikit-learn.cluster     # K-means

可视化:
- matplotlib
- seaborn
- networkx                 # BN可视化
```

### 5.2 数据存储

```
原始数据: CSV
模型权重: .h5 / .pt
BN结构: pickle / JSON
配置文件: YAML / JSON
```

---

## 6. 评估指标设计

### 6.1 预测性能指标

```python
回归指标（预测准确性）:
- MSE (Mean Squared Error): MSE = (1/n)Σ(y_i - ŷ_i)²
- RMSE (Root Mean Squared Error): RMSE = √[(1/n)Σ(y_i - ŷ_i)²]
- MAE (Mean Absolute Error): MAE = Σ|y_i - ŷ_i| / n

分类指标（状态判定）:
- 准确率 (Accuracy)
- 精确率 / 召回率 / F1（针对Peak）
- 混淆矩阵

论文基准性能（表3，15分钟分辨率，UCI数据集）：
方法                  MSE      RMSE     MAE
ARIMA                0.1427   0.3778   0.2445
XGBoost              0.0836   0.2891   0.1752
Standalone CNN       0.0794   0.2818   0.1711
Standalone LSTM      0.0776   0.2786   0.1690
本文方法(并行)       最优     最优     最优

性能提升（表4）：
- 相比串联CNN-LSTM-Attention:
  * UCI数据集: 平均34.84%改进
  * REFIT数据集: 平均13.63%改进
```

### 6.2 解释一致性指标

```python
def explanation_consistency(train_bn, test_bn):
    """
    衡量训练集和测试集学到的BN结构相似度
    
    方法: 结构相似度（余弦相似度）
    
    期望: > 0.95
    
    论文结果（表5）：
    EDP状态              PD Variance  LIME    SHAP    本文方法
    Lower than usual    0.80629      0.69221 0.95310 0.99974
    No significant      0.92643      0.75056 0.96091 0.99983
    Peak warning        0.96062      0.70791 0.95840 0.99940
    """
    train_edges = set(train_bn.edges())
    test_edges = set(test_bn.edges())
    
    intersection = len(train_edges & test_edges)
    union = len(train_edges | test_edges)
    
    similarity = intersection / union
    return similarity
```

### 6.3 计算效率指标

```python
推理时间（论文表8和表9）:
- 深度模型预测时间: 
  * 串联CNN-LSTM: 186.065ms ± 2.773ms
  * 串联CNN-LSTM-Att: 236.639ms ± 14.878ms
  * 并行CNN-LSTM-Att (本文): 260.857ms ± 9.831ms
  
- BN推断时间: 平均1.30秒/样本
  
- 总推理时间: < 1.5秒/样本（对于1小时提前预测，完全可接受）

训练/学习时间:
- 深度模型训练: 可离线进行
- BN结构学习: 一次性，约433.61秒（仅在初始化时）
- BN参数学习: 最大似然估计，快速

计算开销对比（表7，100样本）：
方法              Peak状态    Normal状态   Lower状态
本文方法(BN)      35.50s      77.97s       79.78s
LIME              4033.92s    3783.67s     3868.97s
SHAP              27310.59s   27456.40s    28035.31s
                  (~7.6小时)  (~7.6小时)   (~7.8小时)

关键优势：
- BN结构学习一次性完成
- 推理时间常数，不随样本数增长
- 适合实时/在线应用
```

---

## 7. 可扩展性设计

### 7.1 领域迁移设计

系统设计为**高度可迁移**，以支持其他领域应用：

```python
# 配置文件驱动的领域适配
domain_config = {
    "domain": "energy",  # 或 "medical", "finance" 等
    
    "variables": {
        "physical_env": ["Season", "Day", ...],
        "appliances": ["Kitchen", "ClimateControl", ...],
        "consumption": ["GlobalActivePower", ...],
        "dlp": ["CAM_type", "ATT_type"],
        "target": "EDP"
    },
    
    "causal_constraints": {
        "allowed_directions": [
            ("physical_env", "appliances"),
            ("appliances", "consumption"),
            ...
        ],
        "forbidden_directions": [
            ("target", "*"),
            ...
        ]
    },
    
    "discretization": {
        "n_bins": 4,
        "labels": ["Low", "Medium", "High", "VeryHigh"]
    },
    
    "recommendation_templates": {
        "Peak": "建议降低 {factor} 从 {from_level} 至 {to_level}",
        ...
    }
}
```

### 7.2 模型插件化

```python
# 预测模型可替换
predictor_registry = {
    "cnn_lstm_att": ParallelCNNLSTMAttention,
    "transformer": TransformerPredictor,
    "tcn": TemporalConvNet,
    ...
}

# 贝叶斯网络学习算法可替换
bn_learner_registry = {
    "hill_climb": HillClimbLearner,
    "pc_algorithm": PCLearner,
    "ges": GESLearner,
    ...
}
```

---

## 8. 关键挑战与解决方案

### 8.1 训练数据不平衡

**问题**: Peak状态样本可能远少于Normal

**解决方案**:
- 对Peak样本进行过采样（SMOTE for time series）
- 使用类别权重（class_weight in loss function）
- 分层采样确保验证集分布一致

### 8.2 BN结构学习的计算复杂度

**问题**: 结构搜索空间指数级增长

**解决方案**:
- 强领域约束大幅缩减搜索空间
- 使用关联规则预筛选候选边
- 采用启发式搜索（Hill-Climbing而非穷举）

### 8.3 解释的可读性

**问题**: BN输出的是概率，用户难以理解

**解决方案**:
- 模板化建议生成
- 可视化因果路径
- 量化干预效果（"下降32%"而非"概率0.32"）

---

## 9. 项目里程碑

### 阶段1: 基础搭建（1-2周）
- 数据预处理管道
- 深度模型框架搭建
- 基础训练流程

### 阶段2: 预测模型优化（2-3周）
- CNN+LSTM+Attention完整实现
- 超参数调优
- 性能达到论文水平

### 阶段3: 因果解释模块（2-3周）
- 离散化与聚类
- 关联规则挖掘
- 贝叶斯网络学习

### 阶段4: 推断与建议（1-2周）
- 因果推断实现
- 反事实分析
- 建议生成

### 阶段5: 集成测试（1周）
- 端到端测试
- 性能优化
- 文档完善

---

## 10. 预期输出

### 10.1 模型输出

```json
{
  "prediction": {
    "timestamp": "2024-01-16 20:01:00",
    "predicted_power": 4.25,
    "state": "Peak",
    "probability": {
      "Peak": 0.91,
      "Normal": 0.07,
      "Lower": 0.02
    }
  },
  
  "explanation": {
    "main_factors": [
      {
        "factor": "ClimateControl",
        "level": "VeryHigh",
        "contribution": 0.43
      },
      {
        "factor": "ATT_type",
        "value": "Late",
        "contribution": 0.27
      }
    ],
    
    "causal_path": [
      "Season:Summer → ClimateControl:VeryHigh",
      "ClimateControl:VeryHigh → GlobalActivePower:VeryHigh",
      "GlobalActivePower:VeryHigh → Peak"
    ]
  },
  
  "counterfactual": {
    "intervention": {
      "ClimateControl": "Medium"
    },
    "expected_effect": {
      "Peak_probability": 0.59,
      "reduction": 0.32
    }
  },
  
  "recommendation": {
    "priority": "high",
    "action": "将空调档位从 VeryHigh 调整至 Medium",
    "expected_benefit": "峰值风险预计下降约 32%",
    "additional_tips": [
      "短时间内避免同时开启其他大功率设备"
    ]
  }
}
```

### 10.2 可视化输出

1. **预测曲线图**
   - 真实值 vs 预测值
   - 状态标注（Peak/Normal/Lower）
   - 峰值检测高亮（论文图5a展示了模型有效检测尖峰）

2. **注意力热图**（论文图7）
   - 时间轴上的注意力分布
   - 三种模式可视化：Early/Late/Other

3. **CAM激活图**（论文图6）
   - 哪些特征在哪些时间被激活
   - 两种类型：上升模式（Type 1）vs 下降模式（Type 2）
   - 使用余弦相似度过滤一致模式

4. **因果网络图**（论文图3和图8）
   - 三张BN的DAG可视化（Peak/Normal/Lower各一张）
   - 节点着色（按主题分组）：
     * 绿色：物理环境（Physical Environment）
     * 黄色：电器使用（Appliance Usage）
     * 蓝色：电力消耗（Electricity Consumption）
     * 红色：能耗模式（Energy Demand Pattern）
     * 灰色：深度学习参数（DLPs）
   - 边粗细：按条件概率强度
   - 激活状态高亮显示

5. **敏感性分析图**（论文图9）
   - Tornado图显示各因素影响
   - 三个主要变量的敏感性
   - 绿色条：调整变量后Peak概率
   - 示例结果：
     * Peak状态：空调Very High影响最大
     * Lower状态：其他设备Very High为反向因素

6. **计算开销对比图**（论文图10）
   - SHAP/LIME/本文方法的时间复杂度曲线
   - SHAP呈指数增长
   - BN保持常数级

7. **消融实验图**（论文图11-14）
   - 不同模型配置性能对比
   - DLP集成效果
   - 离散状态数量影响
   - 噪声鲁棒性测试

---

## 11. 论文实验结果总结（用于验证复现）

### 11.1 可执行建议示例（论文第9页）

基于贝叶斯网络推断和敏感性分析，论文为三种状态提供了具体建议：

#### Peak Warning（峰值预警）
**观察结果**：
- Early attention概率：53%（整体下降后突然跳升）
- Global active power: Very High (48%)
- Climate control: Very High (50%)
- Weekend贡献：仅33%

**敏感性分析**：
- Climate control (Very High → Low): Peak概率 66% → 34%（下降32%）

**可执行建议**：
> "鉴于峰值预警状态对空调设备使用的高度敏感性，建议住户谨慎避免将这些设备设置为Very High级别，因为这可能导致过度功耗。"

#### Lower than Usual（低于常态）
**观察结果**：
- CAM type 2概率：85%（厨房、洗衣、空调减少）
- Global intensity: Low (42%)
- Global active power: 减少 (39%)

**反向因素**：
- Weekday（相对Weekend）
- Other appliances: Very High

**可执行建议**：
> "住户应避免将'其他电器'使用到Very High级别，而应保持在Medium强度。此外，减少厨房、洗衣和空调设备的使用可实现显著节能。"

#### No Significant Change（无显著变化）
**观察结果**：
- Global active power和Intensity保持稳定分布
- Low/Medium/High状态均匀分布
- Other attention类型（稳定/波动）
- CAM 2贡献（设备使用下降模式）

**可执行建议**：
> "使用家用电器时避免能耗突然跳变将保持稳定性。类似于Lower状态场景，避免增加厨房、洗衣和空调设备的使用也有助于维持这种稳定性。"

### 11.2 消融实验结论（论文图11-14，表10）

**模型组件贡献**：
- 完整模型 (p-c-l-a): 最佳性能
- 移除attention (p-c-l): 性能略微下降（边际贡献）
- 串联配置 (s-c-l-a): 性能明显下降
- **结论**：并行配置是主要增益来源

**DLP集成效果**（图12）：
- 加入DLP: 解释一致性显著提升
- 不加DLP: 一致性下降
- **结论**：DLP让解释更稳定

**离散状态数量影响**（图13）：
- 增加离散状态数：BN准确率略升
- 但可解释性下降（过于细粒度）
- **结论**：4个状态是准确性与可读性的平衡点

**噪声鲁棒性**（图14）：
- 串联模型更抗噪声
- 添加attention可改善抗噪性
- **结论**：attention有助于缓解噪声影响

### 11.3 关键数值基准

用于验证复现是否成功：

| 指标 | 目标值 | 来源 |
|------|--------|------|
| 预测改进（UCI） | 34.84% | 表4 |
| 预测改进（REFIT） | 13.63% | 表4 |
| Peak一致性 | > 0.999 | 表5 |
| Normal一致性 | > 0.999 | 表5 |
| Lower一致性 | > 0.999 | 表5 |
| BN推理时间 | ~1.3秒 | 表8 |
| BN结构学习时间 | ~434秒 | 表8 |
| 深度模型推理时间 | ~261ms | 表9 |

---

## 11. 文档输出清单

1. **项目设计文档**（本文档）
2. **实现文档**（详细代码说明）
3. **API文档**（函数接口说明）
4. **用户手册**（如何使用系统）
5. **实验报告**（性能评估结果）

---

## 12. 成功标准

### 12.1 预测性能
- **基准对比**：在15分钟分辨率下，RMSE/MAE应优于：
  - ARIMA: RMSE=0.3778, MAE=0.2445
  - XGBoost: RMSE=0.2891, MAE=0.1752
  - Standalone CNN: RMSE=0.2818
  - Standalone LSTM: RMSE=0.2786
  
- **相对改进**：相比串联CNN-LSTM-Attention：
  - UCI数据集：改进 ≥ 30%（论文值：34.84%）
  - REFIT数据集：改进 ≥ 10%（论文值：13.63%）
  
- **多分辨率稳定性**：在15min/30min/1h/1d四种分辨率下均保持优势

### 12.2 解释质量
- **一致性（余弦相似度）**：
  - Peak状态: > 0.995（论文值：0.99940）
  - Normal状态: > 0.995（论文值：0.99983）
  - Lower状态: > 0.995（论文值：0.99974）
  
- **优于基准方法**：
  - SHAP一致性：0.95-0.96
  - LIME一致性：0.70-0.75
  - PD Variance一致性：0.81-0.96
  - **本文方法应显著优于以上所有方法**

- **生成建议的可验证性**：
  - 敏感性分析结果与领域知识一致
  - 可被能源管理专家验证和接受

### 12.3 计算效率
- **推理性能**：
  - 深度模型：< 300ms/样本（论文值：260.857ms）
  - BN推断：< 2秒/样本（论文值：1.30s）
  - 总时间：< 2秒/样本（适用于小时级预测）
  
- **学习时间**：
  - BN结构学习：< 500秒（论文值：433.61s）
  - 可接受的一次性开销

- **可扩展性**：
  - 相比SHAP/LIME：
    * 100样本推理：< 100秒 vs SHAP 7小时+
    * 不随样本数指数增长

### 12.4 可用性
- **代码质量**：
  - 模块化设计，易于理解和修改
  - 充分的文档和注释
  - 通过单元测试验证

- **配置灵活性**：
  - YAML配置文件驱动
  - 支持不同数据集和参数快速切换
  - 领域知识约束可自定义

- **可复现性**：
  - 随机种子固定
  - 数据处理流程可追踪
  - 实验结果可重复

### 12.5 消融实验验证
- **并行 vs 串联**：并行配置性能明显优于串联
- **Attention贡献**：移除attention导致性能下降
- **DLP集成**：加入CAM/Attention显著提升解释一致性
- **噪声鲁棒性**：attention有助于抵抗输入噪声

---

**设计文档版本**: v2.0 (基于PDF原文更新)
**最后更新**: 2026-01-16  
**更新内容**: 
- 补充论文中的数学公式
- 添加实验结果基准数值
- 补充DLP解释映射表
- 添加可执行建议示例
- 更新计算开销对比数据
- 补充消融实验结论
