"""
å¿«é€ŸéªŒè¯è„šæœ¬ - ä½¿ç”¨å°æ•°æ®é›†éªŒè¯å®Œæ•´æµç¨‹
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import json
import numpy as np
import pandas as pd
from datetime import datetime
import logging
from sklearn.metrics import mean_absolute_error, mean_squared_error

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

print("=" * 80)
print("å¿«é€ŸéªŒè¯ï¼šä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•å®Œæ•´æµç¨‹")
print("=" * 80)

# 1. åŠ è½½é…ç½®
print("\n[æ­¥éª¤1] åŠ è½½é…ç½®")
with open('configs/paper_config.json', 'r') as f:
    config = json.load(f)

print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
print(f"   åºåˆ—é•¿åº¦: {config['sequence_length']}")
print(f"   LSTMå•å…ƒ: {config['lstm_units']}")

# 2. åŠ è½½æ•°æ®ï¼ˆåªç”¨å°æ ·æœ¬ï¼‰
print("\n[æ­¥éª¤2] åŠ è½½æ•°æ®ï¼ˆå°æ ·æœ¬ï¼‰")

try:
    train_df = pd.read_csv('data/uci/splits/train.csv')
    test_df = pd.read_csv('data/uci/splits/test.csv')
    
    # åªç”¨å‰1000æ¡è®­ç»ƒï¼Œå‰200æ¡æµ‹è¯•
    train_df = train_df.head(1000)
    test_df = test_df.head(200)
    
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(train_df)}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(test_df)}")
    
except Exception as e:
    logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
    sys.exit(1)

# 3. æ•°æ®é¢„å¤„ç†
print("\n[æ­¥éª¤3] æ•°æ®é¢„å¤„ç†")

from src.preprocessing.data_preprocessor import EnergyDataPreprocessor

preprocessor = EnergyDataPreprocessor(
    sequence_length=config['sequence_length'],
    feature_cols=config['feature_cols'],
    target_col='Global_active_power'
)

X_train, y_train = preprocessor.fit_transform(train_df)
X_test, y_test = preprocessor.transform(test_df)

print(f"âœ… é¢„å¤„ç†å®Œæˆ")
print(f"   X_train: {X_train.shape}")
print(f"   y_train: {y_train.shape}")
print(f"   X_test: {X_test.shape}")
print(f"   y_test: {y_test.shape}")

# 4. æµ‹è¯•æ¨¡å‹è®­ç»ƒï¼ˆ3ä¸ªepochï¼‰
print("\n[æ­¥éª¤4] å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆ3ä¸ªepochï¼‰")

from src.models.predictor import ParallelCNNLSTMAttention
from src.models.baseline_models import SerialCNNLSTM

input_shape = (X_train.shape[1], X_train.shape[2])

# 4.1 å¹¶è¡Œæ¨¡å‹
print("\n[4.1] è®­ç»ƒå¹¶è¡ŒCNN-LSTM-Attention")
model_parallel = ParallelCNNLSTMAttention(
    input_shape=input_shape,
    cnn_filters=config['cnn_filters'][0],
    lstm_units=config['lstm_units'],
    attention_units=config['attention_units'],
    dense_units=config['dense_units']
)

model_parallel.compile(optimizer='adam', loss='mse', metrics=['mae'])

history_parallel = model_parallel.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=3,
    batch_size=32,
    verbose=1
)

print(f"âœ… å¹¶è¡Œæ¨¡å‹è®­ç»ƒå®Œæˆ")

# 4.2 ä¸²è”åŸºçº¿
print("\n[4.2] è®­ç»ƒä¸²è”CNN-LSTMï¼ˆåŸºçº¿ï¼‰")
model_serial = SerialCNNLSTM(
    input_shape=input_shape,
    cnn_filters=config['cnn_filters'][0],
    lstm_units=config['lstm_units'],
    dense_units=config['dense_units']
)

model_serial.compile(optimizer='adam', loss='mse', metrics=['mae'])

history_serial = model_serial.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=3,
    batch_size=32,
    verbose=1
)

print(f"âœ… ä¸²è”åŸºçº¿è®­ç»ƒå®Œæˆ")

# 5. è¯„ä¼°å¯¹æ¯”
print("\n[æ­¥éª¤5] è¯„ä¼°å¯¹æ¯”")

y_pred_parallel = model_parallel.predict(X_test).flatten()
y_pred_serial = model_serial.predict(X_test).flatten()

def compute_metrics(y_true, y_pred, name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mse = mean_squared_error(y_true, y_pred)
    
    # MAPE
    mask = y_true > 0.01
    if np.sum(mask) > 0:
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
    else:
        mape = 0
    
    print(f"\n{name}:")
    print(f"   MAE:  {mae:.4f} kW")
    print(f"   RMSE: {rmse:.4f} kW")
    print(f"   MAPE: {mape:.2f}%")
    
    return {'mae': mae, 'rmse': rmse, 'mse': mse, 'mape': mape}

metrics_parallel = compute_metrics(y_test, y_pred_parallel, "å¹¶è¡ŒCNN-LSTM-Attention")
metrics_serial = compute_metrics(y_test, y_pred_serial, "ä¸²è”CNN-LSTMï¼ˆåŸºçº¿ï¼‰")

# è®¡ç®—æå‡
improvement_mae = (metrics_serial['mae'] - metrics_parallel['mae']) / metrics_serial['mae'] * 100
improvement_rmse = (metrics_serial['rmse'] - metrics_parallel['rmse']) / metrics_serial['rmse'] * 100

print(f"\næ€§èƒ½æå‡ï¼ˆç›¸å¯¹åŸºçº¿ï¼‰:")
print(f"   MAEæå‡:  {improvement_mae:+.2f}%")
print(f"   RMSEæå‡: {improvement_rmse:+.2f}%")

# 6. æµ‹è¯•çŠ¶æ€åˆ†ç±»
print("\n[æ­¥éª¤6] æµ‹è¯•çŠ¶æ€åˆ†ç±»")

from src.models.state_classifier import SnStateClassifier

classifier = SnStateClassifier(threshold=2.0)
classifier.fit(y_train)

states_parallel = classifier.predict(y_pred_parallel[:10])
states_serial = classifier.predict(y_pred_serial[:10])

print(f"âœ… çŠ¶æ€åˆ†ç±»å®Œæˆ")
print(f"\nå‰10ä¸ªæ ·æœ¬çš„çŠ¶æ€é¢„æµ‹ï¼ˆå¹¶è¡Œæ¨¡å‹ï¼‰:")
for i in range(min(10, len(states_parallel))):
    print(f"   æ ·æœ¬{i}: é¢„æµ‹={y_pred_parallel[i]:.3f}kW, çœŸå€¼={y_test[i]:.3f}kW, çŠ¶æ€={states_parallel[i]}")

# 7. æµ‹è¯•DLPæå–
print("\n[æ­¥éª¤7] æµ‹è¯•æ·±åº¦å­¦ä¹ å‚æ•°æå–")

cam = model_parallel.extract_cam(X_test[:5])
attention = model_parallel.extract_attention_weights(X_test[:5])

print(f"âœ… DLPæå–æˆåŠŸ")
print(f"   CAMå½¢çŠ¶: {cam.shape}")
print(f"   Attentionå½¢çŠ¶: {attention.shape}")

# æ€»ç»“
print("\n" + "=" * 80)
print("å¿«é€ŸéªŒè¯æ€»ç»“")
print("=" * 80)

print(f"""
âœ… æ‰€æœ‰æµç¨‹éªŒè¯é€šè¿‡ï¼š
   1. é…ç½®åŠ è½½æ­£å¸¸
   2. æ•°æ®é¢„å¤„ç†æ­£å¸¸
   3. å¹¶è¡Œæ¨¡å‹è®­ç»ƒæˆåŠŸï¼ˆMAE: {metrics_parallel['mae']:.4f}ï¼‰
   4. ä¸²è”åŸºçº¿è®­ç»ƒæˆåŠŸï¼ˆMAE: {metrics_serial['mae']:.4f}ï¼‰
   5. æ€§èƒ½æå‡: MAE {improvement_mae:+.2f}%, RMSE {improvement_rmse:+.2f}%
   6. çŠ¶æ€åˆ†ç±»æ­£å¸¸
   7. DLPæå–æ­£å¸¸

ğŸ“ è¯´æ˜ï¼š
   - è¿™æ˜¯ä½¿ç”¨å°æ•°æ®é›†ï¼ˆ1000è®­ç»ƒ+200æµ‹è¯•ï¼‰çš„å¿«é€ŸéªŒè¯
   - ä»…è®­ç»ƒ3ä¸ªepochç”¨äºæµ‹è¯•æµç¨‹
   - MAPEå¯èƒ½è¾ƒé«˜æ˜¯å› ä¸ºæ ·æœ¬å°‘ä¸”è®­ç»ƒä¸å……åˆ†
   
ğŸš€ ä¸‹ä¸€æ­¥ï¼š
   å¦‚éœ€å®Œæ•´å®éªŒï¼Œè¿è¡Œ: 
   python scripts/run_ablation_study.py
   
   ï¼ˆæ³¨æ„ï¼šå®Œæ•´å®éªŒéœ€è¦æ›´é•¿æ—¶é—´ï¼‰
""")
