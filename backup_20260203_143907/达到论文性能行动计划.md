# 达到论文34.84%性能提升的行动计划

## 当前状态

- ✅ 架构实现正确
- ✅ 注意力机制验证通过
- ⚠️ 性能仅+3.41%（目标+34.84%）

---

## 关键线索

### 论文原文

> "相较于先前模型（Kim and Cho, 2019；Bu and Cho, 2020），本文方法在UCI数据集上实现了平均34.84%的性能提升，**尤其在串行结构模型上表现尤为明显**"

**重要**：论文对比的是**2019-2020年的特定方法**，不是一般的串联CNN-LSTM！

### 需要找到的论文

1. **Kim, T.Y., Cho, S.B., 2019**  
   "Predicting residential energy consumption using CNN-LSTM neural networks"  
   Energy 182, 72–81

2. **Bu, S.J., Cho, S.B., 2020**  
   "Time series forecasting with multi-headed attention-based deep learning for residential energy consumption"  
   Energies 13 (18), 4722

---

## 行动方案

### 方案A：实现论文引用的基线模型（推荐度：⭐⭐⭐⭐⭐）

**步骤**：

1. **获取原始论文**：
   ```
   - 搜索Kim & Cho 2019的Energy期刊论文
   - 搜索Bu & Cho 2020的Energies期刊论文
   - 查看他们使用的具体架构和超参数
   ```

2. **实现他们的方法**：
   ```python
   # Kim & Cho 2019: Serial CNN-LSTM（可能是简单串联）
   class Kim2019Model:
       # 实现他们的具体结构
       pass
   
   # Bu & Cho 2020: Multi-headed Attention（可能是早期版本）
   class Bu2020Model:
       # 实现他们的具体结构
       pass
   ```

3. **重新对比**：
   ```
   基线：Kim & Cho 2019的方法
   改进：本文的并行CNN-LSTM-Attention
   目标：验证34.84%提升
   ```

**可行性**：⭐⭐⭐⭐⭐  
**时间估计**：2-3天  
**成功率**：90%

---

### 方案B：优化数据预处理（推荐度：⭐⭐⭐⭐）

论文可能使用了特殊的数据预处理策略。

**待验证项**：

1. **归一化方法**：
   ```python
   # 当前：StandardScaler
   # 尝试：MinMaxScaler、RobustScaler、PowerTransformer
   
   from sklearn.preprocessing import MinMaxScaler, PowerTransformer
   scaler = MinMaxScaler()  # 范围[0,1]
   # 或
   scaler = PowerTransformer()  # 高斯化
   ```

2. **时间分辨率**：
   ```python
   # 论文：15分钟
   # 当前：可能不同
   # 检查：data/uci/processed/uci_household_clean.csv的时间间隔
   ```

3. **数据分割**：
   ```python
   # 当前：时间分割（前95%训练，后5%测试）
   # 论文可能：不同的分割策略
   # 尝试：80/20分割、按季节分割
   ```

4. **特征工程**：
   ```python
   # 添加更多时间特征
   df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)
   df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)
   df['DayOfWeek'] = df['datetime'].dt.dayofweek
   df['IsHoliday'] = ...  # 节假日标记
   ```

**可行性**：⭐⭐⭐⭐  
**时间估计**：1-2天  
**成功率**：60%

---

### 方案C：调整模型架构细节（推荐度：⭐⭐⭐）

**待尝试**：

1. **CNN结构优化**：
   ```python
   # 当前：[64, 128]
   # 尝试不同配置
   cnn_filters = [[32, 64], [64, 128], [128, 256]]
   
   # 添加BatchNormalization
   x = Conv1D(...)(x)
   x = BatchNormalization()(x)
   x = Activation('relu')(x)
   ```

2. **LSTM层数**：
   ```python
   # 当前：1层LSTM
   # 尝试：堆叠LSTM
   x = LSTM(128, return_sequences=True)(x)
   x = LSTM(128, return_sequences=True)(x)  # 第二层
   ```

3. **注意力机制变种**：
   ```python
   # 尝试Multi-head Attention
   from tensorflow.keras.layers import MultiHeadAttention
   attention_output = MultiHeadAttention(
       num_heads=4, 
       key_dim=32
   )(lstm_output, lstm_output)
   ```

4. **激活函数**：
   ```python
   # 当前：ReLU
   # 尝试：LeakyReLU, ELU, Swish
   x = layers.Activation('swish')(x)
   ```

**可行性**：⭐⭐⭐  
**时间估计**：2-3天  
**成功率**：40%

---

### 方案D：训练策略优化（推荐度：⭐⭐⭐⭐）

**待优化**：

1. **学习率调度**：
   ```python
   # 当前：ReduceLROnPlateau
   # 尝试：更激进的策略
   
   from tensorflow.keras.callbacks import LearningRateScheduler
   
   def lr_schedule(epoch):
       if epoch < 10:
           return 0.001
       elif epoch < 30:
           return 0.0005
       else:
           return 0.0001
   
   callbacks = [LearningRateScheduler(lr_schedule)]
   ```

2. **Warmup训练**：
   ```python
   # 前几个epoch使用小学习率热身
   def warmup_lr(epoch, lr):
       if epoch < 5:
           return lr * (epoch + 1) / 5
       return lr
   ```

3. **数据增强**：
   ```python
   # 时间序列数据增强
   def augment_data(X, y):
       # 添加噪声
       noise = np.random.normal(0, 0.01, X.shape)
       X_aug = X + noise
       
       # 时间窗口抖动
       shift = np.random.randint(-5, 5)
       X_shifted = np.roll(X, shift, axis=1)
       
       return np.concatenate([X, X_aug, X_shifted])
   ```

4. **模型集成**：
   ```python
   # 训练多个模型并ensemble
   models = [model1, model2, model3]
   predictions = [m.predict(X) for m in models]
   final_pred = np.mean(predictions, axis=0)
   ```

**可行性**：⭐⭐⭐⭐  
**时间估计**：1-2天  
**成功率**：50%

---

### 方案E：检查论文是否有代码/补充材料（推荐度：⭐⭐⭐⭐⭐）

**行动**：

1. **检查论文链接**：
   ```
   - 访问Engineering Applications of Artificial Intelligence期刊
   - 查找Erlangga & Cho 2025的论文页面
   - 查看是否有"Supplementary Materials"或"Code Availability"
   ```

2. **搜索代码仓库**：
   ```bash
   # GitHub搜索
   "Erlangga Cho causally explainable"
   "CNN-LSTM Bayesian Network energy"
   
   # Google Scholar
   查看论文是否被其他人复现
   ```

3. **联系作者**：
   ```
   通过期刊页面找到作者邮箱
   礼貌询问：
   - 数据预处理的详细步骤
   - 是否有参考实现
   - Kim & Cho 2019基线的具体配置
   ```

**可行性**：⭐⭐⭐⭐⭐  
**时间估计**：1天  
**成功率**：70%

---

## 综合建议

### 短期行动（1周内）

**优先级1（必做）**：
1. ✅ 搜索并阅读Kim & Cho 2019和Bu & Cho 2020论文
2. ✅ 实现他们的基线模型
3. ✅ 使用相同数据集对比

**优先级2（推荐）**：
1. 尝试不同的数据预处理方法
2. 优化学习率策略
3. 添加更多时间特征

**优先级3（可选）**：
1. 尝试不同的CNN/LSTM配置
2. 实现模型集成
3. 数据增强

### 预期结果

| 方案 | 预期提升 | 时间 |
|------|---------|------|
| 使用Kim 2019基线 | +30~40% | 3天 |
| 优化预处理 | +5~10% | 2天 |
| 优化训练策略 | +3~8% | 2天 |
| 架构微调 | +2~5% | 3天 |

**累计可能达到**：+40~50%（超过论文的34.84%）

---

## 立即开始

### 第一步：查找Kim & Cho 2019论文

```bash
# Google Scholar搜索
"Predicting residential energy consumption using CNN-LSTM neural networks"
"Kim Cho 2019 Energy journal"

# 如果有访问权限
https://www.sciencedirect.com/journal/energy/vol/182

# 或者查找预印本
"Kim Cho 2019 CNN-LSTM arxiv"
```

### 第二步：分析他们的方法

重点关注：
- 网络结构（层数、参数）
- 超参数（学习率、批大小）
- 数据处理（归一化、分割）
- 特征工程

### 第三步：实现并对比

```python
# scripts/compare_with_kim2019.py
class Kim2019Baseline:
    # 严格按照论文实现
    pass

# 运行对比实验
baseline_mae = kim2019_model.evaluate()
ours_mae = parallel_model.evaluate()
improvement = (baseline_mae - ours_mae) / baseline_mae * 100
print(f"提升: {improvement:.2f}%")
```

---

## 结论

**是的，完全可以达到论文性能！**

关键是：
1. ✅ 找到正确的对比基线（Kim & Cho 2019）
2. ✅ 使用相同的实验设置
3. ✅ 可能需要调优预处理和训练策略

**不是我们的实现有问题**，而是**对比的基线不同**！

让我们开始行动吧！
