INFO: ================================================================================
INFO:                     å®Œæ•´è®­ç»ƒæµæ°´çº¿                    
INFO: ================================================================================
INFO: 
[æ­¥éª¤ 1] åŠ è½½è®­ç»ƒæ•°æ®...
INFO: âœ… æ•°æ®åŠ è½½æˆåŠŸ: (2000, 7)
INFO: ç‰¹å¾åˆ—: ['Temperature', 'Humidity', 'WindSpeed', 'EDP', 'Hour', 'DayOfWeek', 'Month']
INFO: 
[æ­¥éª¤ 2] é…ç½®è®­ç»ƒå‚æ•°...
INFO: è®­ç»ƒé…ç½®: epochs=10, batch_size=32
INFO: 
[æ­¥éª¤ 3] åˆ›å»ºè®­ç»ƒæµæ°´çº¿...
INFO: TrainPipeline initialized with output_dir=./outputs/training_run_1
INFO: 
[æ­¥éª¤ 4] å¼€å§‹è®­ç»ƒ...

INFO: ============================================================
INFO: Starting Training Pipeline
INFO: ============================================================
INFO: 
[Step 1/9] Data Preprocessing...
INFO: å¼€å§‹æ•°æ®é¢„å¤„ç†...
INFO: å¼€å§‹æ•°æ®æ¸…æ´—...
/home/severin/Codelib/YS/src/preprocessing/data_preprocessor.py:53: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  df = df.fillna(method='ffill').fillna(method='bfill')
INFO: æ•°æ®æ¸…æ´—å®Œæˆï¼Œæ ·æœ¬æ•°: 2000
INFO: æå–æ—¶é—´ç‰¹å¾...
INFO: é¢„å¤„ç†å®Œæˆï¼Œåºåˆ—æ•°: 1980, ç‰¹å¾ç»´åº¦: (1980, 20, 3)
INFO:   Train: X=(1980, 20, 3), y=(1980,)
INFO: 
[Step 2/9] Training Parallel CNN-LSTM-Attention Model...
2026-01-16 16:16:39.391045: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2026-01-16 16:16:39.391346: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-16 16:16:39.432197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-16 16:16:40.496647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-16 16:16:40.496892: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.
2026-01-16 16:16:40.669767: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
INFO: æ¨¡å‹æ„å»ºå®Œæˆ
INFO: å‚æ•°é‡: 20,307
Epoch 1/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:55[0m 2s/step - loss: 20944.4336 - mae: 143.8397[1m 5/62[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - loss: 21842.1023 - mae: 146.9624[1m 8/62[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 22ms/step - loss: 21990.7295 - mae: 147.4733[1m 9/62[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 26ms/step - loss: 21993.9286 - mae: 147.4887[1m16/62[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - loss: 22012.3414 - mae: 147.5554[1m21/62[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - loss: 21986.3664 - mae: 147.4632[1m32/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 21732.0047 - mae: 146.5758[1m41/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 21356.9385 - mae: 145.2185[1m49/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 20871.9481 - mae: 143.3026[1m56/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 20321.6476 - mae: 140.8169[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - loss: 19792.3886 - mae: 138.1798[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 10ms/step - loss: 14349.9619 - mae: 110.5671
Epoch 2/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - loss: 2296.6206 - mae: 39.8058[1m10/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 6ms/step - loss: 2375.3245 - mae: 39.6462 [1m22/62[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 2100.1644 - mae: 37.1099[1m33/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1961.8257 - mae: 35.7729[1m46/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1858.1521 - mae: 34.6783[1m59/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 5ms/step - loss: 1787.4219 - mae: 33.9135[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1504.5826 - mae: 30.8537
Epoch 3/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 17ms/step - loss: 1568.0787 - mae: 32.9950[1m13/62[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1480.3186 - mae: 31.4176 [1m26/62[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1453.7771 - mae: 31.0554[1m38/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1442.7050 - mae: 30.8367[1m53/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 4ms/step - loss: 1425.9655 - mae: 30.5722[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1348.4634 - mae: 29.5248
Epoch 4/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 1473.8123 - mae: 31.7748[1m16/62[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 1227.2837 - mae: 28.2711 [1m29/62[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1209.2229 - mae: 28.0195[1m43/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1197.4210 - mae: 27.8135[1m59/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 4ms/step - loss: 1188.7386 - mae: 27.6404[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1155.9052 - mae: 27.0566
Epoch 5/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 18ms/step - loss: 1407.6327 - mae: 31.8910[1m14/62[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1272.0549 - mae: 28.7449 [1m28/62[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1251.7575 - mae: 28.3555[1m41/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1229.8883 - mae: 28.0712[1m56/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 4ms/step - loss: 1207.8069 - mae: 27.8277[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1129.4032 - mae: 26.9605
Epoch 6/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 978.7456 - mae: 25.1856[1m16/62[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 1190.7435 - mae: 28.1852[1m31/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1178.0829 - mae: 27.9945[1m45/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1172.0835 - mae: 27.8888[1m60/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 4ms/step - loss: 1167.1631 - mae: 27.7940[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1155.4572 - mae: 27.5054
Epoch 7/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - loss: 1296.9357 - mae: 28.1750[1m16/62[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 1198.4007 - mae: 27.5063 [1m31/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 1143.2740 - mae: 26.7526[1m45/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 1119.6828 - mae: 26.4629[1m60/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 3ms/step - loss: 1099.9683 - mae: 26.2521[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1043.5673 - mae: 25.7241
Epoch 8/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 17ms/step - loss: 1228.3149 - mae: 29.1630[1m16/62[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 1050.6431 - mae: 26.3056 [1m30/62[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1051.7933 - mae: 26.2116[1m45/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1054.3011 - mae: 26.1753[1m61/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 3ms/step - loss: 1055.6962 - mae: 26.1526[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1053.5709 - mae: 26.1050
Epoch 9/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - loss: 943.3438 - mae: 24.9604[1m16/62[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1148.1715 - mae: 27.5245[1m31/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1136.3569 - mae: 27.5563[1m45/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1133.5205 - mae: 27.5251[1m59/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 4ms/step - loss: 1121.9311 - mae: 27.3123[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1057.8661 - mae: 26.1680
Epoch 10/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - loss: 1754.4041 - mae: 32.0370[1m16/62[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 1284.1065 - mae: 28.3745 [1m30/62[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1229.8222 - mae: 27.8755[1m43/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1210.3467 - mae: 27.6914[1m56/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 4ms/step - loss: 1193.6091 - mae: 27.5319[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1142.6564 - mae: 27.1044
INFO:   Final train loss: 1142.6564
INFO: 
[Step 3/9] Extracting Deep Learning Parameters (CAM & Attention)...
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 35ms/step[1m48/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 1ms/step [1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 2ms/step
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m4s[0m 78ms/step[1m33/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 2ms/step [1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step
INFO:   CAM: (1980, 10)
INFO:   Attention: (1980, 20)
INFO: 
[Step 4/9] Clustering DLP Features...
INFO: CAM èšç±»æ‹Ÿåˆ...
INFO: CAM èšç±»å®Œæˆï¼Œèšç±»æ•°: 3
INFO: Attentionæƒé‡èšç±»æ‹Ÿåˆ...
INFO: Attentionèšç±»å®Œæˆï¼Œèšç±»: ['Early', 'Early', 'Other']
INFO:   CAM clusters: [621 925 434]
INFO:   Attention types: {'Early': 1312, 'Other': 668}
INFO: 
[Step 5/9] Sn State Classification...
INFO: è®¡ç®—Snå°ºåº¦ä¼°è®¡å™¨...
INFO: ä¸­ä½æ•°: 146.2809, Snå°ºåº¦: 14.4536
INFO: K-meansèšç±»...
INFO: èšç±»ä¸­å¿ƒ: [-1.03924579  1.5286441   0.11955863]
INFO: çŠ¶æ€æ˜ å°„: {np.int64(0): 'Lower', np.int64(2): 'Normal', np.int64(1): 'Peak'}
INFO:   State distribution: {'Normal': 885, 'Lower': 615, 'Peak': 480}
INFO: 
[Step 6/9] Feature Discretization...
INFO: æ‹Ÿåˆç¦»æ•£åŒ–å™¨ï¼Œç‰¹å¾æ•°: 3
/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.
  warnings.warn(
/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.
  warnings.warn(
/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.
  warnings.warn(
INFO: ç¦»æ•£åŒ–å™¨æ‹Ÿåˆå®Œæˆ
INFO:   Discretized features: ['Temperature', 'Humidity', 'WindSpeed', 'EDP_State', 'CAM_Cluster', 'Attention_Type']
INFO: 
[Step 7/9] Association Rule Mining...
ERROR: 
âŒ è®­ç»ƒå¤±è´¥: No module named 'mlxtend'
Traceback (most recent call last):
  File "/home/severin/Codelib/YS/scripts/run_training.py", line 93, in main
    results = pipeline.run(train_data)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/severin/Codelib/YS/src/pipeline/train_pipeline.py", line 168, in run
    candidate_edges = self._step7_mine_rules(discrete_data)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/severin/Codelib/YS/src/pipeline/train_pipeline.py", line 322, in _step7_mine_rules
    from src.models.association import AssociationRuleMiner
  File "/home/severin/Codelib/YS/src/models/association.py", line 9, in <module>
    from mlxtend.frequent_patterns import apriori, association_rules
ModuleNotFoundError: No module named 'mlxtend'
Command 'python' not found, did you mean:
  command 'python3' from deb python3
  command 'python' from deb python-is-python3
INFO: ================================================================================
INFO:                     å®Œæ•´è®­ç»ƒæµæ°´çº¿                    
INFO: ================================================================================
INFO: 
[æ­¥éª¤ 1] åŠ è½½è®­ç»ƒæ•°æ®...
INFO: âœ… æ•°æ®åŠ è½½æˆåŠŸ: (2000, 7)
INFO: ç‰¹å¾åˆ—: ['Temperature', 'Humidity', 'WindSpeed', 'EDP', 'Hour', 'DayOfWeek', 'Month']
INFO: 
[æ­¥éª¤ 2] é…ç½®è®­ç»ƒå‚æ•°...
INFO: è®­ç»ƒé…ç½®: epochs=10, batch_size=32
INFO: 
[æ­¥éª¤ 3] åˆ›å»ºè®­ç»ƒæµæ°´çº¿...
INFO: TrainPipeline initialized with output_dir=./outputs/training_run_1
INFO: 
[æ­¥éª¤ 4] å¼€å§‹è®­ç»ƒ...

INFO: ============================================================
INFO: Starting Training Pipeline
INFO: ============================================================
INFO: 
[Step 1/9] Data Preprocessing...
INFO: å¼€å§‹æ•°æ®é¢„å¤„ç†...
INFO: å¼€å§‹æ•°æ®æ¸…æ´—...
/home/severin/Codelib/YS/src/preprocessing/data_preprocessor.py:53: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  df = df.fillna(method='ffill').fillna(method='bfill')
INFO: æ•°æ®æ¸…æ´—å®Œæˆï¼Œæ ·æœ¬æ•°: 2000
INFO: æå–æ—¶é—´ç‰¹å¾...
INFO: é¢„å¤„ç†å®Œæˆï¼Œåºåˆ—æ•°: 1980, ç‰¹å¾ç»´åº¦: (1980, 20, 3)
INFO:   Train: X=(1980, 20, 3), y=(1980,)
INFO: 
[Step 2/9] Training Parallel CNN-LSTM-Attention Model...
2026-01-16 16:47:29.818313: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-16 16:47:30.468697: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI AVX_VNNI_INT8 AVX_NE_CONVERT FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2026-01-16 16:47:32.326649: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-16 16:47:32.955446: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
INFO: æ¨¡å‹æ„å»ºå®Œæˆ
INFO: å‚æ•°é‡: 20,307
Epoch 1/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:46[0m 2s/step - loss: 21721.5508 - mae: 146.6289[1m10/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 6ms/step - loss: 21769.9311 - mae: 146.7768 [1m20/62[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 6ms/step - loss: 21732.6362 - mae: 146.6250[1m30/62[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 6ms/step - loss: 21616.2749 - mae: 146.2349[1m40/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 21428.8333 - mae: 145.5850[1m51/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 5ms/step - loss: 21049.2504 - mae: 144.1613[1m61/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 5ms/step - loss: 20482.5018 - mae: 141.7266[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 6ms/step - loss: 16328.3867 - mae: 122.2651
Epoch 2/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 23ms/step - loss: 1762.5193 - mae: 29.3639[1m11/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 2014.3369 - mae: 35.4209 [1m21/62[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 2149.9525 - mae: 36.7707[1m31/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 2174.6112 - mae: 37.0825[1m41/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 2160.4612 - mae: 37.0156[1m51/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 5ms/step - loss: 2137.1681 - mae: 36.8443[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 2109.2418 - mae: 36.6241[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1957.2090 - mae: 35.4167
Epoch 3/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 20ms/step - loss: 2292.7275 - mae: 40.5435[1m11/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1981.8101 - mae: 35.9311 [1m22/62[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1930.9584 - mae: 35.3659[1m33/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1910.1800 - mae: 35.1605[1m44/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1891.8694 - mae: 34.9918[1m55/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 5ms/step - loss: 1869.6320 - mae: 34.7962[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1734.7111 - mae: 33.6752
Epoch 4/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 19ms/step - loss: 2331.5469 - mae: 42.0097[1m12/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1735.7238 - mae: 34.2915 [1m23/62[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1685.2031 - mae: 33.6164[1m34/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1658.1961 - mae: 33.2482[1m46/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1648.3196 - mae: 33.0972[1m56/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 5ms/step - loss: 1641.0144 - mae: 32.9732[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1619.9397 - mae: 32.4531
Epoch 5/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 19ms/step - loss: 1712.8159 - mae: 33.3926[1m12/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1490.0908 - mae: 31.1862 [1m23/62[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1516.6239 - mae: 31.2048[1m35/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1514.7473 - mae: 31.0935[1m47/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1518.1018 - mae: 31.0564[1m58/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 5ms/step - loss: 1510.3044 - mae: 30.9341[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1448.9198 - mae: 30.1848
Epoch 6/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 20ms/step - loss: 1357.3074 - mae: 32.9381[1m12/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1578.7088 - mae: 32.6962 [1m23/62[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1496.8935 - mae: 31.3818[1m34/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1476.8230 - mae: 31.0811[1m46/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1471.2569 - mae: 30.9929[1m57/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 5ms/step - loss: 1473.1381 - mae: 30.9944[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1456.4193 - mae: 30.6858
Epoch 7/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 19ms/step - loss: 1620.2761 - mae: 32.9956[1m13/62[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1352.9284 - mae: 29.7260 [1m26/62[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1399.8476 - mae: 30.2351[1m38/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1408.2799 - mae: 30.2934[1m50/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 4ms/step - loss: 1410.3188 - mae: 30.2654[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 1406.1420 - mae: 30.1714[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1388.5458 - mae: 29.8015
Epoch 8/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 22ms/step - loss: 1386.3335 - mae: 30.2585[1m12/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1497.9831 - mae: 31.6001 [1m24/62[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1443.2775 - mae: 30.8450[1m35/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1415.7111 - mae: 30.4546[1m46/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1398.7339 - mae: 30.2193[1m57/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 5ms/step - loss: 1387.0001 - mae: 30.0794[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1324.0317 - mae: 29.3686
Epoch 9/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - loss: 1626.9414 - mae: 33.0013[1m12/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1311.7063 - mae: 29.1454 [1m24/62[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1306.5541 - mae: 28.9546[1m35/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1316.5289 - mae: 29.0505[1m46/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1316.6186 - mae: 29.0132[1m57/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 5ms/step - loss: 1317.1665 - mae: 29.0328[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1319.7476 - mae: 29.1156
Epoch 10/10
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - loss: 1046.3901 - mae: 25.3058[1m11/62[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1219.8613 - mae: 28.5043 [1m21/62[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1297.5770 - mae: 29.2032[1m31/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1322.8523 - mae: 29.4474[1m42/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 5ms/step - loss: 1333.8044 - mae: 29.5707[1m53/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 5ms/step - loss: 1333.0787 - mae: 29.5358[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 5ms/step - loss: 1294.8627 - mae: 28.9826
INFO:   Final train loss: 1294.8627
INFO: 
[Step 3/9] Extracting Deep Learning Parameters (CAM & Attention)...
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 43ms/step[1m51/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 1ms/step [1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 2ms/step
[1m 1/62[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m4s[0m 78ms/step[1m31/62[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 2ms/step [1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step[1m62/62[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step
INFO:   CAM: (1980, 10)
INFO:   Attention: (1980, 20)
INFO: 
[Step 4/9] Clustering DLP Features...
INFO: CAM èšç±»æ‹Ÿåˆ...
INFO: CAM èšç±»å®Œæˆï¼Œèšç±»æ•°: 3
INFO: Attentionæƒé‡èšç±»æ‹Ÿåˆ...
INFO: Attentionèšç±»å®Œæˆï¼Œèšç±»: ['Other', 'Early', 'Early']
INFO:   CAM clusters: [657 877 446]
INFO:   Attention types: {'Early': 1544, 'Other': 436}
INFO: 
[Step 5/9] Sn State Classification...
INFO: è®¡ç®—Snå°ºåº¦ä¼°è®¡å™¨...
INFO: ä¸­ä½æ•°: 146.2809, Snå°ºåº¦: 14.4536
INFO: K-meansèšç±»...
INFO: èšç±»ä¸­å¿ƒ: [-1.03924579  1.5286441   0.11955863]
INFO: çŠ¶æ€æ˜ å°„: {np.int64(0): 'Lower', np.int64(2): 'Normal', np.int64(1): 'Peak'}
INFO:   State distribution: {'Normal': 885, 'Lower': 615, 'Peak': 480}
INFO: 
[Step 6/9] Feature Discretization...
INFO: æ‹Ÿåˆç¦»æ•£åŒ–å™¨ï¼Œç‰¹å¾æ•°: 3
/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.
  warnings.warn(
/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.
  warnings.warn(
/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:304: FutureWarning: The current default behavior, quantile_method='linear', will be changed to quantile_method='averaged_inverted_cdf' in scikit-learn version 1.9 to naturally support sample weight equivalence properties by default. Pass quantile_method='averaged_inverted_cdf' explicitly to silence this warning.
  warnings.warn(
INFO: ç¦»æ•£åŒ–å™¨æ‹Ÿåˆå®Œæˆ
INFO:   Discretized features: ['Temperature', 'Humidity', 'WindSpeed', 'EDP_State', 'CAM_Cluster', 'Attention_Type']
INFO: 
[Step 7/9] Association Rule Mining...
INFO: AssociationRuleMiner initialized with min_support=0.05, min_confidence=0.6, min_lift=1.2
INFO: Prepared data: 1980 transactions, 20 items
INFO: Found 259 frequent itemsets
INFO: Generated 14 association rules
INFO: Filtered to 2 EDP-related rules
INFO: Extracted 3 candidate edges from top 50 rules
INFO:   Found 3 candidate edges
INFO: Saved 2 rules to ./outputs/training_run_1/results/association_rules.csv
INFO: 
[Step 8/9] Bayesian Network Learning...
ERROR: 
âŒ è®­ç»ƒå¤±è´¥: cannot import name 'BicScore' from 'pgmpy.estimators' (/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/pgmpy/estimators/__init__.py)
Traceback (most recent call last):
  File "/home/severin/Codelib/YS/scripts/run_training.py", line 93, in main
    results = pipeline.run(train_data)
              ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/severin/Codelib/YS/src/pipeline/train_pipeline.py", line 173, in run
    bn_edges = self._step8_learn_bayesian_network(discrete_data, candidate_edges)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/severin/Codelib/YS/src/pipeline/train_pipeline.py", line 355, in _step8_learn_bayesian_network
    from src.models.bayesian_net import CausalBayesianNetwork
  File "/home/severin/Codelib/YS/src/models/bayesian_net.py", line 10, in <module>
    from pgmpy.estimators import (
ImportError: cannot import name 'BicScore' from 'pgmpy.estimators' (/home/severin/Codelib/YS/.venv/lib/python3.12/site-packages/pgmpy/estimators/__init__.py)
